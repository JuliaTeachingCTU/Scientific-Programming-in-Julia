<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Lecture · Scientific Programming in Julia</title><meta name="title" content="Lecture · Scientific Programming in Julia"/><meta property="og:title" content="Lecture · Scientific Programming in Julia"/><meta property="twitter:title" content="Lecture · Scientific Programming in Julia"/><meta name="description" content="Documentation for Scientific Programming in Julia."/><meta property="og:description" content="Documentation for Scientific Programming in Julia."/><meta property="twitter:description" content="Documentation for Scientific Programming in Julia."/><meta property="og:url" content="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_10/lecture/"/><meta property="twitter:url" content="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_10/lecture/"/><link rel="canonical" href="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_10/lecture/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/onlinestats.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Scientific Programming in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Scientific Programming in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Programming in Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation/">Installation</a></li><li><a class="tocitem" href="../../projects/">Projects</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Introduction</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/motivation/">Motivation</a></li><li><a class="tocitem" href="../../lecture_01/basics/">Basics</a></li><li><a class="tocitem" href="../../lecture_01/demo/">Examples</a></li><li><a class="tocitem" href="../../lecture_01/outline/">Outline</a></li><li><a class="tocitem" href="../../lecture_01/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_01/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: The power of type system &amp; multiple dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_02/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_02/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Design patterns</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_03/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_03/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Package development, unit tests &amp; CI</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_04/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_04/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Performance benchmarking</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_05/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_05/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Lanuage introspection</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_06/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_06/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Macros</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_07/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_07/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Automatic differentiation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_08/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_08/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Intermediate representation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_09/lab/">Lab</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox" checked/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Parallel programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Lecture</a><ul class="internal"><li><a class="tocitem" href="#Process-level-paralelism"><span>Process-level paralelism</span></a></li><li><a class="tocitem" href="#Running-example:-Julia-sets"><span>Running example: Julia sets</span></a></li><li><a class="tocitem" href="#Shared-memory"><span>Shared memory</span></a></li><li><a class="tocitem" href="#Synchronization-/-Communication-primitives"><span>Synchronization / Communication primitives</span></a></li><li><a class="tocitem" href="#Sending-data"><span>Sending data</span></a></li><li><a class="tocitem" href="#Practical-advices"><span>Practical advices</span></a></li><li><a class="tocitem" href="#Multi-threadding"><span>Multi-threadding</span></a></li><li><a class="tocitem" href="#Garbage-collector-is-single-threadded"><span>Garbage collector is single-threadded</span></a></li><li><a class="tocitem" href="#Locks-/-lock-free-multi-threadding"><span>Locks / lock-free multi-threadding</span></a></li><li><a class="tocitem" href="#Take-away-message"><span>Take away message</span></a></li></ul></li><li><a class="tocitem" href="../lab/">Lab</a></li><li><a class="tocitem" href="../hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: GPU programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_11/lab/">Lab</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Ordinary Differential Equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_12/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_12/hw/">Homework</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">10: Parallel programming</a></li><li class="is-active"><a href>Lecture</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Lecture</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/2023W/docs/src/lecture_10/lecture.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Parallel-programming-with-Julia"><a class="docs-heading-anchor" href="#Parallel-programming-with-Julia">Parallel programming with Julia</a><a id="Parallel-programming-with-Julia-1"></a><a class="docs-heading-anchor-permalink" href="#Parallel-programming-with-Julia" title="Permalink"></a></h1><p>Julia offers different levels of parallel programming</p><ul><li>distributed processing, where jobs are split among different Julia processes</li><li>multi-threadding, where jobs are split among multiple threads within the same processes</li><li>SIMD instructions</li><li>Task switching.</li></ul><p>In this lecture, we will focus mainly on the first two, since SIMD instructions are mainly used for low-level optimization (such as writing your own very performant BLAS library), and task switching is not a true paralelism, but allows to run a different task when one task is waiting for example for IO.</p><p><strong>The most important lesson is that before you jump into the parallelism, be certain you have made your sequential code as fast as possible.</strong></p><h2 id="Process-level-paralelism"><a class="docs-heading-anchor" href="#Process-level-paralelism">Process-level paralelism</a><a id="Process-level-paralelism-1"></a><a class="docs-heading-anchor-permalink" href="#Process-level-paralelism" title="Permalink"></a></h2><p>Process-level paralelism means we run several instances of Julia (in different processes) and they communicate between each other using inter-process communication (IPC). The implementation of IPC differs if parallel julia instances share the same machine, or they are on different machines spread over the network. By default, different processes <em>do not share any libraries or any variables</em>. They are loaded clean and it is up to the user to set-up all needed code and data.</p><p>Julia&#39;s default modus operandi is a single <em>main</em> instance controlling several workers. This main instance has <code>myid() == 1</code>, worker processes receive higher numbers. Julia can be started with multiple workers from the very beggining, using <code>-p</code> switch as </p><pre><code class="language-julia hljs">julia -p n</code></pre><p>where <code>n</code> is the number of workers, or you can add workers after Julia has been started by</p><pre><code class="language-julia hljs">using Distributed
addprocs(n)</code></pre><p>You can also remove workers using <code>rmprocs</code>.  When Julia is started with <code>-p</code>, <code>Distributed</code> library is loaded by default on main worker. Workers can be on the same physical machines, or on different machines. Julia offer integration via <code>ClusterManagers.jl</code> with most schedulling systems.</p><p>If you want to evaluate piece of code on all workers including main process, a convenience macro <code>@everywhere</code> is offered.</p><pre><code class="language-julia hljs">@everywhere @show myid()</code></pre><p>As we have mentioned, workers are loaded without libraries. We can see that by running</p><pre><code class="language-julia hljs">@everywhere InteractiveUtils.varinfo()</code></pre><p>which fails, but after loading <code>InteractiveUtils</code> everywhere</p><pre><code class="language-julia hljs">using Statistics
@everywhere begin 
	using InteractiveUtils
	println(InteractiveUtils.varinfo(;imported = true))
end</code></pre><p>we see that <code>Statistics</code> was loaded only on the main process. Thus, there is not magical sharing of data and code. With <code>@everywhere</code> macro we can define function and variables, and import libraries on workers as </p><pre><code class="language-julia hljs">@everywhere begin 
	foo(x, y) = x * y + sin(y)
	foo(x) = foo(x, myid())
	x = rand()
end
@everywhere @show foo(1.0)
@everywhere @show x</code></pre><p>The fact that <code>x</code> has different values on different workers and master demonstrates again the independency of processes. While we can set up everything using <code>@everywhere</code> macro, we can also put all the code for workers into a separate file, e.g. <code>worker.jl</code> and load it on all workers using <code>-L worker.jl</code>.</p><p>Julia&#39;s multi-processing model is based on message-passing paradigm, but the abstraction is more akin to procedure calls. This means that users are saved from prepending messages with headers and implementing logic deciding which function should be called for thich header. Instead, we can <em>schedulle</em> an execution of a function on a remote worker and return the control immeadiately to continue in our job. A low-level function providing this functionality is <code>remotecall(fun, worker_id, args...)</code>. For example </p><pre><code class="language-julia hljs">@everywhere begin 
	function delayed_foo(x, y, n )
		sleep(n)
		println(&quot;woked up&quot;)
		foo(x, y)
	end
end
r = remotecall(delayed_foo, 2, 1, 1, 60)</code></pre><p>returns immediately, even though the function will take at least 60 seconds. <code>r</code> does not contain result of <code>foo(1, 1)</code>, but a struct <code>Future</code>, which is a <em>remote reference</em> in Julia&#39;s terminology. It points to data located on some machine, indicates, if they are available and allows to <code>fetch</code> them from the remote worker. <code>fetch</code> is blocking, which means that the execution is blocked until data are available (if they are never available, the process can wait forever.) The presence of data can be checked using <code>isready</code>, which in case of <code>Future</code> returned from <code>remote_call</code> indicate that the computation has finished.</p><pre><code class="language-julia hljs">isready(r)
fetch(r) == foo(1, 1)</code></pre><p>An advantage of the remote reference is that it can be freely shared around processes and the result can be retrieved on different node then the one which issued the call.s</p><pre><code class="language-julia hljs">r = remotecall(delayed_foo, 2, 1, 1, 60)
remotecall(r -&gt; println(&quot;value: &quot;,fetch(r), &quot; retrieved on &quot;, myid()) , 3, r)</code></pre><p>An interesting feature of <code>fetch</code> is that it re-throw an exception raised on a different process.</p><pre><code class="language-julia hljs">@everywhere begin 
	function exfoo()
		throw(&quot;Exception from $(myid())&quot;)
	end
end
r = @spawnat 2 exfoo()</code></pre><p>where we have used <code>@spawnat</code> instead of <code>remote_call</code>. It is higher level alternative executing a closure around the expression (in this case <code>exfoo()</code>) on a specified worker, in this case 2. Coming back to the example, when we fetch the result <code>r</code>, the exception is throwed on the main process, not on the worker</p><pre><code class="language-julia hljs">fetch(r)</code></pre><p><code>@spawnat</code> can be executed with <code>:any</code>  to signal that the user does not care, where the function will be executed and it will be left up to Julia.</p><pre><code class="language-julia hljs">r = @spawnat :any foo(1,1)
fetch(r)</code></pre><p>Finally, if you would for some reason need to wait for the computed value, you can use </p><pre><code class="language-julia hljs">remotecall_fetch(foo, 2, 1, 1)</code></pre><h2 id="Running-example:-Julia-sets"><a class="docs-heading-anchor" href="#Running-example:-Julia-sets">Running example: Julia sets</a><a id="Running-example:-Julia-sets-1"></a><a class="docs-heading-anchor-permalink" href="#Running-example:-Julia-sets" title="Permalink"></a></h2><p>Our example for explaining mechanisms of distributed computing will be Julia set fractals, as they can be easily paralelized.  The example is adapted from <a href="http://www.cs.unb.ca/~aubanel/JuliaMultithreadingNotes.html">Eric Aubanel</a>. Some fractals (Julia set, Mandelbrot) are determined by properties of some complex-valued functions. Julia set counts, how many iteration is required for  <span>$f(z) = z^2+c$</span> to be bigger than two in absolute value, <span>$|f(z)| &gt;=2$</span>. The number of iterations can then be mapped to the pixel&#39;s color, which creates a nice visualization we know.</p><pre><code class="language-julia hljs">function juliaset_pixel(z₀, c)
    z = z₀
    for i in 1:255
        abs2(z)&gt; 4.0 &amp;&amp; return (i - 1)%UInt8
        z = z*z + c
    end
    return UInt8(255)
end</code></pre><p>A nice property of fractals like Julia set is that the computation can be easily paralelized, since the value of each pixel is independent from the remaining. In our experiments, the level of granulity will be one column, since calculation of single pixel is so fast, that thread or process switching will have much higher overhead.</p><pre><code class="language-julia hljs">function juliaset_column!(img, c, n, j)
    x = -2.0 + (j-1)*4.0/(n-1)
    for i in 1:n
        y = -2.0 + (i-1)*4.0/(n-1)
        @inbounds img[i,j] = juliaset_pixel(x+im*y, c)
    end
    nothing
end</code></pre><p>To calculate full image</p><pre><code class="language-julia hljs">function juliaset(x, y, n=1000)
    c = x + y*im
    img = Array{UInt8,2}(undef,n,n)
    for j in 1:n
        juliaset_column!(img, c, n, j)
    end
    return img
end</code></pre><p>and run it and view it</p><pre><code class="language-julia hljs">using Plots
frac = juliaset(-0.79, 0.15)
plot(heatmap(1:size(frac,1),1:size(frac,2), frac, color=:Spectral))</code></pre><p>or with GLMakie</p><pre><code class="language-julia hljs">using GLMakie
frac = juliaset(-0.79, 0.15)
heatmap(frac)</code></pre><p>To observe the execution length, we will use <code>BenchmarkTools.jl</code> </p><pre><code class="nohighlight hljs">using BenchmarkTools
julia&gt; @btime juliaset(-0.79, 0.15);
  39.822 ms (2 allocations: 976.70 KiB)</code></pre><p>Let&#39;s now try to speed-up the computation using more processes. We first make functions available to workers</p><pre><code class="language-julia hljs">@everywhere begin 
	function juliaset_pixel(z₀, c)
	    z = z₀
	    for i in 1:255
	        abs2(z)&gt; 4.0 &amp;&amp; return (i - 1)%UInt8
	        z = z*z + c
	    end
	    return UInt8(255)
	end

	function juliaset_column!(img, c, n, colj, j)
	    x = -2.0 + (j-1)*4.0/(n-1)
	    for i in 1:n
	        y = -2.0 + (i-1)*4.0/(n-1)
	        @inbounds img[i,colj] = juliaset_pixel(x+im*y, c)
	    end
	    nothing
	end
end</code></pre><p>For the actual parallelisation, we split the computation of the whole image into bands, such that each worker computes a smaller portion.</p><pre><code class="language-julia hljs">@everywhere begin 
	function juliaset_columns(c, n, columns)
	    img = Array{UInt8,2}(undef, n, length(columns))
	    for (colj, j) in enumerate(columns)
	        juliaset_column!(img, c, n, colj, j)
	    end
	    img
	end
end

function juliaset_spawn(x, y, n = 1000)
    c = x + y*im
    columns = Iterators.partition(1:n, div(n, nworkers()))
    r_bands = [@spawnat w juliaset_columns(c, n, cols) for (w, cols) in enumerate(columns)]
    slices = map(fetch, r_bands)
    reduce(hcat, slices)
end</code></pre><p>we observe some speed-up over the serial version, but not linear in terms of number of workers</p><pre><code class="language-julia hljs">julia&gt; @btime juliaset(-0.79, 0.15);
  38.699 ms (2 allocations: 976.70 KiB)

julia&gt; @btime juliaset_spawn(-0.79, 0.15);
  21.521 ms (480 allocations: 1.93 MiB)</code></pre><p>In the above example, we spawn one function on each worker and collect the results. In essence, we are performing <code>map</code> over bands. Julia offers for this usecase a parallel version of map <code>pmap</code>. With that, our example can look like</p><pre><code class="language-julia hljs">function juliaset_pmap(x, y, n = 1000, np = nworkers())
    c = x + y*im
    columns = Iterators.partition(1:n, div(n, np))
    slices = pmap(cols -&gt; juliaset_columns(c, n, cols), columns)
    reduce(hcat, slices)
end

julia&gt; @btime juliaset_pmap(-0.79, 0.15);
  17.597 ms (451 allocations: 1.93 MiB)</code></pre><p>which has slightly better timing then the version based on <code>@spawnat</code> and <code>fetch</code> (as explained below in section about <code>Threads</code>, the parallel computation of Julia set suffers from each pixel taking different time to compute, which can be relieved by dividing the work into more parts:</p><pre><code class="language-julia hljs">julia&gt; @btime juliaset_pmap(-0.79, 0.15, 1000, 16);
  12.686 ms (1439 allocations: 1.96 MiB)</code></pre><h2 id="Shared-memory"><a class="docs-heading-anchor" href="#Shared-memory">Shared memory</a><a id="Shared-memory-1"></a><a class="docs-heading-anchor-permalink" href="#Shared-memory" title="Permalink"></a></h2><p>When main and all workers are located on the same process, and the OS supports sharing memory between processes (by sharing memory pages), we can use <code>SharedArrays</code> to avoid sending the matrix with results.</p><pre><code class="language-julia hljs">@everywhere begin
	using SharedArrays
	function juliaset_shared(x, y, n=1000)
	    c = x + y*im
	    img = SharedArray(Array{UInt8,2}(undef,n,n))
	    @sync @distributed for j in 1:n
	        juliaset_column!(img, c, n, j, j)
	    end
	    return img
	end 
end

julia&gt; @btime juliaset_shared(-0.79, 0.15);
  19.088 ms (963 allocations: 1017.92 KiB)</code></pre><p>The allocation of the Shared Array mich be costly, let&#39;s try to put the allocation outside of the loop</p><pre><code class="language-julia hljs">img = SharedArray(Array{UInt8,2}(undef,1000,1000))
function juliaset_shared!(img, x, y, n=1000)
    c = x + y*im
    @sync @distributed for j in 1:n
        juliaset_column!(img, c, n, j, j)
    end
    return img
end 

julia&gt; @btime juliaset_shared!(img, -0.79, 0.15);
  17.399 ms (614 allocations: 27.61 KiB)</code></pre><p>but both versions are not akin. It seems like the alocation of <code>SharedArray</code> costs approximately <code>2</code>ms.</p><p><code>@distributed for</code> (<code>Distributed.pfor</code>) does not allows to supply, as it splits the for cycle to <code>nworkers()</code> processes. Above we have seen that more splits is better</p><pre><code class="language-julia hljs">@everywhere begin 
	function juliaset_columns!(img, c, n, columns)
	    for (colj, j) in enumerate(columns)
	        juliaset_column!(img, c, n, colj, j)
	    end
	end
end

img = SharedArray(Array{UInt8,2}(undef,1000,1000))
function juliaset_shared!(img, x, y, n=1000, np = nworkers())
    c = x + y*im
    columns = Iterators.partition(1:n, div(n, np))
    pmap(cols -&gt; juliaset_columns!(img, c, n, cols), columns)
    return img
end 

julia&gt; @btime juliaset_shared!(img, -0.79, 0.15, 1000, 16);
  11.760 ms (1710 allocations: 85.98 KiB)</code></pre><p>Which is almost 1ms faster than without used of pre-allocated <code>SharedArray</code>. Notice the speedup is now <code>38.699 / 11.76 = 3.29×</code></p><h2 id="Synchronization-/-Communication-primitives"><a class="docs-heading-anchor" href="#Synchronization-/-Communication-primitives">Synchronization / Communication primitives</a><a id="Synchronization-/-Communication-primitives-1"></a><a class="docs-heading-anchor-permalink" href="#Synchronization-/-Communication-primitives" title="Permalink"></a></h2><p>The orchestration of a complicated computation might be difficult with relatively low-level remote calls. A <em>producer / consumer</em> paradigm is a synchronization paradigm that uses queues. Consumer fetches work intructions from the queue and pushes results to different queue. Julia supports this paradigm with <code>Channel</code> and <code>RemoteChannel</code> primitives. Importantly, putting to and taking from queue is an atomic operation, hence we do not have take care of race conditions. The code for the worker might look like</p><pre><code class="language-julia hljs">@everywhere begin 
	function juliaset_channel_worker(instructions, results)
		while isready(instructions)
			c, n, cols = take!(instructions)
			put!(results, (cols, juliaset_columns(c, n, cols)))
		end
	end
end</code></pre><p>The code for the main will look like</p><pre><code class="language-julia hljs">function juliaset_channels(x, y, n = 1000, np = nworkers())
	c = x + y*im
	columns = Iterators.partition(1:n, div(n, np))
	instructions = RemoteChannel(() -&gt; Channel(np))
	foreach(cols -&gt; put!(instructions, (c, n, cols)), columns)
	results = RemoteChannel(()-&gt;Channel(np))
	rfuns = [@spawnat i juliaset_channel_worker(instructions, results) for i in workers()]

	img = Array{UInt8,2}(undef, n, n)
	for i in 1:np
		cols, impart = take!(results)
		img[:,cols] .= impart;
	end
	img
end

julia&gt; @btime juliaset_channels(-0.79, 0.15);</code></pre><p>The execution time is much higher then what we have observed in the previous cases and changing the number of workers does not help much. What went wrong? The reason is that setting up the infrastructure around remote channels is a costly process. Consider the following alternative, where (i) we let workers to run endlessly and (ii) the channel infrastructure is set-up once and wrapped into an anonymous function</p><pre><code class="language-julia hljs">@everywhere begin 
	function juliaset_channel_worker(instructions, results)
		while true
		  c, n, cols = take!(instructions)
		  put!(results, (cols, juliaset_columns(c, n, cols)))
		end
	end
end

function juliaset_init(x, y, n = 1000, np = nworkers())
  c = x + y*im
  columns = Iterators.partition(1:n, div(n, np))
  T = Tuple{ComplexF64,Int64,UnitRange{Int64}}
  instructions = RemoteChannel(() -&gt; Channel{T}(np))
  T = Tuple{UnitRange{Int64},Array{UInt8,2}}
  results = RemoteChannel(()-&gt;Channel{T}(np))
  foreach(p -&gt; remote_do(juliaset_channel_worker, p, instructions, results), workers())
  function compute()
    img = Array{UInt8,2}(undef, n, n)
    foreach(cols -&gt; put!(instructions, (c, n, cols)), columns)
    for i in 1:np
      cols, impart = take!(results)
      img[:,cols] .= impart;
    end
    img
  end 
end

t = juliaset_init(-0.79, 0.15)
julia&gt; @btime t();
  17.697 ms (776 allocations: 1.94 MiB)</code></pre><p>with which we obtain the comparable speed to the <code>pmap</code> approach.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><h3><code>remote_do</code> vs <code>remote_call</code></h3><p>Instead of <code>@spawnat</code> (<code>remote_call</code>) we can also use <code>remote_do</code> as foreach<code>(p -&gt; remote_do(juliaset_channel_worker, p, instructions, results), workers)</code>, which executes the function <code>juliaset_channel_worker</code> at worker <code>p</code> with parameters <code>instructions</code> and <code>results</code> but does not return <code>Future</code> handle to receive the future results.</p></div></div><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><h3><code>Channel</code> and <code>RemoteChannel</code></h3><p><code>AbstractChannel</code> has to implement the interface <code>put!</code>, <code>take!</code>, <code>fetch</code>, <code>isready</code> and <code>wait</code>, i.e. it should behave like a queue. <code>Channel</code> is an implementation if an <code>AbstractChannel</code> that facilitates a communication within a single process (for the purpose of multi-threadding and task switching). Channel can be easily created by <code>Channel{T}(capacity)</code>, which can be infinite. The storage of a channel can be seen in <code>data</code> field, but a direct access will of course break all guarantees like atomicity of <code>take!</code> and <code>put!</code>.  For communication between proccesses, the <code>&lt;:AbstractChannel</code> has to be wrapped in <code>RemoteChannel</code>. The constructor for <code>RemoteChannel(f::Function, pid::Integer=myid())</code> has a first argument a function (without arguments) which constructs the <code>Channel</code> (or something like that) on the remote machine identified by <code>pid</code> and returns the <code>RemoteChannel</code>. The storage thus resides on the machine specified by <code>pid</code> and the handle provided by the <code>RemoteChannel</code> can be freely passed to any process. (For curious, <code>ProcessGroup</code> <code>Distributed.PGRP</code> contains an information about channels on machines.) </p></div></div><p>In the above example, <code>juliaset_channel_worker</code> defined as </p><pre><code class="language-julia hljs">function juliaset_channel_worker(instructions, results)
	while true
	  c, n, cols = take!(instructions)
	  put!(results, (cols, juliaset_columns(c, n, cols)))
	end
end</code></pre><p>runs forever due to the <code>while true</code> loop. </p><p>Julia does not provide by default any facility to kill the remote execution except sending <code>ctrl-c</code> to the remote worker as <code>interrupt(pids::Integer...)</code>. To stop the computation, we usually extend the type accepted by the <code>instructions</code> channel to accept some stopping token (e.g. :stop) and stop.</p><pre><code class="language-julia hljs">@everywhere begin 
	function juliaset_channel_worker(instructions, results)
		while true
			i = take!(instructions)
			i === :stop &amp;&amp; break
			c, n, cols = i
			put!(results, (cols, juliaset_columns(c, n, cols)))
		end
		println(&quot;worker $(myid()) stopped&quot;)
		put!(results, :stop)
	end
end

function juliaset_init(x, y, n = 1000, np = nworkers())
  c = x + y*im
  columns = Iterators.partition(1:n, div(n, np))
  instructions = RemoteChannel(() -&gt; Channel(np))
  results = RemoteChannel(()-&gt;Channel(np))
  foreach(p -&gt; remote_do(juliaset_channel_worker, p, instructions, results), workers())
  function compute()
    img = Array{UInt8,2}(undef, n, n)
    foreach(cols -&gt; put!(instructions, (c, n, cols)), columns)
    for i in 1:np
      cols, impart = take!(results)
      img[:,cols] .= impart;
    end
    img
  end 
end

t = juliaset_init(-0.79, 0.15)
t()
foreach(i -&gt; put!(t.instructions, :stop), workers())</code></pre><p>In the above example we paid the price of introducing type instability into the channels, which now contain types <code>Any</code> instead of carefully constructed tuples. But the impact on the overall running time is negligible</p><pre><code class="language-julia hljs">t = juliaset_init(-0.79, 0.15)
julia&gt; @btime t()
  17.551 ms (774 allocations: 1.94 MiB)
foreach(i -&gt; put!(t.instructions, :stop), workers())</code></pre><p>In some use-cases, the alternative can be to put all jobs to the <code>RemoteChannel</code> before workers are started, and then stop the workers when the remote channel is empty as </p><pre><code class="language-julia hljs">@everywhere begin 
	function juliaset_channel_worker(instructions, results)
		while !isready(instructions)
		  c, n, cols = take!(instructions)
		  put!(results, (cols, juliaset_columns(c, n, cols)))
		end
	end
end</code></pre><h2 id="Sending-data"><a class="docs-heading-anchor" href="#Sending-data">Sending data</a><a id="Sending-data-1"></a><a class="docs-heading-anchor-permalink" href="#Sending-data" title="Permalink"></a></h2><p>Sending parameters of functions and receiving results from a remotely called functions migh incur a significant cost. </p><ol><li>Try to minimize the data movement as much as possible. A prototypical example is</li></ol><pre><code class="language-julia hljs">A = rand(1000,1000);
Bref = @spawnat :any A^2;</code></pre><p>and</p><pre><code class="language-julia hljs">Bref = @spawnat :any rand(1000,1000)^2;</code></pre><ol><li>It is not only volume of data (in terms of the number of bytes), but also a complexity of objects that are being sent. Serialization can be very time consuming, an efficient converstion to something simple might be worth</li></ol><pre><code class="language-julia hljs">using BenchmarkTools
@everywhere begin 
	using Random
	v = [randstring(rand(1:20)) for i in 1:1000];
	p = [i =&gt; v[i] for i in 1:1000]
	d = Dict(p)

	send_vec() = v
	send_dict() = d
	send_pairs() = p
	custom_serialization() = (length.(v), join(v, &quot;&quot;))
end

@btime remotecall_fetch(send_vec, 2);
@btime remotecall_fetch(send_dict, 2);
@btime remotecall_fetch(send_pairs, 2);
@btime remotecall_fetch(custom_serialization, 2);</code></pre><ol><li>Some type of objects cannot be properly serialized and deserialized</li></ol><pre><code class="language-julia hljs">a = IdDict(
	:a =&gt; rand(1,1),
	)
b = remotecall_fetch(identity, 2, a)
a[:a] === a[:a]
a[:a] === b[:a]</code></pre><ol><li>If you need to send the data to worker, i.e. you want to define (overwrite) a global variable there</li></ol><pre><code class="language-julia hljs">@everywhere begin 
	g = rand()
	show_secret() = println(&quot;secret of &quot;, myid(), &quot; is &quot;, g)
end
@everywhere show_secret()

for i in workers()
	remotecall_fetch(g -&gt; eval(:(g = $(g))), i, g)
end
@everywhere show_secret()</code></pre><p>which is implemented in the <code>ParallelDataTransfer.jl</code> with other variants, but in general, this construct should be avoided.</p><p>Alternatively, you can overwrite a global variable</p><pre><code class="language-julia hljs">@everywhere begin 
	g = rand()
	show_secret() = println(&quot;secret of &quot;, myid(), &quot; is &quot;, g)
	function set_g(x) 
		global g
		g = x
		nothing
	end
end

@everywhere show_secret()
remote_do(set_g, 2, 2)
@everywhere show_secret()</code></pre><h2 id="Practical-advices"><a class="docs-heading-anchor" href="#Practical-advices">Practical advices</a><a id="Practical-advices-1"></a><a class="docs-heading-anchor-permalink" href="#Practical-advices" title="Permalink"></a></h2><p>Recall that (i) workers are started as clean processes and (ii) they might not share the same environment with the main process. The latter is due to the possibility of remote machines to have a different directory structure. </p><pre><code class="language-julia hljs">@everywhere begin 
	using Pkg
	println(Pkg.project().path)
end</code></pre><p>Our advices earned by practice are:</p><ul><li>to have shared directory (shared home) with code and to share the location of packages</li><li>to place all code for workers to one file, let&#39;s call it <code>worker.jl</code> (author of this includes the code for master as well).</li><li>put to the beggining of <code>worker.jl</code> code activating specified environment as (or specify environmnet for all workers in environment variable as <code>export JULIA_PROJECT=&quot;$PWD&quot;</code>)</li></ul><pre><code class="language-julia hljs">using Pkg
Pkg.activate(@__DIR__)</code></pre><p>and optionally</p><pre><code class="language-julia hljs">Pkg.resolve()
Pkg.instantiate()</code></pre><ul><li>run julia as</li></ul><pre><code class="language-julia hljs">julia -p ?? -L worker.jl main.jl</code></pre><p>where <code>main.jl</code> is the script to be executed on the main node. Or</p><pre><code class="language-julia hljs">julia -p ?? -L worker.jl -e &quot;main()&quot;</code></pre><p>where <code>main()</code> is the function defined in <code>worker.jl</code> to be executed on the main node.</p><p>A complete example can be seen in <a href="../juliaset_p.jl"><code>juliaset_p.jl</code></a>.</p><h2 id="Multi-threadding"><a class="docs-heading-anchor" href="#Multi-threadding">Multi-threadding</a><a id="Multi-threadding-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-threadding" title="Permalink"></a></h2><p>So far, we have been able to decrese the computation from 39ms to something like 13ms. Can we improve? Let&#39;s now turn our attention to multi-threadding, where we will not pay the penalty for IPC. Moreover, the computation of Julia set is multi-thread friendly, as all the memory can be pre-allocatted. We slightly modify our code to accept different methods distributing the work among slices in the pre-allocated matrix. To start Julia with support of multi-threadding, run it with <code>julia -t n</code>, where <code>n</code> is the number of threads. It is reccomended to set <code>n</code> to number of physical cores, since in hyper-threadding two threads shares arithmetic units of a single core, and in applications for which Julia was built, they are usually saturated.</p><pre><code class="language-julia hljs">using BenchmarkTools
function juliaset_pixel(z₀, c)
    z = z₀
    for i in 1:255
        abs2(z)&gt; 4.0 &amp;&amp; return (i - 1)%UInt8
        z = z*z + c
    end
    return UInt8(255)
end

function juliaset_column!(img, c, n, j)
    x = -2.0 + (j-1)*4.0/(n-1)
    for i in 1:n
        y = -2.0 + (i-1)*4.0/(n-1)
        @inbounds img[i,j] = juliaset_pixel(x+im*y, c)
    end
    nothing
end

function juliaset(x, y, n=1000)
    c = x + y*im
    img = Array{UInt8,2}(undef,n,n)
    for j in 1:n
        juliaset_column!(img, c, n, j)
    end
    return img
end

julia&gt; @btime juliaset(-0.79, 0.15, 1000);
   38.932 ms (2 allocations: 976.67 KiB)</code></pre><p>Let&#39;s now try to speed-up the calculation using multi-threadding. <code>Julia v0.5</code> has introduced multi-threadding with static-scheduller with a simple syntax: just prepend the for-loop with a <code>Threads.@threads</code> macro. With that, the first multi-threaded version will looks like</p><pre><code class="language-julia hljs">function juliaset_static(x, y, n=1000)
    c = x + y*im
    img = Array{UInt8,2}(undef,n,n)
    Threads.@threads :static for j in 1:n
        juliaset_column!(img, c, n, j)
    end
    return img
end</code></pre><p>with benchmark</p><pre><code class="nohighlight hljs">julia&gt; 	@btime juliaset_static(-0.79, 0.15, 1000);
  15.751 ms (27 allocations: 978.75 KiB)</code></pre><p>Although we have used four-threads, and the communication overhead should be next to zero, the speed improvement is <span>$2.4$</span>. Why is that? </p><p>To understand bettern what is going on, we have improved the profiler we have been developing last week. The logging profiler logs time of entering and exitting every function call of every thread, which is useful to understand, what is going on. The api is not yet polished, but it will do its job. Importantly, to prevent excessive logging, we ask to log only some functions.</p><pre><code class="language-julia hljs">using LoggingProfiler
function juliaset_static(x, y, n=1000)
    c = x + y*im
    img = Array{UInt8,2}(undef,n,n)
    Threads.@threads :dynamic for j in 1:n
        LoggingProfiler.@recordfun juliaset_column!(img, c, n, j)
    end
    return img
end

LoggingProfiler.initbuffer!(1000)
juliaset_static(-0.79, 0.15, 1000);
LoggingProfiler.recorded()
LoggingProfiler.adjustbuffer!()
juliaset_static(-0.79, 0.15, 1000)
LoggingProfiler.export2svg(&quot;/tmp/profile.svg&quot;)
LoggingProfiler.export2luxor(&quot;profile.png&quot;)</code></pre><p><img src="../profile.png" alt="profile.png"/> From the visualization of the profiler we can see not all threads were working the same time. Thread 1 and 4 were working less that Thread 2 and 3. The reason is that the static scheduller partition the total number of columns (1000) into equal parts, where the total number of parts is equal to the number of threads, and assign each to a single thread. In our case, we will have four parts each of size 250. Since execution time of computing value of each pixel is not the same, threads with a lot zero iterations will finish considerably faster. This is the incarnation of one of the biggest problems in multi-threadding / schedulling. A contemprary approach is to switch to  dynamic schedulling, which divides the problem into smaller parts, and when a thread is finished with one part, it assigned new not-yet computed part.</p><p>From 1.5, one can specify the scheduller for <code>Threads.@thread [scheduller] for</code> construct to be either <code>:static</code> and / or <code>:dynamic</code>. The <code>:dynamic</code> is compatible with the <code>partr</code> dynamic scheduller. From <code>1.8</code>, <code>:dynamic</code> is default, but the range is dividided into <code>nthreads()</code> parts, which is the reason why we do not see an improvement.</p><p>Dynamic scheduller is also supported using by <code>Threads.@spawn</code> macro. The prototypical approach used for invocation is the fork-join model, where one recursivelly partitions the problems and wait in each thread for the other</p><pre><code class="language-julia hljs">function juliaset_recspawn!(img, c, n, lo=1, hi=n, ntasks=128)
    if hi - lo &gt; n/ntasks-1
        mid = (lo+hi)&gt;&gt;&gt;1
        finish = Threads.@spawn juliaset_recspawn!(img, c, n, lo, mid, ntasks)
        juliaset_recspawn!(img, c, n, mid+1, hi, ntasks)
        wait(finish)
        return
    end
    for j in lo:hi
        juliaset_column!(img, c, n, j)
    end
    nothing
end</code></pre><p>Measuring the time we observe four-times speedup, which corresponds to the number of threads.</p><pre><code class="language-julia hljs">function juliaset_forkjoin(x, y, n=1000, ntasks = 16)
    c = x + y*im
    img = Array{UInt8,2}(undef,n,n)
    juliaset_recspawn!(img, c, n, 1, n, ntasks)
    return img
end

julia&gt; @btime juliaset_forkjoin(-0.79, 0.15);
  10.326 ms (142 allocations: 986.83 KiB)</code></pre><p>This is so far our fastest construction with speedup <code>38.932 / 10.326 = 3.77×</code>.</p><p>Unfortunatelly, the <code>LoggingProfiler</code> does not handle task migration at the moment, which means that we cannot visualize the results. Due to task switching overhead, increasing the granularity might not pay off.</p><pre><code class="language-julia hljs">4 tasks: 16.262 ms (21 allocations: 978.05 KiB)
8 tasks: 10.660 ms (45 allocations: 979.80 KiB)
16 tasks: 10.326 ms (142 allocations: 986.83 KiB)
32 tasks: 10.786 ms (238 allocations: 993.83 KiB)
64 tasks: 10.211 ms (624 allocations: 1021.89 KiB)
128 tasks: 10.224 ms (1391 allocations: 1.05 MiB)
256 tasks: 10.617 ms (2927 allocations: 1.16 MiB)
512 tasks: 11.012 ms (5999 allocations: 1.38 MiB)</code></pre><pre><code class="language-julia hljs">using FLoops, FoldsThreads
function juliaset_folds(x, y, n=1000, basesize = 2)
    c = x + y*im
    img = Array{UInt8,2}(undef,n,n)
    @floop ThreadedEx(basesize = basesize) for j in 1:n
        juliaset_column!(img, c, n, j)
    end
    return img
end

julia&gt; @btime juliaset_folds(-0.79, 0.15, 1000);
  10.253 ms (3960 allocations: 1.24 MiB)</code></pre><p>where <code>basesize</code> is the size of the smallest part allocated to a single thread, in this case 2 columns.</p><pre><code class="language-julia hljs">julia&gt; @btime juliaset_folds(-0.79, 0.15, 1000);
  10.575 ms (52 allocations: 980.12 KiB)</code></pre><pre><code class="language-julia hljs">function juliaset_folds(x, y, n=1000, basesize = 2)
    c = x + y*im
    img = Array{UInt8,2}(undef,n,n)
    @floop DepthFirstEx(basesize = basesize) for j in 1:n
        juliaset_column!(img, c, n, j)
    end
    return img
end
julia&gt; @btime juliaset_folds(-0.79, 0.15, 1000);
  10.421 ms (3582 allocations: 1.20 MiB)</code></pre><p>We can identify the best smallest size of the work <code>basesize</code> and measure its influence on the time</p><pre><code class="language-julia hljs">map(2 .^ (0:7)) do bs 
	t = @belapsed juliaset_folds(-0.79, 0.15, 1000, $(bs));
	(;basesize = bs, time = t)
end |&gt; DataFrame</code></pre><pre><code class="language-julia hljs"> Row │ basesize  time
     │ Int64     Float64
─────┼─────────────────────
   1 │        1  0.0106803
   2 │        2  0.010267
   3 │        4  0.0103081
   4 │        8  0.0101652
   5 │       16  0.0100204
   6 │       32  0.0100097
   7 │       64  0.0103293
   8 │      128  0.0105411</code></pre><p>We observe that the minimum is for <code>basesize = 32</code>, for which we got <code>3.8932×</code> speedup. </p><h2 id="Garbage-collector-is-single-threadded"><a class="docs-heading-anchor" href="#Garbage-collector-is-single-threadded">Garbage collector is single-threadded</a><a id="Garbage-collector-is-single-threadded-1"></a><a class="docs-heading-anchor-permalink" href="#Garbage-collector-is-single-threadded" title="Permalink"></a></h2><p>Keep reminded that while threads are very easy very convenient to use, there are use-cases where you might be better off with proccess, even though there will be some communication overhead. One such case happens when you need to allocate and free a lot of memory. This is because Julia&#39;s garbage collector is single-threadded (in 1.10 it is now partially multi-threaded). Imagine a task of making histogram of bytes in a directory. For a fair comparison, we will use <code>Transducers</code>, since they offer thread and process based paralelism</p><pre><code class="language-julia hljs">using Transducers
@everywhere begin 
	function histfile(filename)
		h = Dict{UInt8,Int}()
		foreach(open(read, filename, &quot;r&quot;)) do b 
			h[b] = get(h, b, 0) + 1
		end
		h
	end
end

files = filter(isfile, readdir(&quot;/Users/tomas.pevny/Downloads/&quot;, join = true))
@elapsed foldxd(mergewith(+), files |&gt; Map(histfile))
150.863183701</code></pre><p>and using the multi-threaded version of <code>map</code></p><pre><code class="language-julia hljs">@elapsed foldxt(mergewith(+), files |&gt; Map(histfile))
205.309952618</code></pre><p>we see that the threadding is actually worse than process based paralelism despite us paying the price for serialization  and deserialization of  <code>Dict</code>. Needless to say that changing <code>Dict</code> to <code>Vector</code> as</p><pre><code class="language-julia hljs">using Transducers
@everywhere begin 
	function histfile(filename)
		h = Dict{UInt8,Int}()
		foreach(open(read, filename, &quot;r&quot;)) do b 
			h[b] = get(h, b, 0) + 1
		end
		h
	end
end
files = filter(isfile, readdir(&quot;/Users/tomas.pevny/Downloads/&quot;, join = true))
@elapsed foldxd(mergewith(+), files |&gt; Map(histfile))
86.44577969
@elapsed foldxt(mergewith(+), files |&gt; Map(histfile))
105.32969331</code></pre><p>is much better.</p><h2 id="Locks-/-lock-free-multi-threadding"><a class="docs-heading-anchor" href="#Locks-/-lock-free-multi-threadding">Locks / lock-free multi-threadding</a><a id="Locks-/-lock-free-multi-threadding-1"></a><a class="docs-heading-anchor-permalink" href="#Locks-/-lock-free-multi-threadding" title="Permalink"></a></h2><p>Avoid locks.</p><h2 id="Take-away-message"><a class="docs-heading-anchor" href="#Take-away-message">Take away message</a><a id="Take-away-message-1"></a><a class="docs-heading-anchor-permalink" href="#Take-away-message" title="Permalink"></a></h2><p>When deciding, what kind of paralelism to employ, consider following</p><ul><li>for tightly coupled computation over shared data, multi-threadding is more suitable due to non-existing sharing of data between processes</li><li>but if the computation requires frequent allocation and freeing of memery, or IO, separate processes are multi-suitable, since garbage collectors are independent between processes</li><li>Making all cores busy while achieving an ideally linear speedup is difficult and needs a lot of experience and knowledge. Tooling and profilers supporting debugging of parallel processes is not much developped.</li><li><code>Transducers</code> thrives for (almost) the same code to support thread- and process-based paralelism.</li></ul><h3 id="Materials"><a class="docs-heading-anchor" href="#Materials">Materials</a><a id="Materials-1"></a><a class="docs-heading-anchor-permalink" href="#Materials" title="Permalink"></a></h3><ul><li><a href="http://cecileane.github.io/computingtools/pages/notes1209.html">http://cecileane.github.io/computingtools/pages/notes1209.html</a></li><li><a href="https://lucris.lub.lu.se/ws/portalfiles/portal/61129522/julia_parallel.pdf">https://lucris.lub.lu.se/ws/portalfiles/portal/61129522/julia_parallel.pdf</a></li><li><a href="http://igoro.com/archive/gallery-of-processor-cache-effects/">http://igoro.com/archive/gallery-of-processor-cache-effects/</a></li><li><a href="https://www.csd.uwo.ca/~mmorenom/cs2101a_moreno/Parallel_computing_with_Julia.pdf">https://www.csd.uwo.ca/~mmorenom/cs2101a<em>moreno/Parallel</em>computing<em>with</em>Julia.pdf</a></li><li>Complexity of thread schedulling <a href="https://www.youtube.com/watch?v=YdiZa0Y3F3c">https://www.youtube.com/watch?v=YdiZa0Y3F3c</a></li><li>TapIR –- Teaching paralelism to Julia compiler <a href="https://www.youtube.com/watch?v=-JyK5Xpk7jE">https://www.youtube.com/watch?v=-JyK5Xpk7jE</a></li><li>Threads: <a href="https://juliahighperformance.com/code/Chapter09.html">https://juliahighperformance.com/code/Chapter09.html</a></li><li>Processes: <a href="https://juliahighperformance.com/code/Chapter10.html">https://juliahighperformance.com/code/Chapter10.html</a></li><li>Alan Adelman uses FLoops in <a href="https://www.youtube.com/watch?v=dczkYlOM2sg">https://www.youtube.com/watch?v=dczkYlOM2sg</a></li><li>Examples: ?Heat equation? from [https://hpc.llnl.gov/training/tutorials/](introduction-parallel-computing-tutorial#Examples(https://hpc.llnl.gov/training/tutorials/)</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_09/lab/">« Lab</a><a class="docs-footer-nextpage" href="../lab/">Lab »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Wednesday 28 February 2024 13:00">Wednesday 28 February 2024</span>. Using Julia version 1.10.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
