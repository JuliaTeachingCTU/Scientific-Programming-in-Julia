<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Lab · Scientific Programming in Julia</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_11/lab/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/onlinestats.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Scientific Programming in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Scientific Programming in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Programming in Julia</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation/">Installation</a></li><li><a class="tocitem" href="../../projects/">Projects</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Introduction</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/motivation/">Motivation</a></li><li><a class="tocitem" href="../../lecture_01/basics/">Basics</a></li><li><a class="tocitem" href="../../lecture_01/demo/">Examples</a></li><li><a class="tocitem" href="../../lecture_01/outline/">Outline</a></li><li><a class="tocitem" href="../../lecture_01/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_01/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: The power of Type System &amp; multiple dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_02/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_02/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Design Patterns</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_03/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_03/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Package development, Unit test &amp; CI</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_04/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_04/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Performance Benchmarking</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_05/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_05/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Language introspection</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_06/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_06/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Macros</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_07/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_07/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Automatic differentiation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_08/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_08/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Intermediate representation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_09/lab/">Lab</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Parallel programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_10/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_10/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox" checked/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: GPU programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../lecture/">Lecture</a></li><li class="is-active"><a class="tocitem" href>Lab</a><ul class="internal"><li><a class="tocitem" href="#We-DON&#39;T-want-to-get-our-hands-dirty"><span>We DON&#39;T want to get our hands dirty</span></a></li><li><a class="tocitem" href="#We-DO-want-to-get-our-hands-dirty"><span>We DO want to get our hands dirty</span></a></li><li><a class="tocitem" href="#GPU-vendor-agnostic-code"><span>GPU vendor agnostic code</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Uncertainty propagation in ODE</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_12/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_12/hw/">Homework</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">11: GPU programming</a></li><li class="is-active"><a href>Lab</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Lab</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/2022W/docs/src/lecture_11/lab.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="gpu_lab"><a class="docs-heading-anchor" href="#gpu_lab">Lab 11: GPU programming</a><a id="gpu_lab-1"></a><a class="docs-heading-anchor-permalink" href="#gpu_lab" title="Permalink"></a></h1><p>In this lab we are going to delve into the topic of using GPU hardware in order to accelerate scientific computation. We will focus mainly on NVidia graphics hardware and it&#39;s <a href="https://developer.nvidia.com/cuda-zone">CUDA</a> framework<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>, however we will no go into as much detail and thus what you learn today should be aplicable to alternative HW/frameworks such as AMD&#39;s <a href="https://www.amd.com/en/graphics/servers-solutions-rocm">HIP/ROCM</a> or Intel&#39;s <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html#gs.lfd9th">oneAPI</a>. We have chosen to use primarily NVidia CUDA for demonstration due to it&#39;s maturity and the availability of HW on our side.</p><div class="admonition is-warning"><header class="admonition-header">Disclaimer</header><div class="admonition-body"><p>With the increasing complexity of GPU HW some statements may become outdated. Moreover we won&#39;t cover as many tips that you may encounter on a GPU parallel programming specific course.</p></div></div><h2 id="We-DON&#39;T-want-to-get-our-hands-dirty"><a class="docs-heading-anchor" href="#We-DON&#39;T-want-to-get-our-hands-dirty">We DON&#39;T want to get our hands dirty</a><a id="We-DON&#39;T-want-to-get-our-hands-dirty-1"></a><a class="docs-heading-anchor-permalink" href="#We-DON&#39;T-want-to-get-our-hands-dirty" title="Permalink"></a></h2><p>We can do quite a lot without even knowing that we are using GPU instead of CPU. This marvel is the combination of Julia&#39;s multiple dispatch and array abstractions. Based on the size of the problem and intricacy of the computation we may be achieve both incredible speedups as well as slowdowns. </p><p>The gateway to working with CUDA in Julia is the <code>CUDA.jl</code> library, which offers the following user facing functionalities</p><ul><li>device management <code>versioninfo</code>, <code>device!</code></li><li>definition of arrays on gpu <code>CuArray</code></li><li>data copying from host(CPU) to device(GPU) and the other way around</li><li>wrapping already existing library code in <code>CuBLAS</code>, <code>CuRAND</code>, <code>CuDNN</code>, <code>CuSparse</code> and others</li><li>kernel based programming (more on this in the second half of the lab)</li></ul><p>Let&#39;s use this to inspect our GPU hardware.</p><pre><code class="language-julia hljs">julia&gt; using CUDA
julia&gt; CUDA.versioninfo()
CUDA toolkit 11.5, artifact installation
NVIDIA driver 460.56.0, for CUDA 11.2
CUDA driver 11.2

Libraries: 
- CUBLAS: 11.7.4
- CURAND: 10.2.7
- CUFFT: 10.6.0
- CUSOLVER: 11.3.2
- CUSPARSE: 11.7.0
- CUPTI: 16.0.0
- NVML: 11.0.0+460.56
- CUDNN: 8.30.1 (for CUDA 11.5.0)
- CUTENSOR: 1.3.3 (for CUDA 11.4.0)

Toolchain:
- Julia: 1.6.5
- LLVM: 11.0.1
- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0
- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80

2 devices:
  0: GeForce RTX 2080 Ti (sm_75, 10.111 GiB / 10.758 GiB available)
  1: TITAN Xp (sm_61, 11.897 GiB / 11.910 GiB available)</code></pre><div class="admonition is-info"><header class="admonition-header">CUDA.jl compatibility</header><div class="admonition-body"><p>The latest development version of CUDA.jl requires Julia 1.6 or higher. <code>CUDA.jl</code> currently also requires a CUDA-capable GPU with compute capability 3.5 (Kepler) or higher, and an accompanying NVIDIA driver with support for CUDA 10.1 or newer. These requirements are not enforced by the Julia package manager when installing CUDA.jl. Depending on your system and GPU, you may need to install an older version of the package.<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup></p></div></div><p>As we have already seen in the <a href="../lecture/#gpu_lecture_no_kernel">lecture</a>, we can simply import <code>CUDA.jl</code> define some arrays, move them to the GPU and do some computation. In the following code we define two matrices <code>1000x1000</code> filled with random numbers and multiply them using usual <code>x * y</code> syntax.</p><pre><code class="language-julia hljs">x = randn(Float32, 60, 60)
y = randn(Float32, 60, 60)
x * y 
cx = CuArray(x)
cy = CuArray(y)
cx * cy
x * y ≈ Matrix(cx * cy)</code></pre><p>This may not be anything remarkable, as such functionality is available in many other languages albeit usually with a less mathematical notation like <code>x.dot(y)</code>. With Julia&#39;s multiple dispatch, we can simply dispatch the multiplication operator/function <code>*</code> to a specific method that works on <code>CuArray</code> type. Check with <code>@code_typed</code> shows the call to CUBLAS library under the hood.</p><pre><code class="language-julia hljs">julia&gt; @code_typed cx * cy
CodeInfo(
1 ─ %1 = Base.getfield(A, :dims)::Tuple{Int64, Int64}
│   %2 = Base.getfield(%1, 1, true)::Int64
│   %3 = Base.getfield(B, :dims)::Tuple{Int64, Int64}
│   %4 = Base.getfield(%3, 2, true)::Int64
│   %5 = Core.tuple(%2, %4)::Tuple{Int64, Int64}
│   %6 = invoke CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}(CUDA.undef::UndefInitializer, %5::Tuple{Int64, Int64})::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}
│   %7 = invoke CUDA.CUBLAS.gemm_dispatch!(%6::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, A::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, B::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, true::Bool, false::Bool)::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}
└──      return %7
) =&gt; CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}</code></pre><p>Let&#39;s now explore what the we can do with this array programming paradigm on few practical examples</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Load a sufficiently large image to the GPU such as the one provided in the lab (anything &gt;1Mpx should be enough) and manipulate it in the following ways:</p><ul><li>create a negative</li><li>half the pixel brightness</li><li>find the brightest pixels</li><li>get it&#39;s FFT image</li></ul><p>Measure the runtime difference with <code>BenchmarkTools</code>. Load the image with the following code, which adds all the necessary dependencies and loads the image into Floa32 matrix.</p><pre><code class="language-julia hljs">using Pkg; 
Pkg.add([&quot;FileIO&quot;, &quot;ImageMagick&quot;, &quot;ImageShow&quot;, &quot;ColorTypes&quot;, &quot;FFTW&quot;])

using FileIO, ImageMagick, ImageShow, ColorTypes
rgb_img = FileIO.load(&quot;image.jpg&quot;);
gray_img = Float32.(Gray.(rgb_img));
cgray_img = CuArray(gray_img);</code></pre><p><strong>HINTS</strong>:</p><ul><li>use <code>Float32</code> everywhere for better performance</li><li>use <code>CUDA.@sync</code> during benchmarking in order to ensure that the computation has completed</li></ul><p><strong>BONUS</strong>: Remove high frequency signal by means of modifying Fourier image.</p><div class="admonition is-warning"><header class="admonition-header">Scalar indexing</header><div class="admonition-body"><p>Some operations such as showing an image calls fallback implementation which requires <code>getindex!</code> called from the CPU. As such it is incredibly slow and should be avoided. In order to show the image use <code>Array(cimg)</code> to move it as a whole. Another option is to suppress the output with semicolon</p><pre><code class="language-julia hljs">julia&gt; cimg
┌ Warning: Performing scalar indexing on task Task (runnable) @0x00007f25931b6380.
│ Invocation of getindex resulted in scalar indexing of a GPU array.
│ This is typically caused by calling an iterating implementation of a method.
│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,
│ and therefore are only permitted from the REPL for prototyping purposes.
│ If you did intend to index this array, annotate the caller with @allowscalar.
└ @ GPUArrays ~/.julia/packages/GPUArrays/gkF6S/src/host/indexing.jl:56
julia&gt; Array(cimg)
Voila!
julia&gt; cimg;</code></pre></div></div></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><pre><code class="language-julia hljs">negative(i) = 1.0f0 .- i
darken(i) = i .* 0.5f0
using CUDA.CUFFT
using FFTW
fourier(i) = fft(i)
brightest(i) = findmax(i)</code></pre><p>Benchmarking</p><pre><code class="language-julia hljs">julia&gt; @btime CUDA.@sync negative($cgray_img);
  28.990 μs (28 allocations: 1.69 KiB)

julia&gt; @btime negative($gray_img);
  490.301 μs (2 allocations: 4.57 MiB)

julia&gt; @btime CUDA.@sync darken($cgray_img);
  29.170 μs (28 allocations: 1.69 KiB)

julia&gt; @btime darken($gray_img);
  507.921 μs (2 allocations: 4.57 MiB)

julia&gt; @btime CUDA.@sync fourier($cgray_img);
  609.593 μs (80 allocations: 4.34 KiB)

julia&gt; @btime fourier($gray_img);
  45.891 ms (37 allocations: 18.29 MiB)

julia&gt; @btime CUDA.@sync brightest($cgray_img);
  44.840 μs (89 allocations: 5.06 KiB)

julia&gt; @btime brightest($gray_img);
  2.764 ms (0 allocations: 0 bytes)</code></pre></p></details><p>In the next example we will try to solve a system of linear equations <span>$Ax=b$</span>, where A is a large (possibly sparse) matrix.</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Benchmark the solving of the following linear system with <code>N</code> equations and <code>N</code> unknowns. Experiment with increasing <code>N</code> to find a value , from which the advantage of sending the matrix to GPU is significant (include the time of sending the data to and from the device). For the sake of this example significant means 2x speedup. At what point the memory requirements are incompatible with your hardware, i.e. exceeding the memory of a GPU?</p><pre><code class="language-julia hljs">α = 10.0f0
β = 10.0f0

function init(N, α, β, r = (0.f0, π/2.0f0))
    dx = (r[2] - r[1]) / N
    A = zeros(Float32, N+2, N+2)
    A[1,1] = 1.0f0
    A[end,end] = 1.0f0
    for i in 2:N+1
        A[i,i-1] = 1.0f0/(dx*dx)
        A[i,i] = -2.0f0/(dx*dx) - 16.0f0
        A[i,i+1] = 1.0f0/(dx*dx)
    end

    b = fill(-8.0f0, N+2)
    b[1] = α
    b[end] = β
    A, b
end

N = 30
A, b = init(N, α, β)</code></pre><p><strong>HINTS</strong>:</p><ul><li>use backslash operator <code>\</code> to solve the system</li><li>use <code>CuArray</code> and <code>Array</code> for moving the date to and from device respectively</li><li>use <code>CUDA.@sync</code> during benchmarking in order to ensure that the computation has completed</li></ul><p><strong>BONUS 1</strong>: Visualize the solution <code>x</code>. What may be the origin of our linear system of equations? <strong>BONUS 2</strong>: Use sparse matrix <code>A</code> to achieve the same thing. Can we exploit the structure of the matrix for more effective solution?</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><pre><code class="language-julia hljs">A, b = init(N, α, β)
cA, cb = CuArray(A), CuArray(b)
A
b
A\b
cA\cb

@btime $A \ $b;
@btime CUDA.@sync Array(CuArray($A) \ CuArray($b));</code></pre><p><strong>BONUS 1</strong>: The system comes from a solution of second order ODR with <em>boundary conditions</em>.</p><p><strong>BONUS 2</strong>: The matrix is tridiagonal, therefore we don&#39;t have to store all the entries.</p></p></details><p>Programming GPUs in this way is akin to using NumPy, MATLAB and other array based toolkits, which force users not to use for loops. There are attempts to make GPU programming in Julia more powerful without delving deeper into writing of GPU kernels. One of the attempts is <a href="https://github.com/mcabbott/Tullio.jl"><code>Tulio.jl</code></a>, which uses macros to annotate parallel for loops, similar to <a href="https://www.openmp.org/"><code>OpenMP</code></a>&#39;s <code>pragma</code> intrinsics, which can be compiled to GPU as well.</p><p>Note also that Julia&#39;s <code>CUDA.jl</code> is not a tensor compiler. With the exception of broadcast fusion, which is easily transferable to GPUs, there is no optimization between different kernels from the compiler point of view. Furthermore, memory allocations on GPU are handled by Julia&#39;s GC, which is single threaded and often not as aggressive, therefore similar application code can have different memory footprints on the GPU.</p><p>Nowadays there is a big push towards simplifying programming of GPUs, mainly in the machine learning community, which often requires switching between running on GPU/CPU to be a one click deal. However this may not always yield the required results, because the GPU&#39;s computation model is different from the CPU, see <a href="../lecture/#gpu_lecture">lecture</a>. This being said Julia&#39;s <code>Flux.jl</code> framework does offer such capabilities <sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup></p><pre><code class="language-julia hljs">using Flux, CUDA
m = Dense(10,5) |&gt; gpu
x = rand(10) |&gt; gpu
y = m(x)
y |&gt; cpu</code></pre><h2 id="We-DO-want-to-get-our-hands-dirty"><a class="docs-heading-anchor" href="#We-DO-want-to-get-our-hands-dirty">We DO want to get our hands dirty</a><a id="We-DO-want-to-get-our-hands-dirty-1"></a><a class="docs-heading-anchor-permalink" href="#We-DO-want-to-get-our-hands-dirty" title="Permalink"></a></h2><p>There are two paths that lead to the necessity of programming GPUs more directly via kernels</p><ol><li>We cannot express our algorithm in terms of array operations.</li><li>We want to get more out of the code,</li></ol><p>Note that the ability to write kernels in the language of your choice is not granted, as this club includes a limited amount of members - C, C++, Fortran and Julia <sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup>. Consider then the following comparison between <code>CUDA C</code> and <code>CUDA.jl</code> implementation of a simple vector addition kernels as seen in the <a href="../lecture/#gpu_lecture_yes_kernel">lecture</a>. Which one would you choose?</p><pre><code class="language-c hljs">#define cudaCall(err) // check return code for error
#define frand() (float)rand() / (float)(RAND_MAX)

__global__ void vadd(const float *a, const float *b, float *c) {
	int i = blockIdx.x * blockDim.x + threadIdx.x;
	c[i] = a[i] + b[i];
}

const int len = 100;
int main() {
	float *a, *b;
	a = new float[len];
	b = new float[len];
	for (int i = 0; i &lt; len; i++) {
		a[i] = frand(); b[i] = frand();
	}
	float *d_a, *d_b, *d_c;
	cudaCall(cudaMalloc(&amp;d_a, len * sizeof(float)));
	cudaCall(cudaMemcpy(d_a, a, len * sizeof(float), cudaMemcpyHostToDevice));
	cudaCall(cudaMalloc(&amp;d_b, len * sizeof(float)));

	cudaCall(cudaMemcpy(d_b, b, len * sizeof(float), cudaMemcpyHostToDevice));
	cudaCall(cudaMalloc(&amp;d_c, len * sizeof(float)));

	vadd&lt;&lt;&lt;1, len&gt;&gt;&gt;(d_a, d_b, d_c);

	float *c = new float[len];
	cudaCall(cudaMemcpy(c, d_c, len * sizeof(float), cudaMemcpyDeviceToHost));
	cudaCall(cudaFree(d_c));
	cudaCall(cudaFree(d_b));
	cudaCall(cudaFree(d_a));

	return 0;
}</code></pre><p>Compared to CUDA C the code is less bloated, while having the same functionality.<sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup></p><pre><code class="language-julia hljs">function vadd(a, b, c)
	i = (blockIdx().x-1) * blockDim().x + threadIdx().x
	c[i] = a[i] + b[i]
	return
end

len = 100
a = rand(Float32, len)
b = rand(Float32, len)
d_a = CuArray(a)
d_b = CuArray(b)
d_c = similar(d_a)
@cuda threads = (1, len) vadd(d_a, d_b, d_c)
c = Array(d_c)</code></pre><h3 id="CUDA-programming-model"><a class="docs-heading-anchor" href="#CUDA-programming-model">CUDA programming model</a><a id="CUDA-programming-model-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-programming-model" title="Permalink"></a></h3><p>Recalling from the lecture, in CUDA programming model, you usually write kernels, which represent the body of some parallel for loop. </p><ul><li>A kernel is executed on multiple threads, which are grouped into thread blocks. </li><li>All threads in a block are executed in the same Streaming Multi-processor (SM), having access to some shared pool of memory. </li><li>The number of threads launched is always a multiple of 32 (32 threads = 1 warp, therefore length of a thread block should be divisible by 32). </li><li>All threads in a single warps are executed simultaneously. </li><li>We have to take care of how many threads will be launched in order to complete the task at hand, i.e. if there are insufficiently many threads/blocks spawned we may end up doing only part of the task. </li><li>We can spawn threads/thread blocks in both in 1D, 2D or 3D blocks, which may ease the indexing inside the kernel when dealing with higher dimensional data.</li></ul><h4 id="Thread-indexing"><a class="docs-heading-anchor" href="#Thread-indexing">Thread indexing</a><a id="Thread-indexing-1"></a><a class="docs-heading-anchor-permalink" href="#Thread-indexing" title="Permalink"></a></h4><p>Stopping for a moment here to illustrate the last point with a visual aid<sup class="footnote-reference"><a id="citeref-5" href="#footnote-5">[5]</a></sup> <img src="../grid_block_thread.png" alt="grid_block_thread"/></p><p>This explains the indexing into a linear array from above</p><pre><code class="language-julia hljs">i = (blockIdx().x-1) * blockDim().x + threadIdx().x</code></pre><p>which is similar to the computation a linear index of multidimensional (in our case 2D array row ~ <code>blockIdx</code> and column <code>threadIdx</code>). Again let&#39;s use a visual help for this 1D vector<sup class="footnote-reference"><a id="citeref-6" href="#footnote-6">[6]</a></sup> <img src="../thread_index.png" alt="thread_indexing"/></p><h4 id="Launching-a-kernel"><a class="docs-heading-anchor" href="#Launching-a-kernel">Launching a kernel</a><a id="Launching-a-kernel-1"></a><a class="docs-heading-anchor-permalink" href="#Launching-a-kernel" title="Permalink"></a></h4><p>Let&#39;s now dig into what is happening during execution of the line <code>@cuda threads = (1, len) vadd(d_a, d_b, d_c)</code>:</p><ol><li>Compile the <code>vadd</code> kernel to GPU code (via LLVM and it&#39;s <a href="https://www.llvm.org/docs/NVPTXUsage.html">NVPTX backend</a>)</li><li>Parse and construct launch configuration of the kernel. Here we are creating <code>1</code> thread block with <code>1x100</code> threads (in reality 128 threads may be launched).</li><li>Schedule to run <code>vadd</code> kernel with constructed launch configuration and arguments.</li><li>Return the task status.</li></ol><p>It&#39;s important to stress that we only schedule the kernel to run, however in order to get the result we have to first wait for the completion. This can be done either via</p><ul><li><code>CUDA.@sync</code>, which we have already seen earlier</li><li>or a command to copy result to host (<code>Array(c)</code>), which always synchronizes kernels beforehand</li></ul><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Fix the <code>vadd</code> kernel such that it can work with different launch configurations, such as</p><pre><code class="language-julia hljs">@cuda threads=64 blocks=2 vadd(d_a, d_b, d_c)
@cuda threads=32 blocks=4 vadd(d_a, d_b, d_c)</code></pre><p>Is there some performance difference? Try increasing the size and corresponding number of blocks to cover the larger arrays.</p><p>What happens if we launch the kernel in the following way?</p><pre><code class="language-julia hljs">@cuda threads=32 blocks=2 vadd(d_a, d_b, d_c)</code></pre><p>Write a wrapper function <code>vadd_wrap(a::CuArray, b::CuArray)</code> for <code>vadd</code> kernel, such that it spawns the right amount of threads and returns only when the kernels has finished.</p><div class="admonition is-info"><header class="admonition-header">Wrapping kernels</header><div class="admonition-body"><p>A usual patter that you will see in GPU related code is that the kernel is written inside a function</p><pre><code class="language-julia hljs">function do_something(a,b)
	function do_something_kernel!(c,a,b)
		...
	end

	# handle allocation
	# handle launch configuration
	@cuda ... do_something_kernel!(c,a,b)
end</code></pre><p>Note that there are hardware limitations as to how many threads can be scheduled on a GPU. You can check it with the following code</p><pre><code class="language-julia hljs">k = @cuda vadd(d_a, d_b, d_c)
CUDA.maxthreads(k)</code></pre></div></div><p><strong>HINTS</strong>:</p><ul><li>if you don&#39;t know what is wrong with the current implementation just try it, but be warned that you might need to restart Julia after that</li><li>don&#39;t forget to use <code>CUDA.@sync</code> when benchmarking</li><li>you can inspect the kernel with analogs of <code>@code_warntype</code> ~ <code>@device_code_warntype @cuda vadd(d_a, d_b, d_c)</code></li><li>lookup <code>cld</code> function for computing the number of blocks when launching kernels on variable sized input</li></ul></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>In order to fix the out of bounds accesses we need to add manual bounds check, otherwise we may run into some nice Julia crashes.</p><pre><code class="language-julia hljs">function vadd(a, b, c)
	i = (blockIdx().x-1) * blockDim().x + threadIdx().x
    if i &lt;= length(c)
	    c[i] = a[i] + b[i]
    end
	return
end</code></pre><p>Launching kernel with insufficient number of threads leads to only partial results.</p><pre><code class="language-julia hljs">d_c = similar(d_a)
@cuda threads=32 blocks=2 vadd(d_a, d_b, d_c) # insufficient number of threads
Array(d_c)</code></pre><p>Benchmarking different implementation shows that in this case running more threads per block may be beneficial, however only up to some point.</p><pre><code class="language-julia hljs">len = 10_000
a = rand(Float32, len)
b = rand(Float32, len)
d_a = CuArray(a)
d_b = CuArray(b)
d_c = similar(d_a)

julia&gt; @btime CUDA.@sync @cuda threads=256 blocks=cld(len, 256) vadd($d_a, $d_b, $d_c)
       @btime CUDA.@sync @cuda threads=128 blocks=cld(len, 128) vadd($d_a, $d_b, $d_c)
       @btime CUDA.@sync @cuda threads=64 blocks=cld(len, 64) vadd($d_a, $d_b, $d_c)
       @btime CUDA.@sync @cuda threads=32 blocks=cld(len, 32) vadd($d_a, $d_b, $d_c)
  8.447 μs (24 allocations: 1.22 KiB)
  8.433 μs (24 allocations: 1.22 KiB)
  8.550 μs (24 allocations: 1.22 KiB)
  8.634 μs (24 allocations: 1.22 KiB)</code></pre></p></details><p>The launch configuration depends heavily on user&#39;s hardware and the actual computation in the kernel, where in some cases having more threads in a block is better (up to some point).</p><h3 id="Image-processing-with-kernels"><a class="docs-heading-anchor" href="#Image-processing-with-kernels">Image processing with kernels</a><a id="Image-processing-with-kernels-1"></a><a class="docs-heading-anchor-permalink" href="#Image-processing-with-kernels" title="Permalink"></a></h3><p>Following up on exercise with image processing let&#39;s use kernels for some functions that cannot be easily expressed as array operations.</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Implement <code>translate_kernel!(output, input, translation)</code>, which translates an image <code>input</code> in the direction of <code>translation</code> tuple (values given in pixels). The resulting image should be stored in <code>output</code>. Fill in the empty space with zeros.</p><p><strong>HINTS</strong>:</p><ul><li>use 2D grid of threads and blocks to simplify indexing</li><li>check all sides of an image for out of bounds accesses</li></ul><p><strong>BONUS</strong>: In a similar fashion you can create <code>scale_kernel!</code>, <code>rotate_kernel!</code> for scaling and rotation of an image.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><pre><code class="language-julia hljs">using CUDA
function translate_kernel!(output, input, translation)
    x_idx = (blockIdx().x-1) * blockDim().x + threadIdx().x
    y_idx = (blockIdx().y-1) * blockDim().y + threadIdx().y

    x_outidx = x_idx + translation[1]
    y_outidx = y_idx + translation[2]

    if (1 &lt;= x_outidx &lt;= size(output,1)) &amp;&amp; 
        (1 &lt;= y_outidx &lt;= size(output,2)) &amp;&amp; 
        (x_idx &lt;= size(output,1)) &amp;&amp; (y_idx &lt;= size(output,2))
        output[x_outidx, y_outidx] = input[x_idx, y_idx]
    end

    return
end

using FileIO, ImageMagick, ImageShow, ColorTypes
rgb_img = FileIO.load(&quot;image.jpg&quot;);
gray_img = Float32.(Gray.(rgb_img));
cgray_img = CuArray(gray_img);
cgray_img_moved = CUDA.fill(0.0f0, size(cgray_img));

blocks = cld.((size(cgray_img,1), size(cgray_img,2)), 32)
@cuda threads=(32, 32) blocks=blocks translate_kernel!(cgray_img_moved, cgray_img, (100, -100))
Gray.(Array(cgray_img_moved))

#@cuda threads=(64, 64) blocks=(1,1) translate_kernel!(cgray_img_moved, cgray_img, (-500, 500)) # too many threads per block (fails on some weird exception) - CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)</code></pre></p></details><h3 id="Profiling"><a class="docs-heading-anchor" href="#Profiling">Profiling</a><a id="Profiling-1"></a><a class="docs-heading-anchor-permalink" href="#Profiling" title="Permalink"></a></h3><p>CUDA framework offers a wide variety of developer tooling for debugging and profiling our own kernels. In this section we will focus profiling using the Nsight Systems software that you can download after registering <a href="https://developer.nvidia.com/nsight-systems">here</a>. It contains both <code>nsys</code> profiler as well as <code>nsys-ui</code>GUI application for viewing the results. First we have to run <code>julia</code> using <code>nsys</code> application.</p><ul><li>on Windows with PowerShell (available on the lab computers)</li></ul><pre><code class="language-ps hljs">&amp; &quot;C:\Program Files\NVIDIA Corporation\Nsight Systems 2021.2.4\target-windows-x64\nsys.exe&quot; launch --trace=cuda,nvtx H:/Downloads/julia-1.6.3/bin/julia.exe --color=yes --color=yes --project=$((Get-Item .).FullName)</code></pre><ul><li>on Linux</li></ul><pre><code class="language-bash hljs">/full/path/to/nsys launch --trace=cuda,nvtx /home/honza/Apps/julia-1.6.5/bin/julia --color=yes --project=.</code></pre><p>Once <code>julia</code> starts we have to additionally (on the lab computers, where we cannot modify env path) instruct <code>CUDA.jl</code>, where <code>nsys.exe</code> is located.</p><pre><code class="language-julia hljs">ENV[&quot;JULIA_CUDA_NSYS&quot;] = &quot;C:\\Program Files\\NVIDIA Corporation\\Nsight Systems 2021.2.4\\target-windows-x64\\nsys.exe&quot;</code></pre><p>Now we should be ready to start profiling our kernels.</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Choose a function/kernel out of previous exercises, in order to profile it. Use the <code>CUDA.@profile</code> macro the following patter to launch profiling of a block of code with <code>CUDA.jl</code></p><pre><code class="language-julia hljs">CUDA.@profile CUDA.@sync begin 
    NVTX.@range &quot;something&quot; begin
    		# run some kernel
    end 

    NVTX.@range &quot;something&quot; begin
    		# run some kernel
    end 
end</code></pre><p>where <code>NVTX.@range &quot;something&quot;</code> is part of <code>CUDA.jl</code> as well and serves us to mark a piece of execution for better readability later. Inspect the result in <code>NSight Systems</code>.</p><div class="admonition is-info"><header class="admonition-header">Profiling overhead</header><div class="admonition-body"><p>It is recommended to run the code twice as shown above, because the first execution with profiler almost always takes longer, even after compilation of the kernel itself. </p></div></div></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>In order to show multiple kernels running let&#39;s demonstrate profiling of the first image processing exercise</p><pre><code class="language-julia hljs">CUDA.@profile CUDA.@sync begin
    NVTX.@range &quot;copy H2D&quot; begin
        rgb_img = FileIO.load(&quot;image.jpg&quot;);
        gray_img = Float32.(Gray.(rgb_img));
        cgray_img = CuArray(gray_img);
    end

    NVTX.@range &quot;negative&quot; begin 
        negative(cgray_img);
    end
    NVTX.@range &quot;darken&quot; begin 
        darken(cgray_img);
    end
    NVTX.@range &quot;fourier&quot; begin 
        fourier(cgray_img);
    end
    NVTX.@range &quot;brightest&quot; begin 
        brightest(cgray_img);
    end
end</code></pre><p>Running this code should create a report in the current directory with the name <code>report-**.***</code>, which we can examine in <code>NSight Systems</code>.</p></p></details><h3 id="Matrix-multiplication"><a class="docs-heading-anchor" href="#Matrix-multiplication">Matrix multiplication</a><a id="Matrix-multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-multiplication" title="Permalink"></a></h3><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Write a generic matrix multiplication <code>generic_matmatmul!(C, A, B)</code>, which wraps a GPU kernel inside. For simplicity assume that both <code>A</code> and <code>B</code> input matrices have only <code>Float32</code> elements. Benchmark your implementation against <code>CuBLAS</code>&#39;s <code>mul!(C,A,B)</code>.</p><p><strong>HINTS</strong>:</p><ul><li>use 2D blocks for easier indexing</li><li>import <code>LinearAlgebra</code> to be able to directly call <code>mul!</code></li><li>in order to avoid a headache with the choice of launch config use the following code</li></ul><pre><code class="language-julia hljs">max_threads = 256
threads_x = min(max_threads, size(C,1))
threads_y = min(max_threads ÷ threads_x, size(C,2))
threads = (threads_x, threads_y)
blocks = ceil.(Int, (size(C,1), size(C,2)) ./ threads)</code></pre></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Adapted from the <code>CUDA.jl</code> source <a href="https://github.com/JuliaGPU/CuArrays.jl/blob/cee6253edeca2029d8d0522a46e2cdbb638e0a50/src/matmul.jl#L4-L50">code</a>.</p><pre><code class="language-julia hljs">function generic_matmatmul!(C, A, B)
    function kernel(C, A, B)
        i = (blockIdx().x-1) * blockDim().x + threadIdx().x
        j = (blockIdx().y-1) * blockDim().y + threadIdx().y

        if i &lt;= size(A,1) &amp;&amp; j &lt;= size(B,2)
            Ctmp = 0.0f0
            for k in 1:size(A,2)
                Ctmp += A[i, k]*B[k, j]
            end
            C[i,j] = Ctmp
        end

        return
    end

    max_threads = 256
    threads_x = min(max_threads, size(C,1))
    threads_y = min(max_threads ÷ threads_x, size(C,2))
    threads = (threads_x, threads_y)
    blocks = ceil.(Int, (size(C,1), size(C,2)) ./ threads)

    @cuda threads=threads blocks=blocks kernel(C, A, B)

    C
end

K, L, M = 10 .* (200, 100, 50)
A = CuArray(randn(K, L));
B = CuArray(randn(L, M));
C = similar(A, K, M);

generic_matmatmul!(C, A, B)

using LinearAlgebra
CC = similar(A, K, M)
mul!(CC, A, B)


using BenchmarkTools
@btime CUDA.@sync generic_matmatmul!(C, A, B);
@btime CUDA.@sync mul!(CC, A, B);</code></pre></p></details><h2 id="GPU-vendor-agnostic-code"><a class="docs-heading-anchor" href="#GPU-vendor-agnostic-code">GPU vendor agnostic code</a><a id="GPU-vendor-agnostic-code-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-vendor-agnostic-code" title="Permalink"></a></h2><p>There is an interesting direction that is allowed with the high level abstraction of Julia - <a href="https://github.com/JuliaGPU/KernelAbstractions.jl"><code>KernelAbstractions.jl</code></a>, which offer an overarching API over CUDA, AMD ROCM and Intel oneAPI frameworks.</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Disclaimer on <code>CUDA.jl</code>&#39;s GitHub page: <a href="https://github.com/JuliaGPU/CUDA.jl">url</a></li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>Taken from <code>Flux.jl</code> <a href="https://fluxml.ai/Flux.jl/stable/gpu/#GPU-Support">documentation</a></li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>There may be more of them, however these are the main ones.</li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>This comparison is not fair to <code>CUDA C</code>, where memory management is left to the user and all the types have to be specified. However at the end of the day the choice of a high level language makes more sense as it offers the same functionality and is far more approachable.</li><li class="footnote" id="footnote-5"><a class="tag is-link" href="#citeref-5">5</a>The number of blocks to be run are given by the grid dimension. Image taken from http://tdesell.cs.und.edu/lectures/cuda_2.pdf</li><li class="footnote" id="footnote-6"><a class="tag is-link" href="#citeref-6">6</a>Taken from <a href="https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing-1024x463.png">https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing-1024x463.png</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../lecture/">« Lecture</a><a class="docs-footer-nextpage" href="../../lecture_12/lecture/">Lecture »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 11 January 2023 19:27">Wednesday 11 January 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
