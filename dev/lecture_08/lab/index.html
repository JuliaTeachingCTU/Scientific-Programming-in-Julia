<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Lab · Scientific Programming in Julia</title><meta name="title" content="Lab · Scientific Programming in Julia"/><meta property="og:title" content="Lab · Scientific Programming in Julia"/><meta property="twitter:title" content="Lab · Scientific Programming in Julia"/><meta name="description" content="Documentation for Scientific Programming in Julia."/><meta property="og:description" content="Documentation for Scientific Programming in Julia."/><meta property="twitter:description" content="Documentation for Scientific Programming in Julia."/><meta property="og:url" content="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_08/lab/"/><meta property="twitter:url" content="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_08/lab/"/><link rel="canonical" href="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_08/lab/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/onlinestats.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Scientific Programming in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Scientific Programming in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Programming in Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation/">Installation</a></li><li><a class="tocitem" href="../../projects/">Projects</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Introduction</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/motivation/">Motivation</a></li><li><a class="tocitem" href="../../lecture_01/basics/">Basics</a></li><li><a class="tocitem" href="../../lecture_01/demo/">Examples</a></li><li><a class="tocitem" href="../../lecture_01/outline/">Outline</a></li><li><a class="tocitem" href="../../lecture_01/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_01/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: The power of type system &amp; multiple dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_02/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_02/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Design patterns</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_03/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_03/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Package development, unit tests &amp; CI</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_04/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_04/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Performance benchmarking</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_05/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_05/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Lanuage introspection</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_06/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_06/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Macros</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_07/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_07/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox" checked/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Automatic differentiation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../lecture/">Lecture</a></li><li class="is-active"><a class="tocitem" href>Lab</a><ul class="internal"><li><a class="tocitem" href="#Reverse-Mode-AD"><span>Reverse Mode AD</span></a></li><li><a class="tocitem" href="#Naively-Vectorized-Reverse-AD"><span>Naively Vectorized Reverse AD</span></a></li><li><a class="tocitem" href="#Reverse-AD-with-TrackedArrays"><span>Reverse AD with <code>TrackedArray</code>s</span></a></li></ul></li><li><a class="tocitem" href="../hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Intermediate representation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_09/lab/">Lab</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Parallel programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_10/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_10/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: GPU programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_11/lab/">Lab</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Ordinary Differential Equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_12/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_12/hw/">Homework</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">8: Automatic differentiation</a></li><li class="is-active"><a href>Lab</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Lab</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/2023W/docs/src/lecture_08/lab.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Lab-08-Reverse-Mode-Differentiation"><a class="docs-heading-anchor" href="#Lab-08-Reverse-Mode-Differentiation">Lab 08 - Reverse Mode Differentiation</a><a id="Lab-08-Reverse-Mode-Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Lab-08-Reverse-Mode-Differentiation" title="Permalink"></a></h1><p><img src="../gd-path.gif" alt="descend"/></p><p>In the lecture you have seen how to implement <em>forward-mode</em> automatic differentiation (AD). Assume you want to find the derivative <span>$\frac{df}{dx}$</span> of the function <span>$f:\mathbb R^2 \rightarrow \mathbb R$</span></p><pre><code class="language-julia hljs">f(x,y) = x*y + sin(x)</code></pre><p>If we have rules for <code>*</code>, <code>+</code>, and <code>sin</code> we could simply <em>seed</em> the function with <code>Dual(x,one(x))</code> and read out the derivative <span>$\frac{df}{dx}$</span> from the <code>Dual</code> that is returned by <code>f</code>. If we are also interested in the derivative <span>$\frac{df}{dy}$</span> we will have to run <code>f</code> again, this time seeding the second argument with <code>Dual(y,one(y))</code>. Hence, we have to evaluate <code>f</code> <em>twice</em> if we want derivatives w.r.t to both its arguments which means that forward differentiation scales as <span>$O(N)$</span> where <span>$N$</span> is the number of inputs to <code>f</code>.</p><pre><code class="language-julia hljs">dfdx = f(Dual(x,one(x)), Dual(y,zero(y)))
dfdy = f(Dual(x,zero(x)), Dual(y,one(y)))</code></pre><p><em>Reverse-mode</em> AD can compute gradients of functions with many inputs and one output in one go. This is great because very often we want to optimize loss functions which are exactly that: Functions with many input variables and one loss output.</p><h2 id="Reverse-Mode-AD"><a class="docs-heading-anchor" href="#Reverse-Mode-AD">Reverse Mode AD</a><a id="Reverse-Mode-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-Mode-AD" title="Permalink"></a></h2><p>With functions <span>$f:\mathbb R^N\rightarrow\mathbb R^M$</span> and <span>$g:\mathbb R^L\rightarrow \mathbb R^N$</span> with an input vector <span>$\bm x$</span> we can define the composition of <span>$f$</span> and <span>$g$</span> as</p><p class="math-container">\[\bm z = (f \circ g)(\bm x), \qquad \text{where} \qquad \bm y=g(\bm x), \qquad \bm z = f(\bm y).\]</p><p>The multivariate chainrule reads</p><p class="math-container">\[\left.\frac{\partial z_i}{\partial x_j}\right|_{\bm x} =
    \sum_{k=1}^N \left.\frac{\partial z_i}{\partial y_k}\right|_{\bm y}
                 \left.\frac{\partial y_k}{\partial x_i}\right|_{\bm x}\]</p><p>If you want to read about where this comes from you can check <a href="https://math.stackexchange.com/questions/3785018/intuitive-proof-of-the-multivariable-chain-rule">here</a> or <a href="https://people.math.harvard.edu/~shlomo/docs/Advanced_Calculus.pdf">here</a>. It is essentially one row of the <em>Jacobian matrix</em> <span>$J$</span>. Note that in order to compute the derivative we always have to know the input to the respective function, because we can only compute the derivative <em>at a specific point</em> (denoted by the <span>$|_x$</span> <span>$_{}$</span> notation).  For our example</p><p class="math-container">\[z = f(x,y) = xy + \sin(x)\]</p><p>with the sub-functions <span>$g(x,y)=xy$</span> and <span>$h(x)=\sin(x)$</span> we get</p><p class="math-container">\[\left.{\frac {df}{dx}}\right|_{x,y}
    = \left.{\frac {df}{dg}}\right|_{g(x,y)}\cdot \left.{\frac {dg}{dx}}\right|_{x,y}
    + \left.{\frac {df}{dh}}\right|_{h(x)}\cdot \left.{\frac {dh}{dx}}\right|_{x}
    = 1 \cdot y |_{y} + 1\cdot\cos(x)|_{x}.\]</p><p>You can see that, in order to implement reverse-mode AD we have to trace and remember all inputs to our intermediate functions during the forward pass such that we can compute their gradients during the backward pass. The simplest way of doing this is by dynamically building a computation graph which tracks how each input variable affects its output variables. The graph below represents the computation of our function <code>f</code>.</p><pre><code class="language-julia hljs">z = x*y + sin(x)

# as a Wengert list   # Partial derivatives
a = x*y               # da/dx = y;     da/dy = x
b = sin(x)            # db/dx = cos(x)
z = a + b             # dz/da = 1;     dz/db = 1</code></pre><p><img src="../graph.png" alt="graph"/></p><p>In the graph you can see that the variable <code>x</code> can directly affect <code>b</code> and <code>a</code>. Hence, <code>x</code> has two children <code>a</code> and <code>b</code>.  During the forward pass we build the graph, keeping track of which input affects which output. Additionally we include the corresponding local derivatives (which we can already compute). To implement a dynamically built graph we can introduce a new number type <code>TrackedReal</code> which has three fields:</p><ul><li><code>data</code> contains the value of this node in the computation graph as obtained in the forward pass.</li><li><code>grad</code> is initialized to <code>nothing</code> and will later hold the accumulated gradients (the sum in the multivariate chain rule)</li><li><code>children</code> is a <code>Dict</code> that keeps track which output variables are affected by the current node and also stores the corresponding local derivatives <span>$\frac{\partial f}{\partial g_k}$</span>.</li></ul><pre><code class="language-julia hljs">mutable struct TrackedReal{T&lt;:Real}
    data::T
    grad::Union{Nothing,T}
    children::Dict
    # this field is only need for printing the graph. you can safely remove it.
    name::String
end

track(x::Real,name=&quot;&quot;) = TrackedReal(x,nothing,Dict(),name)

function Base.show(io::IO, x::TrackedReal)
    t = isempty(x.name) ? &quot;(tracked)&quot; : &quot;(tracked $(x.name))&quot;
    print(io, &quot;$(x.data) $t&quot;)
end</code></pre><p>The backward pass is nothing more than the application of the chainrule. To compute the derivative. Assuming we know how to compute the <em>local derivatives</em> <span>$\frac{\partial f}{\partial g_k}$</span> for simple functions such as <code>+</code>, <code>*</code>, and <code>sin</code>, we can write a simple function that implements the gradient accumulation from above via the chainrule</p><p class="math-container">\[\left.\frac{\partial f}{\partial x_i}\right|_{\bm x} =
    \sum_{k=1}^N \left.\frac{\partial f}{\partial g_k}\right|_{\bm g(\bm x)}
                 \left.\frac{\partial g_k}{\partial x_i}\right|_{\bm x}.\]</p><p>We just have to loop over all children, collect the local derivatives, and recurse:</p><pre><code class="language-julia hljs">function accum!(x::TrackedReal)
    if isnothing(x.grad)
        x.grad = sum(w*accum!(v) for (v,w) in x.children)
    end
    x.grad
end</code></pre><p>where <code>w</code> corresponds to <span>$\frac{\partial f}{\partial g_k}$</span> and <code>accum!(v)</code> corresponds to <span>$\frac{\partial g_k}{\partial x_i}$</span>. At this point we have already implemented the core functionality of our first reverse-mode AD! The only thing left to do is implement the reverse rules for basic functions.  Via recursion the chainrule is applied until we arrive at the final output <code>z</code>.  This final output has to be seeded (just like with forward-mode) with <span>$\frac{\partial z}{\partial z}=1$</span>.</p><h3 id="Writing-Reverse-Rules"><a class="docs-heading-anchor" href="#Writing-Reverse-Rules">Writing Reverse Rules</a><a id="Writing-Reverse-Rules-1"></a><a class="docs-heading-anchor-permalink" href="#Writing-Reverse-Rules" title="Permalink"></a></h3><p>Lets start by overloading the three functions <code>+</code>, <code>*</code>, and <code>sin</code> that we need to build our computation graph. First, we have to track the forward computation and then we <em>register</em> the output <code>z</code> as a child of its inputs by using <code>z</code> as a key in the dictionary of children. The corresponding value holds the  derivatives, in the case of multiplication case we simply have</p><p class="math-container">\[z = a \cdot b\]</p><p>for which the derivatives are</p><p class="math-container">\[\frac{\partial z}{\partial a}=b, \qquad
\frac{\partial z}{\partial b}=a.\]</p><p>Knowing the derivatives of <code>*</code> at a given point we can write our reverse rule</p><pre><code class="language-julia hljs">function Base.:*(a::TrackedReal, b::TrackedReal)
    z = track(a.data * b.data, &quot;*&quot;)
    a.children[z] = b.data  # dz/da=b
    b.children[z] = a.data  # dz/db=a
    z
end</code></pre><p>Creating two tracked numbers and adding them results in</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; x = track(2.0)</code><code class="nohighlight hljs ansi" style="display:block;">2.0 (tracked)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y = track(3.0)</code><code class="nohighlight hljs ansi" style="display:block;">3.0 (tracked)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; z = x*y</code><code class="nohighlight hljs ansi" style="display:block;">6.0 (tracked *)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x.children</code><code class="nohighlight hljs ansi" style="display:block;">Dict{Any, Any} with 1 entry:
  6.0 (tracked *) =&gt; 3.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y.children</code><code class="nohighlight hljs ansi" style="display:block;">Dict{Any, Any} with 1 entry:
  6.0 (tracked *) =&gt; 2.0</code></pre><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Implement the two remaining rules for <code>+</code> and <code>sin</code> by overloading the appropriate methods like we did for <code>*</code>. First you have to compute the tracked forward pass, and then register the local derivatives in the children of your input variables.  Remember to return the tracked result of the forward pass in the end.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><pre><code class="language-julia hljs">function Base.:+(a::TrackedReal{T}, b::TrackedReal{T}) where T
    z = track(a.data + b.data, &quot;+&quot;)
    a.children[z] = one(T)
    b.children[z] = one(T)
    z
end

function Base.sin(x::TrackedReal)
    z = track(sin(x.data), &quot;sin&quot;)
    x.children[z] = cos(x.data)
    z
end</code></pre></p></details><h3 id="Forward-and-Backward-Pass"><a class="docs-heading-anchor" href="#Forward-and-Backward-Pass">Forward &amp; Backward Pass</a><a id="Forward-and-Backward-Pass-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-and-Backward-Pass" title="Permalink"></a></h3><p>To visualize that with reverse-mode AD we really do save computation we can visualize the computation graph at different stages. We start with the forward pass and keep observing <code>x</code></p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; x = track(2.0,&quot;x&quot;);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y = track(3.0,&quot;y&quot;);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; a = x*y;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; print_tree(x)</code><code class="nohighlight hljs ansi" style="display:block;">x data: 2.0 grad: nothing
└─ * data: 6.0 grad: nothing</code></pre><p>We can see that we <code>x</code> now has one child <code>a</code> which has the value <code>2.0*3.0==6.0</code>. All the gradients are still <code>nothing</code>. Computing another value that depends on <code>x</code> will add another child.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; b = sin(x)</code><code class="nohighlight hljs ansi" style="display:block;">0.9092974268256817 (tracked sin)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; print_tree(x)</code><code class="nohighlight hljs ansi" style="display:block;">x data: 2.0 grad: nothing
├─ sin data: 0.91 grad: nothing
└─ * data: 6.0 grad: nothing</code></pre><p>In the final step we compute <code>z</code> which does not mutate the children of <code>x</code> because it does not depend directly on it. The result <code>z</code> is added as a child to both <code>a</code> and <code>b</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; z = a + b</code><code class="nohighlight hljs ansi" style="display:block;">6.909297426825682 (tracked +)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; print_tree(x)</code><code class="nohighlight hljs ansi" style="display:block;">x data: 2.0 grad: nothing
├─ sin data: 0.91 grad: nothing
│  └─ + data: 6.91 grad: nothing
└─ * data: 6.0 grad: nothing
   └─ + data: 6.91 grad: nothing</code></pre><p>For the backward pass we have to seed the initial gradient value of <code>z</code> and call <code>accum!</code> on the variable that we are interested in.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; z.grad = 1.0</code><code class="nohighlight hljs ansi" style="display:block;">1.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; dx = accum!(x)</code><code class="nohighlight hljs ansi" style="display:block;">2.5838531634528574</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; dx ≈ y.data + cos(x.data)</code><code class="nohighlight hljs ansi" style="display:block;">true</code></pre><p>By accumulating the gradients for <code>x</code>, the gradients in the sub-tree connected to <code>x</code> will be evaluated. The parts of the tree that are only connected to <code>y</code> stay untouched.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; print_tree(x)</code><code class="nohighlight hljs ansi" style="display:block;">x data: 2.0 grad: 2.5838531634528574
├─ sin data: 0.91 grad: 1.0
│  └─ + data: 6.91 grad: 1.0
└─ * data: 6.0 grad: 1.0
   └─ + data: 6.91 grad: 1.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; print_tree(y)</code><code class="nohighlight hljs ansi" style="display:block;">y data: 3.0 grad: nothing
└─ * data: 6.0 grad: 1.0
   └─ + data: 6.91 grad: 1.0</code></pre><p>If we now accumulate the gradients over <code>y</code> we re-use the gradients that are already computed. In larger computations this will save us <em>a lot</em> of effort!</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>This also means that we have to re-build the graph for every new set of inputs!</p></div></div><h3 id="Optimizing-2D-Functions"><a class="docs-heading-anchor" href="#Optimizing-2D-Functions">Optimizing 2D Functions</a><a id="Optimizing-2D-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Optimizing-2D-Functions" title="Permalink"></a></h3><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Implement a function <code>gradient(f, args::Real...)</code> which takes a function <code>f</code> and its corresponding arguments (as <code>Real</code> numbers) and outputs the corresponding gradients</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><pre><code class="language-julia hljs">function gradient(f, args::Real...)
    ts = track.(args)
    y  = f(ts...)
    y.grad = 1.0
    accum!.(ts)
end</code></pre></p></details><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; f(x,y) = x*y + sin(x)</code><code class="nohighlight hljs ansi" style="display:block;">f (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; gradient(f, 2.0, 3.0)</code><code class="nohighlight hljs ansi" style="display:block;">(2.5838531634528574, 2.0)</code></pre><p>As an example we can find a local minimum of the function <code>g</code> (slightly modified to show you that we can now actually do <em>automatic</em> differentiation).</p><pre><code class="language-julia hljs">g(x,y) = y*y + sin(x)

using Plots
color_scheme = cgrad(:RdYlBu_5, rev=true)
contour(-4:0.1:4, -2:0.1:2, g, fill=true, c=color_scheme, xlabel=&quot;x&quot;, ylabel=&quot;y&quot;)</code></pre><img src="c71c248d.svg" alt="Example block output"/><p>We can find a local minimum of <span>$g$</span> by starting at an initial point <span>$(x_0,y_0)$</span> and taking small steps in the opposite direction of the gradient</p><p class="math-container">\[\begin{align}
x_{i+1} &amp;= x_i - \lambda \frac{\partial f}{\partial x_i} \\
y_{i+1} &amp;= y_i - \lambda \frac{\partial f}{\partial y_i},
\end{align}\]</p><p>where <span>$\lambda$</span> is the learning rate that has to be tuned manually.</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Implement a function <code>descend</code> performs one step of Gradient Descent (GD) on a function <code>f</code> with an arbitrary number of inputs. For GD you also have to specify the learning rate <span>$\lambda$</span> so the function signature should look like this</p><pre><code class="language-julia hljs">descend(f::Function, λ::Real, args::Real...)</code></pre></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><pre><code class="language-julia hljs">function descend(f::Function, λ::Real, args::Real...)
    Δargs = gradient(f, args...)
    args .- λ .* Δargs
end</code></pre></p></details><p>Running one <code>descend</code> step should result in two new inputs with a smaller output for <code>g</code></p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; g(1.0, 1.0)</code><code class="nohighlight hljs ansi" style="display:block;">1.8414709848078965</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; (x,y) = descend(g, 0.2, 1.0, 1.0)</code><code class="nohighlight hljs ansi" style="display:block;">(0.891939538826372, 0.8)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; g(x,y)</code><code class="nohighlight hljs ansi" style="display:block;">1.4182910542267546</code></pre><p>You can <code>minimize</code> a <code>g</code> starting from an initial value. Below is a code snippet that performs a number of <code>descend</code> steps on two different initial points and creates an animation of each step of the GD algorithm.</p><pre><code class="language-julia hljs">function minimize(f::Function, args::T...; niters=20, λ=0.01) where T&lt;:Real
    paths = ntuple(_-&gt;Vector{T}(undef,niters), length(args))
    for i in 1:niters
        args = descend(f, λ, args...)
        @info f(args...)
        for j in 1:length(args)
            paths[j][i] = args[j]
        end
    end
    paths
end

xs1, ys1 = minimize(g, 1.5, -2.4, λ=0.2, niters=34)
xs2, ys2 = minimize(g, 1.8, -2.4, λ=0.2, niters=16)

p1 = contour(-4:0.1:4, -2:0.1:2, g, fill=true, c=color_scheme, xlabel=&quot;x&quot;, ylabel=&quot;y&quot;)
scatter!(p1, [xs1[1]], [ys1[1]], mc=:black, marker=:star, ms=7, label=&quot;Minimum&quot;)
scatter!(p1, [xs2[1]], [ys2[1]], mc=:black, marker=:star, ms=7, label=false)
scatter!(p1, [-π/2], [0], mc=:red, marker=:star, ms=7, label=&quot;Initial Point&quot;)
scatter!(p1, xs1[1:1], ys1[1:1], mc=:black, label=&quot;GD Path&quot;, xlims=(-4,4), ylims=(-2,2))

@gif for i in 1:max(length(xs1), length(xs2))
    if i &lt;= length(xs1)
        scatter!(p1, xs1[1:i], ys1[1:i], mc=:black, lw=3, xlims=(-4,4), ylims=(-2,2), label=false)
    end
    if i &lt;= length(xs2)
        scatter!(p1, xs2[1:i], ys2[1:i], mc=:black, lw=3, label=false)
    end
    p1
end</code></pre><p><img src="../gd-path.gif" alt="descend"/></p><hr/><p>At this point you can move to the <a href="../hw/#hw08">homework</a> of this lab.  If you want to know how to generalize this simple reverse AD to work with functions that operate on <code>Array</code>s, feel free to continue with the remaining <strong>volutary part of the lab</strong>.</p><hr/><h2 id="Naively-Vectorized-Reverse-AD"><a class="docs-heading-anchor" href="#Naively-Vectorized-Reverse-AD">Naively Vectorized Reverse AD</a><a id="Naively-Vectorized-Reverse-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Naively-Vectorized-Reverse-AD" title="Permalink"></a></h2><p>A naive solution to use our <code>TrackedReal</code> number type to differentiate functions that operate on vectors is to just use <code>Array{&lt;:TrackedReal}</code>. Unfortunately, this means that we have to replace the fast BLAS matrix operations with our own matrix multiplication methods that know how to deal with <code>TrackedReal</code>s.  This results in large performance hits and your task during the rest of the lab is to implement a smarter solution to this problem.</p><pre><code class="language-julia hljs">using LinearAlgebra
Base.zero(::TrackedReal{T}) where T = TrackedReal(zero(T))
LinearAlgebra.adjoint(x::TrackedReal) = x
track(x::Array) = track.(x)
accum!(xs::Array{&lt;:TrackedReal}) = accum!.(xs)

const VecTracked = AbstractVector{&lt;:TrackedReal}
const MatTracked = AbstractMatrix{&lt;:TrackedReal}

LinearAlgebra.dot(xs::VecTracked, ys::VecTracked) = mapreduce(*, +, xs, ys)
Base.:*(X::MatTracked, y::VecTracked) = map(x-&gt;dot(x,y), eachrow(X))
Base.:*(X::MatTracked, Y::MatTracked) = mapreduce(y-&gt;X*y, hcat, eachcol(Y))
Base.sum(xs::AbstractArray{&lt;:TrackedReal}) = reduce(+,xs)

function reset!(x::TrackedReal)
    x.grad = nothing
    reset!.(keys(x.children))
    x.children = Dict()
end

X = rand(2,3)
Y = rand(3,2)

function run()
    Xv = track(X)
    Yv = track(Y)
    z = sum(Xv * Yv)
    z.grad = 1.0
    accum!(Yv)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">run (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">julia&gt; using BenchmarkTools
julia&gt; @benchmark run()
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range (min … max):  44.838 μs …   8.404 ms  ┊ GC (min … max): 0.00% … 98.78%
 Time  (median):     48.680 μs               ┊ GC (median):    0.00%
 Time  (mean ± σ):   53.048 μs ± 142.403 μs  ┊ GC (mean ± σ):  4.61% ±  1.71%

         ▃▆█▃                                                  
  ▂▁▁▂▂▃▆████▇▅▄▄▄▄▄▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂ ▃
  44.8 μs         Histogram: frequency by time         66.7 μs &lt;

 Memory estimate: 26.95 KiB, allocs estimate: 392.</code></pre><h2 id="Reverse-AD-with-TrackedArrays"><a class="docs-heading-anchor" href="#Reverse-AD-with-TrackedArrays">Reverse AD with <code>TrackedArray</code>s</a><a id="Reverse-AD-with-TrackedArrays-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-AD-with-TrackedArrays" title="Permalink"></a></h2><p>To make use of the much faster BLAS methods we have to implement a custom array type which will offload the heavy matrix multiplications to the normal matrix methods.  Start with a <strong>fresh REPL</strong> and possibly a <strong>new file</strong> that only contains the definition of our <code>TrackedReal</code>:</p><pre><code class="language-julia hljs">mutable struct TrackedReal{T&lt;:Real}
    data::T
    grad::Union{Nothing,T}
    children::Dict
end

track(x::Real) = TrackedReal(x, nothing, Dict())</code></pre><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Define a new <code>TrackedArray</code> type which subtypes and <code>AbstractArray{T,N}</code> and contains the three fields: <code>data</code>, <code>grad</code>, and <code>children</code>. Which type should <code>grad</code> have?</p><p>Additionally define <code>track(x::Array)</code>, and forward <code>size</code>, <code>length</code>, and <code>eltype</code> to <code>x.data</code> (maybe via metaprogrammming? ;).</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><pre><code class="language-julia hljs">mutable struct TrackedArray{T,N,A&lt;:AbstractArray{T,N}} &lt;: AbstractArray{T,N}
    data::A
    grad::Union{Nothing,A}
    children::Dict
end

track(x::Array) = TrackedArray(x, nothing, Dict())
track(x::Union{TrackedArray,TrackedReal}) = x

for f in [:size, :length, :eltype]
	eval(:(Base.$(f)(x::TrackedArray, args...) = $(f)(x.data, args...)))
end

# only needed for hashing in the children dict...
Base.getindex(x::TrackedArray, args...) = getindex(x.data,args...)

# pretty print TrackedArray
Base.show(io::IO, x::TrackedArray) = print(io, &quot;Tracked $(x.data)&quot;)
Base.print_array(io::IO, x::TrackedArray) = Base.print_array(io, x.data)</code></pre></p></details><p>Creating a <code>TrackedArray</code> should work like this:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; track(rand(2,2))</code><code class="nohighlight hljs ansi" style="display:block;">2×2 Main.TrackedArray{Float64, 2, Matrix{Float64}}:
 0.522527  0.718882
 0.313791  0.245961</code></pre><pre><code class="language-julia hljs">function accum!(x::Union{TrackedReal,TrackedArray})
    if isnothing(x.grad)
        x.grad = sum(λ(accum!(Δ)) for (Δ,λ) in x.children)
    end
    x.grad
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">accum! (generic function with 1 method)</code></pre><p>To implement the first rule for <code>*</code> i.e. matrix multiplication we would first have to derive it. In the case of general matrix multiplication (which is a function <span>$(R^{N\times M}, R^{M\times L}) \rightarrow R^{N\times L}$</span>) we are not dealing with simple derivatives anymore, but with a so-called <em>pullback</em> which takes a <em>wobble</em> in the output space <span>$R^{N\times L}$</span>  and returns a <em>wiggle</em> in the input space (either <span>$R^{N\times M}$</span> or <span>$R^{M\times L}$</span>).</p><p>Luckily <a href="https://juliadiff.org/ChainRulesCore.jl/dev/arrays.html"><code>ChainRules.jl</code></a> has a nice guide on how to derive array rules, so we will only state the solution for the reverse rule such that you can implement it. They read:</p><p class="math-container">\[\bar A = \bar\Omega B^T, \qquad \bar B = A^T\bar\Omega\]</p><p>Where <span>$\bar\Omega$</span> is the given output <em>wobble</em>, which in the simplest case can be the seeded value of the final node. The crucial problem to note here is that the two rules rely in <span>$\bar\Omega$</span> being multiplied <em>from different sides</em>. This information would be lost if would just store <span>$B^T$</span> as the pullback for <span>$A$</span>.  Hence we will store our pullbacks as closures:</p><pre><code class="language-julia hljs">Ω̄ -&gt; Ω̄  * B&#39;
Ω̄ -&gt; A&#39; * Ω̄</code></pre><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Define the pullback for matrix multiplication i.e. <code>Base.:*(A::TrackedArray,B::TrackedArray)</code> by computing the primal and storing the partials as closures.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><pre><code class="language-julia hljs">function Base.:*(X::TrackedArray, Y::TrackedArray)
    Z = track(X.data * Y.data)
    X.children[Z] = Δ -&gt; Δ * Y.data&#39;
    Y.children[Z] = Δ -&gt; X.data&#39; * Δ
    Z
end</code></pre></p></details><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; X = rand(2,3) |&gt; track</code><code class="nohighlight hljs ansi" style="display:block;">2×3 Main.TrackedArray{Float64, 2, Matrix{Float64}}:
 0.865375  0.42931   0.380619
 0.823463  0.833136  0.635136</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Y = rand(3,2) |&gt; track</code><code class="nohighlight hljs ansi" style="display:block;">3×2 Main.TrackedArray{Float64, 2, Matrix{Float64}}:
 0.13503   0.947074
 0.404891  0.700818
 0.399567  0.651617</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Z = X*Y</code><code class="nohighlight hljs ansi" style="display:block;">2×2 Main.TrackedArray{Float64, 2, Matrix{Float64}}:
 0.442758  1.36846
 0.7023    1.77762</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; f = X.children[Z]</code><code class="nohighlight hljs ansi" style="display:block;">#3 (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Ω̄ = ones(size(Z)...)</code><code class="nohighlight hljs ansi" style="display:block;">2×2 Matrix{Float64}:
 1.0  1.0
 1.0  1.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; f(Ω̄)</code><code class="nohighlight hljs ansi" style="display:block;">2×3 Matrix{Float64}:
 1.0821  1.10571  1.05118
 1.0821  1.10571  1.05118</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Ω̄*Y.data&#39;</code><code class="nohighlight hljs ansi" style="display:block;">2×3 Matrix{Float64}:
 1.0821  1.10571  1.05118
 1.0821  1.10571  1.05118</code></pre><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise</header>
<div class="admonition-body"><p>Implement rules for <code>sum</code>, <code>+</code>, <code>-</code>, and  <code>abs2</code>.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p></p></details><pre><code class="language-julia hljs">function Base.sum(x::TrackedArray)
    z = track(sum(x.data))
    x.children[z] = Δ -&gt; Δ*ones(eltype(x), size(x)...)
    z
end

function Base.:+(X::TrackedArray, Y::TrackedArray)
    Z = track(X.data + Y.data)
    X.children[Z] = Δ -&gt; Δ
    Y.children[Z] = Δ -&gt; Δ
    Z
end

function Base.:-(X::TrackedArray, Y::TrackedArray)
    Z = track(X.data - Y.data)
    X.children[Z] = Δ -&gt; Δ
    Y.children[Z] = Δ -&gt; -Δ
    Z
end

function Base.abs2(x::TrackedArray)
    y = track(abs2.(x.data))
    x.children[y] = Δ -&gt; Δ .* 2x.data
    y
end</code></pre><pre><code class="language-julia hljs">X = rand(2,3)
Y = rand(3,2)
function run()
    Xv = track(X)
    Yv = track(Y)
    z = sum(Xv * Yv)
    z.grad = 1.0
    accum!(Yv)
end</code></pre><pre><code class="language-julia hljs">julia&gt; using BenchmarkTools
julia&gt; @benchmark run()
BenchmarkTools.Trial: 10000 samples with 6 evaluations.
 Range (min … max):  5.797 μs …  1.618 ms  ┊ GC (min … max): 0.00% … 98.97%
 Time  (median):     6.530 μs              ┊ GC (median):    0.00%
 Time  (mean ± σ):   7.163 μs ± 22.609 μs  ┊ GC (mean ± σ):  4.42% ±  1.40%

   ▆█▇▇▇▆▅▄▃▃▂▂▂▁▁ ▁▁                                        ▂
  █████████████████████▇▇▇▆▆▅▅▅▅▆▅▄▅▅▄▁▃▁▁▄▁▃▁▁▁▃▃▄▁▁▁▄▁▃▁▅▄ █
  5.8 μs       Histogram: log(frequency) by time     15.8 μs &lt;

 Memory estimate: 3.08 KiB, allocs estimate: 31.</code></pre><p>Even for this tiny example we are already 10 times faster than with the naively vectorized approach!</p><p>In order to implement a full neural network we need two more rules. One for the non-linearity and one for concatentation of individual training points to a batch.</p><pre><code class="language-julia hljs">σ(x::Real) = 1/(1+exp(-x))
σ(x::AbstractArray) = σ.(x)
function σ(x::TrackedArray)
    z = track(σ(x.data))
    d = z.data
    x.children[z] = Δ -&gt; Δ .* d .* (1 .- d)
    z
end

function Base.hcat(xs::TrackedArray...)
    y  = track(hcat(data.(xs)...))
    stops  = cumsum([size(x,2) for x in xs])
    starts = vcat([1], stops[1:end-1] .+ 1)
    for (start,stop,x) in zip(starts,stops,xs)
        x.children[y] = function (Δ)
            δ = if ndims(x) == 1
                Δ[:,start]
            else
                ds = map(_ -&gt; :, size(x)) |&gt; Base.tail |&gt; Base.tail
                Δ[:, start:stop, ds...]
            end
            δ
        end
    end
    y
end</code></pre><p>You can see a full implementation of our tracing based AD <a href="https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/master/src/ReverseDiff.jl">here</a> and a simple implementation of a Neural Network that can learn an approximation to the function <code>g</code> <a href="https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/master/docs/src/lecture_08/reverse-nn.jl">here</a>. Running the latter script will produce an animation that shows how the network is learning.</p><p><img src="../anim.gif" alt="anim"/></p><p>This lab is heavily inspired by <a href="https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation">Rufflewind</a></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../lecture/">« Lecture</a><a class="docs-footer-nextpage" href="../hw/">Homework »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Thursday 21 December 2023 15:58">Thursday 21 December 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
