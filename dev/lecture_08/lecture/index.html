<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Lecture · Scientific Programming in Julia</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_08/lecture/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Scientific Programming in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Scientific Programming in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Programming in Julia</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation/">Installation</a></li><li><a class="tocitem" href="../../projects/">Projects</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Introduction</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/motivation/">Motivation</a></li><li><a class="tocitem" href="../../lecture_01/basics/">Basics</a></li><li><a class="tocitem" href="../../lecture_01/demo/">Examples</a></li><li><a class="tocitem" href="../../lecture_01/outline/">Outline</a></li><li><a class="tocitem" href="../../lecture_01/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_01/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: The power of Type System &amp; multiple dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_02/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_02/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Design patterns</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_03/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_03/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages development, Unit Tests &amp; CI</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_04/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_04/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Benchmarking, profiling, and performance gotchas</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_05/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_05/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Language introspection</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_06/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_06/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Macros</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_07/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_07/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox" checked/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Introduction to automatic differentiation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Lecture</a><ul class="internal"><li><a class="tocitem" href="#Motivation"><span>Motivation</span></a></li><li><a class="tocitem" href="#Theory"><span>Theory</span></a></li><li><a class="tocitem" href="#Calculation-of-the-Forward-mode"><span>Calculation of the Forward mode</span></a></li><li><a class="tocitem" href="#Reverse-mode"><span>Reverse mode</span></a></li><li><a class="tocitem" href="#Implementation-details-of-reverse-AD"><span>Implementation details of reverse AD</span></a></li><li><a class="tocitem" href="#ChainRules"><span>ChainRules</span></a></li><li><a class="tocitem" href="#Source-to-source-AD-using-Wengert"><span>Source-to-source AD using Wengert</span></a></li><li><a class="tocitem" href="#ChainRules-2"><span>ChainRules</span></a></li></ul></li><li><a class="tocitem" href="../lab/">Lab</a></li><li><a class="tocitem" href="../hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Manipulating intermediate representation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_09/lab/">Lab</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Different levels of parallel programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_10/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_10/hw/">Homework</a></li></ul></li><li><span class="tocitem">11: Julia for GPU programming</span></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Uncertainty propagation in ODE</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/lecture/">Lecture</a></li></ul></li><li><span class="tocitem">13: Learning ODE from data</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">8: Introduction to automatic differentiation</a></li><li class="is-active"><a href>Lecture</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Lecture</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/master/docs/src/lecture_08/lecture.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Automatic-Differentiation"><a class="docs-heading-anchor" href="#Automatic-Differentiation">Automatic Differentiation</a><a id="Automatic-Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation" title="Permalink"></a></h1><h2 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h2><ul><li>It supports a lot of modern machine learning by allowing quick differentiation of complex mathematical functions. The 1st order optimization methods are ubiquitous in finding parameters of functions (not only in  deep learning).</li><li>AD is interesting to study from the implementation perspective. There are different takes on it with different trade-offs and Julia offers many implementations (some of them are not maintained anymore).</li><li>We (authors of this course) believe that it is good to understand (at least roughly), how the methods work in order to use them effectively in our work.</li><li>Julia is unique in the effort separating definitions of AD rules from AD engines that use those rules to perform the AD. This allows authors of generic libraries to add new rules that would be compatible with many frameworks. See <a href="https://juliadiff.org/">juliadiff.org</a> for a list.</li></ul><h2 id="Theory"><a class="docs-heading-anchor" href="#Theory">Theory</a><a id="Theory-1"></a><a class="docs-heading-anchor-permalink" href="#Theory" title="Permalink"></a></h2><p>The differentiation is routine process, as most of the time we break complicated functions down into small pieces that we know, how to differentiate and from that to assemble the gradient of the complex function back. Thus, the essential piece is the differentiation of the composed function <span>$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span></p><p class="math-container">\[f(x) = f_1(f_2(f_3(\ldots f_n(x)))) = (f_1 \circ f_2 \circ \ldots \circ f_n)(x)\]</p><p>which is computed by chainrule. Before we dive into the details, let&#39;s define the notation, which for the sake of clarity needs to be precise. The gradient of function <span>$f(x)$</span> with respect to <span>$x$</span> at point <span>$x_0$</span> will be denoted as  <span>$\left.\frac{\partial f}{\partial x}\right|_{x^0}$</span></p><p>For a composed function <span>$f(x)$</span> the gradient with respect to <span>$x$</span> at point <span>$x_0$</span> is equal to</p><p class="math-container">\[\left.\frac{\partial f}{\partial x}\right|_{x^0} = \left.\frac{f_1}{\partial y_1}\right|_{y_1^0} \times \left.\frac{f_2}{\partial y_2}\right|_{y_2^0} \times \ldots \times \left.\frac{f_n}{\partial y_n}\right|_{y_n^0},\]</p><p>where <span>$y_i$</span> denotes the input of function <span>$f_i$</span> and</p><p class="math-container">\[\begin{alignat*}{2}
y_i^0 = &amp;\ \left(f_{i+1} \circ \ldots \circ f_n\right) (x^0) \\
y_n^0 = &amp;\ x^0 \\
y_0^0 = &amp;\ f(x^0) \\
\end{alignat*}\]</p><p>How <span>$\left.\frac{f_i}{\partial y_i}\right|_{y_i^0}$</span> looks like? </p><ul><li>If <span>$f_i: \mathbb{R} \rightarrow \mathbb{R}$</span>, then <span>$\frac{f_i}{\partial y_i} \in \mathbb{R}$</span> is a real number <span>$\mathbb{R}$</span> and we live in a high-school world, where it was sufficient to multiply real numbers.</li><li>If <span>$f_i: \mathbb{R}^{m_i} \rightarrow \mathbb{R}^{n_i}$</span>, then <span>$\mathbf{J}_i = \left.\frac{f_i}{\partial y_i}\right|_{y_i^0} \in \mathbb{R}^{n_i,m_i}$</span> is a matrix with <span>$m_i$</span> rows and <span>$n_i$</span> columns. </li></ul><p>The computation of gradient <span>$\frac{\partial f}{\partial x}$</span> <em>theoretically</em> boils down to </p><ol><li>computing Jacobians <span>$\left\{\mathbf{J}_i\right\}_{i=1}^n$</span> </li><li>multiplication of Jacobians as it holds that <span>$\left.\frac{\partial f}{\partial x}\right|_{y_0} = J_1 \times J_2 \times \ldots \times J_n$</span>. </li></ol><p>The complexity of the computation (at least one part of it) is therefore therefore determined by the  Matrix multiplication, which is generally expensive, as theoretically it has complexity at least <span>$O(n^{2.3728596}),$</span> but in practice a little bit more as the lower bound hides the devil in the <span>$O$</span> notation. The order in which the Jacobians are multiplied has therefore a profound effect on the complexity of the AD engine. While determining the optimal order of multiplication of sequence of matrices is costly, in practice, we recognize two important cases.</p><ol><li>Jacobians are multiplied from right to left as  <span>$J_1 \times (J_2 \times ( \ldots \times (J_{n-1} \times J_n) \ldots))$</span> which has the advantage when the input dimension of <span>$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span> is smaller than the output dimension, <span>$n &lt; m$</span>. - referred to as the <strong>FORWARD MODE</strong></li><li>Jacobians are multiplied from left to right as <span>$( \ldots ((J_1 \times J_2) \times J_3) \times \ldots ) \times J_n$</span> which has the advantage when the input dimension of <span>$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span> is larger than the output dimension, <span>$n &gt; m$</span>. - referred to as the <strong>BACKWARD MODE</strong></li></ol><p>The ubiquitous in machine learning to minimization of a scalar (loss) function of a large number of parameters. Also notice that for <code>f</code> of certain structures, it pays-off to do a mixed-mode AD, where some parts are done using forward diff and some parts using reverse diff. </p><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><p>Let&#39;s workout an example</p><p class="math-container">\[z = xy + sin(x)\]</p><p>How it maps to the notation we have used above? Particularly, what are <span>$f_1, f_2, \ldots, f_n$</span> and the corresponding <span>$\{y_i\}_{i=1}^n$</span>, such that <span>$(f_1 \circ f_2 \circ \ldots \circ f_n)(x,y) = xy + sin(x)$</span> ?</p><p class="math-container">\[\begin{alignat*}{6}
f_1:&amp;\mathbb{R}^2 \rightarrow \mathbb{R} \quad&amp;f_1(y_1)&amp; = y_{1,1} + y_{1,2}            \quad &amp; y_0 = &amp; (xy + \sin(x))           \\
f_2:&amp;\mathbb{R}^3 \rightarrow \mathbb{R}^2 \quad&amp;f_2(y_2)&amp; = (y_{2,1}y_{1,2}, y_{2,3}) \quad &amp; y_1 = &amp;  (xy, \sin(x))&amp;\\
f_3:&amp; \mathbb{R}^2 \rightarrow \mathbb{R}^3 \quad&amp;f_3(y_3)&amp; = (y_{3,1}, y_{3,2}, \sin(y_{3,1}))  \quad &amp; y_2 =&amp; (x, y, \sin(x))\\
\end{alignat*}\]</p><p>The corresponding jacobians are </p><p class="math-container">\[\begin{alignat*}{4}
f_1(y_1) &amp; = y_{1,1} + y_{1,2}             \quad &amp; \mathbf{J}_1&amp; = \begin{bmatrix} 1 \\ 1 \end{bmatrix}  \\
f_2(y_2) &amp; = (y_{2,1}y_{1,2}, y_{2,3})     \quad &amp; \mathbf{J}_2&amp; = \begin{bmatrix} y_{2, 2} &amp; 0 \\ y_{2,1} &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\\
f_3(y_3) &amp; = (y_{3,1}, y_{3,2}, \sin(y_{3,1}))     \quad &amp; \mathbf{J}_3 &amp; = \begin{bmatrix} 1 &amp; 0 &amp; \cos(y_{3,1}) \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \\
\end{alignat*}\]</p><p>and for the gradient it holds that</p><p class="math-container">\[\begin{bmatrix} \frac{\partial f(x, y)}{\partial{x}} \\ \frac{\partial f(x,y)}{\partial{y}} \end{bmatrix} = \mathbf{J}_3 \times \mathbf{J}_2 \times \mathbf{J}_1 =  \begin{bmatrix} 1 &amp; 0 &amp; \cos(x) \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \\  \times \begin{bmatrix} y &amp; 0 \\ x &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \times \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} y &amp; \cos(x) \\ x &amp; 0 \end{bmatrix} \times \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} y + \cos(x) \\ x \end{bmatrix}\]</p><p>Note that from theoretical point of view this decomposition of a function is not unique, however as we will see later it usually given by the computational graph in a particular language/environment.</p><h2 id="Calculation-of-the-Forward-mode"><a class="docs-heading-anchor" href="#Calculation-of-the-Forward-mode">Calculation of the Forward mode</a><a id="Calculation-of-the-Forward-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Calculation-of-the-Forward-mode" title="Permalink"></a></h2><p>In theory, we can calculate the gradient using forward mode as follows Initialize the Jacobian of <span>$y_n$</span> with respect to <span>$x$</span> to an identity matrix, because as we have stated above <span>$y^0_n = x$</span>, i.e. <span>$\frac{\partial y_n}{\partial x} = \mathbb{I}$</span>. Iterate <code>i</code> from <code>n</code> down to <code>1</code> as</p><ul><li>calculate the next intermediate output as <span>$y^0_{i-1} = f_i({y^0_i})$</span> </li><li>calculate Jacobian <span>$J_i = \left.\frac{f_i}{\partial y_i}\right|_{y^0_i}$</span></li><li><em>push forward</em> the gradient as <span>$\left.\frac{\partial y_{i-1}}{\partial x}\right|_x = J_i \times \left.\frac{\partial y_n}{\partial x}\right|_x$</span></li></ul><p>Notice that </p><ul><li>on the very end, we are left with <span>$y = y^0_0$</span> and with <span>$\frac{\partial y_0}{\partial x}$</span>, which is the gradient we wanted to calculate;</li><li>if <code>y</code> is a scalar, then <span>$\frac{\partial y_0}{\partial x}$</span> is a matrix with single row</li><li>the Jacobian and the output of the function is calculated in one sweep.</li></ul><p>The above is an idealized computation. The real implementation is a bit different, as we will see later.</p><h3 id="Implementation-of-the-forward-mode-using-Dual-numbers"><a class="docs-heading-anchor" href="#Implementation-of-the-forward-mode-using-Dual-numbers">Implementation of the forward mode using Dual numbers</a><a id="Implementation-of-the-forward-mode-using-Dual-numbers-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-of-the-forward-mode-using-Dual-numbers" title="Permalink"></a></h3><p>Forward modes need to keep track of the output of the function and of the derivative at each computation step in the computation of the complicated function <span>$f$</span>. This can be elegantly realized with a <a href="https://en.wikipedia.org/wiki/Dual_number"><strong>dual number</strong></a>, which are conceptually similar to complex numbers, but instead of the imaginary number <span>$i$</span> dual numbers use <span>$\epsilon$</span> in its second component:</p><p class="math-container">\[x = v + \dot v \epsilon,\]</p><p>where <span>$(v,\dot v) \in \mathbb R$</span> and by definition <span>$\epsilon^2=0$</span> (instead of <span>$i^2=-1$</span> in complex numbers). What are the properties of these Dual numbers?</p><p class="math-container">\[\begin{align}
(v + \dot v \epsilon) + (u + \dot u \epsilon) &amp;= (v + u) + (\dot v + \dot u)\epsilon  \\
(v + \dot v \epsilon)(u + \dot u \epsilon) &amp;= vu + (u\dot v + \dot u v)\epsilon + \dot v \dot u \epsilon^2 = vu + (u\dot v + \dot u v)\epsilon \\
\frac{v + \dot v \epsilon}{u + \dot u \epsilon} &amp;= \frac{v + \dot v \epsilon}{u + \dot u \epsilon} \frac{u - \dot u \epsilon}{u - \dot u \epsilon} = \frac{v}{u} - \frac{(\dot u v - u \dot v)\epsilon}{u^2}
\end{align}\]</p><h4 id="How-are-dual-numbers-related-to-differentiation?"><a class="docs-heading-anchor" href="#How-are-dual-numbers-related-to-differentiation?">How are dual numbers related to differentiation?</a><a id="How-are-dual-numbers-related-to-differentiation?-1"></a><a class="docs-heading-anchor-permalink" href="#How-are-dual-numbers-related-to-differentiation?" title="Permalink"></a></h4><p>Let&#39;s evaluate the above equations at <span>$(v, \dot v) = (v, 1)$</span> and <span>$(u, \dot u) = (u, 0)$</span> we obtain </p><p class="math-container">\[\begin{align}
(v + \dot v \epsilon) + (u + \dot u \epsilon) &amp;= (v + u) + 1\epsilon  \\
(v + \dot v \epsilon)(u + \dot u \epsilon) &amp;= vu + u\epsilon\\
\frac{v + \dot v \epsilon}{u + \dot u \epsilon} &amp;= \frac{v}{u}  + \frac{1}{u} \epsilon
\end{align}\]</p><p>and notice that terms <span>$(1, u, \frac{1}{u})$</span> corresponds to gradient of functions <span>$(u+v, uv, \frac{v}{u})$</span> with respect to <span>$v$</span>. We can repeat it with changed values of <span>$\epsilon$</span> as <span>$(v, \dot v) = (v, 0)$</span> and <span>$(u, \dot u) = (u, 1)$</span> and we obtain</p><p class="math-container">\[\begin{align}
(v + \dot v \epsilon) + (u + \dot u \epsilon) &amp;= (v + u) + 1\epsilon  \\
(v + \dot v \epsilon)(u + \dot u \epsilon) &amp;= vu + v\epsilon\\
\frac{v + \dot v \epsilon}{u + \dot u \epsilon} &amp;= \frac{v}{u}  - \frac{v}{u^2} \epsilon
\end{align}\]</p><p>meaning that at this moment we have obtained gradients with respect to <span>$u$</span>.</p><p>All above functions <span>$(u+v, uv, \frac{u}{v})$</span> are of <span>$\mathbb{R}^2 \rightarrow \mathbb{R}$</span>, therefore we had to repeat the calculations twice to get gradients with respect to both inputs. This is inline with the above theory, where we have said that if input dimension is larger then output dimension, the backward mode is better. But consider a case, where we have a function </p><p class="math-container">\[f(v) = (v + 5, 5*v, 5 / v) \]</p><p>which is <span>$\mathbb{R} \rightarrow \mathbb{R}^3$</span>. In this case, we obtain the Jacobian <span>$[1, 5, -\frac{5}{v^2}]$</span> in a single forward pass (whereas the reverse would require three passes over the backward calculation, as will be seen later).</p><h4 id="Does-dual-numbers-work-universally?"><a class="docs-heading-anchor" href="#Does-dual-numbers-work-universally?">Does dual numbers work universally?</a><a id="Does-dual-numbers-work-universally?-1"></a><a class="docs-heading-anchor-permalink" href="#Does-dual-numbers-work-universally?" title="Permalink"></a></h4><p>Let&#39;s first work out polynomial. Let&#39;s assume the polynomial</p><p class="math-container">\[p(v) = \sum_{i=1}^n p_iv^i\]</p><p>and compute its value at <span>$v + \dot v \epsilon$</span> (note that we know how to do addition and multiplication)</p><p class="math-container">\[\begin{split}
p(v) &amp;= 
    \sum_{i=0}^n p_i(v + \dot{v} \epsilon )^i = 
    \sum_{i=0}^n \left[p_i \sum_{j=0}^{n}\binom{i}{j}v^{i-j}(\dot v \epsilon)^{i}\right] = 
    p_0 + \sum_{i=1}^n \left[p_i \sum_{j=0}^{1}\binom{i}{j}v^{i-j}(\dot v \epsilon)^{j}\right] = \\
    &amp;= p_0 + \sum_{i=1}^n p_i(v^i + i v^{i-1} \dot v \epsilon ) 
    = p(v) + \left(\sum_{i=1}^n ip_i v^{i-1}\right) \dot v \epsilon
\end{split}\]</p><p>where in the multiplier of <span>$\dot{v} \epsilon$</span>: <span>$\sum_{i=1}^n ip_i v^{i - 1}$</span>, we recognize the derivative of <span>$p(v)$</span> with respect to <span>$v$</span>. This proves that Dual numbers can be used to calculate the gradient of polynomials.</p><p>Let&#39;s now consider a general function <span>$f:\mathbb{R} \rightarrow \mathbb{R}$</span>. Its value at point <span>$v + \dot v \epsilon$</span> can be approximated using Taylor expansion at function at point <span>$v$</span> as</p><p class="math-container">\[f(v+\dot v \epsilon) = \sum_{i=0}^\infty \frac{f^i(v)\dot v^i\epsilon^n}{i!}
  = f(v) + f&#39;(v)\dot v\epsilon,\]</p><p>where all higher order terms can be dropped because <span>$\epsilon^i=0$</span> for <span>$i&gt;1$</span>. This shows that we can calculate the gradient of <span>$f$</span> at point <span>$v$</span> by calculating its value at <span>$f(v + \epsilon)$</span> and taking the multiplier of <span>$\epsilon$</span>.</p><h4 id="Implementing-Dual-number-with-Julia"><a class="docs-heading-anchor" href="#Implementing-Dual-number-with-Julia">Implementing Dual number with Julia</a><a id="Implementing-Dual-number-with-Julia-1"></a><a class="docs-heading-anchor-permalink" href="#Implementing-Dual-number-with-Julia" title="Permalink"></a></h4><p>To demonstrate the simplicity of Dual numbers, consider following definition of Dual numbers, where we define a new number type and overload functions <code>+</code>, <code>-</code>, <code>*</code>, and <code>/</code>.  In Julia, this reads:</p><pre><code class="language-julia hljs">struct Dual{T&lt;:Number} &lt;: Number
    x::T
    d::T
end

Base.:+(a::Dual, b::Dual)   = Dual(a.x+b.x, a.d+b.d)
Base.:-(a::Dual, b::Dual)   = Dual(a.x-b.x, a.d-b.d)
Base.:/(a::Dual, b::Dual)   = Dual(a.x/b.x, (a.d*b.x - a.x*b.d)/b.x^2) # recall  (a/b) =  a/b + (a&#39;b - ab&#39;)/b^2 ϵ
Base.:*(a::Dual, b::Dual)   = Dual(a.x*b.x, a.d*b.x + a.x*b.d)

# Let&#39;s define some promotion rules
Dual(x::S, d::T) where {S&lt;:Number, T&lt;:Number} = Dual{promote_type(S, T)}(x, d)
Dual(x::Number) = Dual(x, zero(typeof(x)))
Dual{T}(x::Number) where {T} = Dual(T(x), zero(T))
Base.promote_rule(::Type{Dual{T}}, ::Type{S}) where {T&lt;:Number,S&lt;:Number} = Dual{promote_type(T,S)}
Base.promote_rule(::Type{Dual{T}}, ::Type{Dual{S}}) where {T&lt;:Number,S&lt;:Number} = Dual{promote_type(T,S)}

# and define api for forward differentionation
forward_diff(f::Function, x::Real) = _dual(f(Dual(x,1.0)))
_dual(x::Dual) = x.d
_dual(x::Vector) = _dual.(x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">_dual (generic function with 2 methods)</code></pre><p>And let&#39;s test the <strong><em>Babylonian Square Root</em></strong> (an algorithm to compute <span>$\sqrt x$</span>):</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; babysqrt(x, t=(1+x)/2, n=10) = n==0 ? t : babysqrt(x, (t+x/t)/2, n-1)</code><code class="nohighlight hljs ansi" style="display:block;">babysqrt (generic function with 3 methods)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_diff(babysqrt, 2)</code><code class="nohighlight hljs ansi" style="display:block;">0.35355339059327373</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_diff(x -&gt; [1 + x, 5x, 5/x], 2)</code><code class="nohighlight hljs ansi" style="display:block;">3-element Vector{Float64}:
  1.0
  5.0
 -1.25</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_diff(babysqrt, 2) ≈ 1/(2sqrt(2))</code><code class="nohighlight hljs ansi" style="display:block;">true</code></pre><p>We now compare the analytic solution to values computed by the <code>forward_diff</code> and byt he finite differencing</p><p class="math-container">\[f(x) = \sqrt{x} \qquad f&#39;(x) = \frac{1}{2\sqrt{x}}\]</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using FiniteDifferences</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: ArgumentError: Package FiniteDifferences not found in current path:
- Run `import Pkg; Pkg.add(&quot;FiniteDifferences&quot;)` to install the FiniteDifferences package.</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_dsqrt(x) = forward_diff(babysqrt,x)</code><code class="nohighlight hljs ansi" style="display:block;">forward_dsqrt (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; analytc_dsqrt(x) = 1/(2babysqrt(x))</code><code class="nohighlight hljs ansi" style="display:block;">analytc_dsqrt (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_dsqrt(2.0)</code><code class="nohighlight hljs ansi" style="display:block;">0.35355339059327373</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; analytc_dsqrt(2.0)</code><code class="nohighlight hljs ansi" style="display:block;">0.3535533905932738</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; central_fdm(5, 1)(babysqrt, 2.0)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: central_fdm not defined</code></pre><pre><code class="language-julia hljs">plot(0.0:0.01:2, babysqrt, label=&quot;f(x) = babysqrt(x)&quot;, lw=3)
plot!(0.1:0.01:2, analytc_dsqrt, label=&quot;Analytic f&#39;&quot;, ls=:dot, lw=3)
plot!(0.1:0.01:2, forward_dsqrt, label=&quot;Dual Forward Mode f&#39;&quot;, lw=3, ls=:dash)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip980">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip980)" d="
M0 1600 L2400 1600 L2400 0 L0 0  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip981">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip980)" d="
M156.112 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.112 47.2441  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip982">
    <rect x="156" y="47" width="2198" height="1440"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip982)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  218.281,1486.45 218.281,47.2441 
  "/>
<polyline clip-path="url(#clip982)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  736.358,1486.45 736.358,47.2441 
  "/>
<polyline clip-path="url(#clip982)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1254.43,1486.45 1254.43,47.2441 
  "/>
<polyline clip-path="url(#clip982)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1772.51,1486.45 1772.51,47.2441 
  "/>
<polyline clip-path="url(#clip982)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  2290.59,1486.45 2290.59,47.2441 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,1486.45 2352.76,1486.45 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  218.281,1486.45 218.281,1467.55 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  736.358,1486.45 736.358,1467.55 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1254.43,1486.45 1254.43,1467.55 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1772.51,1486.45 1772.51,1467.55 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  2290.59,1486.45 2290.59,1467.55 
  "/>
<path clip-path="url(#clip980)" d="M195.666 1517.37 Q192.055 1517.37 190.226 1520.93 Q188.42 1524.47 188.42 1531.6 Q188.42 1538.71 190.226 1542.27 Q192.055 1545.82 195.666 1545.82 Q199.3 1545.82 201.105 1542.27 Q202.934 1538.71 202.934 1531.6 Q202.934 1524.47 201.105 1520.93 Q199.3 1517.37 195.666 1517.37 M195.666 1513.66 Q201.476 1513.66 204.531 1518.27 Q207.61 1522.85 207.61 1531.6 Q207.61 1540.33 204.531 1544.94 Q201.476 1549.52 195.666 1549.52 Q189.856 1549.52 186.777 1544.94 Q183.721 1540.33 183.721 1531.6 Q183.721 1522.85 186.777 1518.27 Q189.856 1513.66 195.666 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M215.828 1542.97 L220.712 1542.97 L220.712 1548.85 L215.828 1548.85 L215.828 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M240.897 1517.37 Q237.286 1517.37 235.457 1520.93 Q233.652 1524.47 233.652 1531.6 Q233.652 1538.71 235.457 1542.27 Q237.286 1545.82 240.897 1545.82 Q244.531 1545.82 246.337 1542.27 Q248.165 1538.71 248.165 1531.6 Q248.165 1524.47 246.337 1520.93 Q244.531 1517.37 240.897 1517.37 M240.897 1513.66 Q246.707 1513.66 249.763 1518.27 Q252.841 1522.85 252.841 1531.6 Q252.841 1540.33 249.763 1544.94 Q246.707 1549.52 240.897 1549.52 Q235.087 1549.52 232.008 1544.94 Q228.953 1540.33 228.953 1531.6 Q228.953 1522.85 232.008 1518.27 Q235.087 1513.66 240.897 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M714.24 1517.37 Q710.629 1517.37 708.8 1520.93 Q706.994 1524.47 706.994 1531.6 Q706.994 1538.71 708.8 1542.27 Q710.629 1545.82 714.24 1545.82 Q717.874 1545.82 719.68 1542.27 Q721.508 1538.71 721.508 1531.6 Q721.508 1524.47 719.68 1520.93 Q717.874 1517.37 714.24 1517.37 M714.24 1513.66 Q720.05 1513.66 723.105 1518.27 Q726.184 1522.85 726.184 1531.6 Q726.184 1540.33 723.105 1544.94 Q720.05 1549.52 714.24 1549.52 Q708.43 1549.52 705.351 1544.94 Q702.295 1540.33 702.295 1531.6 Q702.295 1522.85 705.351 1518.27 Q708.43 1513.66 714.24 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M734.402 1542.97 L739.286 1542.97 L739.286 1548.85 L734.402 1548.85 L734.402 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M749.517 1514.29 L767.874 1514.29 L767.874 1518.22 L753.8 1518.22 L753.8 1526.7 Q754.818 1526.35 755.837 1526.19 Q756.855 1526 757.874 1526 Q763.661 1526 767.04 1529.17 Q770.42 1532.34 770.42 1537.76 Q770.42 1543.34 766.948 1546.44 Q763.476 1549.52 757.156 1549.52 Q754.98 1549.52 752.712 1549.15 Q750.466 1548.78 748.059 1548.04 L748.059 1543.34 Q750.142 1544.47 752.365 1545.03 Q754.587 1545.58 757.064 1545.58 Q761.068 1545.58 763.406 1543.48 Q765.744 1541.37 765.744 1537.76 Q765.744 1534.15 763.406 1532.04 Q761.068 1529.94 757.064 1529.94 Q755.189 1529.94 753.314 1530.35 Q751.462 1530.77 749.517 1531.65 L749.517 1514.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1221.59 1544.91 L1229.23 1544.91 L1229.23 1518.55 L1220.92 1520.21 L1220.92 1515.95 L1229.18 1514.29 L1233.86 1514.29 L1233.86 1544.91 L1241.49 1544.91 L1241.49 1548.85 L1221.59 1548.85 L1221.59 1544.91 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1250.94 1542.97 L1255.82 1542.97 L1255.82 1548.85 L1250.94 1548.85 L1250.94 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1276.01 1517.37 Q1272.4 1517.37 1270.57 1520.93 Q1268.76 1524.47 1268.76 1531.6 Q1268.76 1538.71 1270.57 1542.27 Q1272.4 1545.82 1276.01 1545.82 Q1279.64 1545.82 1281.45 1542.27 Q1283.28 1538.71 1283.28 1531.6 Q1283.28 1524.47 1281.45 1520.93 Q1279.64 1517.37 1276.01 1517.37 M1276.01 1513.66 Q1281.82 1513.66 1284.87 1518.27 Q1287.95 1522.85 1287.95 1531.6 Q1287.95 1540.33 1284.87 1544.94 Q1281.82 1549.52 1276.01 1549.52 Q1270.2 1549.52 1267.12 1544.94 Q1264.06 1540.33 1264.06 1531.6 Q1264.06 1522.85 1267.12 1518.27 Q1270.2 1513.66 1276.01 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1740.16 1544.91 L1747.8 1544.91 L1747.8 1518.55 L1739.49 1520.21 L1739.49 1515.95 L1747.75 1514.29 L1752.43 1514.29 L1752.43 1544.91 L1760.07 1544.91 L1760.07 1548.85 L1740.16 1548.85 L1740.16 1544.91 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1769.51 1542.97 L1774.4 1542.97 L1774.4 1548.85 L1769.51 1548.85 L1769.51 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1784.63 1514.29 L1802.98 1514.29 L1802.98 1518.22 L1788.91 1518.22 L1788.91 1526.7 Q1789.93 1526.35 1790.95 1526.19 Q1791.97 1526 1792.98 1526 Q1798.77 1526 1802.15 1529.17 Q1805.53 1532.34 1805.53 1537.76 Q1805.53 1543.34 1802.06 1546.44 Q1798.59 1549.52 1792.27 1549.52 Q1790.09 1549.52 1787.82 1549.15 Q1785.58 1548.78 1783.17 1548.04 L1783.17 1543.34 Q1785.25 1544.47 1787.48 1545.03 Q1789.7 1545.58 1792.17 1545.58 Q1796.18 1545.58 1798.52 1543.48 Q1800.86 1541.37 1800.86 1537.76 Q1800.86 1534.15 1798.52 1532.04 Q1796.18 1529.94 1792.17 1529.94 Q1790.3 1529.94 1788.42 1530.35 Q1786.57 1530.77 1784.63 1531.65 L1784.63 1514.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2261.83 1544.91 L2278.14 1544.91 L2278.14 1548.85 L2256.2 1548.85 L2256.2 1544.91 Q2258.86 1542.16 2263.45 1537.53 Q2268.05 1532.88 2269.23 1531.53 Q2271.48 1529.01 2272.36 1527.27 Q2273.26 1525.51 2273.26 1523.82 Q2273.26 1521.07 2271.32 1519.33 Q2269.39 1517.6 2266.29 1517.6 Q2264.09 1517.6 2261.64 1518.36 Q2259.21 1519.13 2256.43 1520.68 L2256.43 1515.95 Q2259.26 1514.82 2261.71 1514.24 Q2264.16 1513.66 2266.2 1513.66 Q2271.57 1513.66 2274.77 1516.35 Q2277.96 1519.03 2277.96 1523.52 Q2277.96 1525.65 2277.15 1527.57 Q2276.36 1529.47 2274.26 1532.07 Q2273.68 1532.74 2270.58 1535.95 Q2267.47 1539.15 2261.83 1544.91 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2287.96 1542.97 L2292.84 1542.97 L2292.84 1548.85 L2287.96 1548.85 L2287.96 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2313.03 1517.37 Q2309.42 1517.37 2307.59 1520.93 Q2305.78 1524.47 2305.78 1531.6 Q2305.78 1538.71 2307.59 1542.27 Q2309.42 1545.82 2313.03 1545.82 Q2316.66 1545.82 2318.47 1542.27 Q2320.3 1538.71 2320.3 1531.6 Q2320.3 1524.47 2318.47 1520.93 Q2316.66 1517.37 2313.03 1517.37 M2313.03 1513.66 Q2318.84 1513.66 2321.89 1518.27 Q2324.97 1522.85 2324.97 1531.6 Q2324.97 1540.33 2321.89 1544.94 Q2318.84 1549.52 2313.03 1549.52 Q2307.22 1549.52 2304.14 1544.94 Q2301.08 1540.33 2301.08 1531.6 Q2301.08 1522.85 2304.14 1518.27 Q2307.22 1513.66 2313.03 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip982)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  156.112,1446.14 2352.76,1446.14 
  "/>
<polyline clip-path="url(#clip982)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  156.112,1016.65 2352.76,1016.65 
  "/>
<polyline clip-path="url(#clip982)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  156.112,587.16 2352.76,587.16 
  "/>
<polyline clip-path="url(#clip982)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  156.112,157.673 2352.76,157.673 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,1486.45 156.112,47.2441 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,1446.14 175.01,1446.14 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,1016.65 175.01,1016.65 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,587.16 175.01,587.16 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,157.673 175.01,157.673 
  "/>
<path clip-path="url(#clip980)" d="M62.9365 1431.93 Q59.3254 1431.93 57.4967 1435.5 Q55.6912 1439.04 55.6912 1446.17 Q55.6912 1453.28 57.4967 1456.84 Q59.3254 1460.38 62.9365 1460.38 Q66.5707 1460.38 68.3763 1456.84 Q70.205 1453.28 70.205 1446.17 Q70.205 1439.04 68.3763 1435.5 Q66.5707 1431.93 62.9365 1431.93 M62.9365 1428.23 Q68.7467 1428.23 71.8022 1432.84 Q74.8809 1437.42 74.8809 1446.17 Q74.8809 1454.9 71.8022 1459.5 Q68.7467 1464.09 62.9365 1464.09 Q57.1264 1464.09 54.0477 1459.5 Q50.9921 1454.9 50.9921 1446.17 Q50.9921 1437.42 54.0477 1432.84 Q57.1264 1428.23 62.9365 1428.23 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M83.0984 1457.54 L87.9827 1457.54 L87.9827 1463.42 L83.0984 1463.42 L83.0984 1457.54 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M108.168 1431.93 Q104.557 1431.93 102.728 1435.5 Q100.922 1439.04 100.922 1446.17 Q100.922 1453.28 102.728 1456.84 Q104.557 1460.38 108.168 1460.38 Q111.802 1460.38 113.608 1456.84 Q115.436 1453.28 115.436 1446.17 Q115.436 1439.04 113.608 1435.5 Q111.802 1431.93 108.168 1431.93 M108.168 1428.23 Q113.978 1428.23 117.033 1432.84 Q120.112 1437.42 120.112 1446.17 Q120.112 1454.9 117.033 1459.5 Q113.978 1464.09 108.168 1464.09 Q102.358 1464.09 99.2789 1459.5 Q96.2234 1454.9 96.2234 1446.17 Q96.2234 1437.42 99.2789 1432.84 Q102.358 1428.23 108.168 1428.23 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M63.9319 1002.45 Q60.3208 1002.45 58.4921 1006.01 Q56.6865 1009.55 56.6865 1016.68 Q56.6865 1023.79 58.4921 1027.35 Q60.3208 1030.9 63.9319 1030.9 Q67.5661 1030.9 69.3717 1027.35 Q71.2004 1023.79 71.2004 1016.68 Q71.2004 1009.55 69.3717 1006.01 Q67.5661 1002.45 63.9319 1002.45 M63.9319 998.743 Q69.742 998.743 72.7976 1003.35 Q75.8763 1007.93 75.8763 1016.68 Q75.8763 1025.41 72.7976 1030.02 Q69.742 1034.6 63.9319 1034.6 Q58.1217 1034.6 55.043 1030.02 Q51.9875 1025.41 51.9875 1016.68 Q51.9875 1007.93 55.043 1003.35 Q58.1217 998.743 63.9319 998.743 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M84.0938 1028.05 L88.978 1028.05 L88.978 1033.93 L84.0938 1033.93 L84.0938 1028.05 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M99.2095 999.368 L117.566 999.368 L117.566 1003.3 L103.492 1003.3 L103.492 1011.77 Q104.51 1011.43 105.529 1011.27 Q106.547 1011.08 107.566 1011.08 Q113.353 1011.08 116.733 1014.25 Q120.112 1017.42 120.112 1022.84 Q120.112 1028.42 116.64 1031.52 Q113.168 1034.6 106.848 1034.6 Q104.672 1034.6 102.404 1034.23 Q100.159 1033.86 97.7511 1033.12 L97.7511 1028.42 Q99.8345 1029.55 102.057 1030.11 Q104.279 1030.66 106.756 1030.66 Q110.76 1030.66 113.098 1028.56 Q115.436 1026.45 115.436 1022.84 Q115.436 1019.23 113.098 1017.12 Q110.76 1015.02 106.756 1015.02 Q104.881 1015.02 103.006 1015.43 Q101.154 1015.85 99.2095 1016.73 L99.2095 999.368 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M53.7467 600.505 L61.3856 600.505 L61.3856 574.139 L53.0754 575.806 L53.0754 571.547 L61.3393 569.88 L66.0152 569.88 L66.0152 600.505 L73.654 600.505 L73.654 604.44 L53.7467 604.44 L53.7467 600.505 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M83.0984 598.56 L87.9827 598.56 L87.9827 604.44 L83.0984 604.44 L83.0984 598.56 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M108.168 572.959 Q104.557 572.959 102.728 576.524 Q100.922 580.065 100.922 587.195 Q100.922 594.301 102.728 597.866 Q104.557 601.408 108.168 601.408 Q111.802 601.408 113.608 597.866 Q115.436 594.301 115.436 587.195 Q115.436 580.065 113.608 576.524 Q111.802 572.959 108.168 572.959 M108.168 569.255 Q113.978 569.255 117.033 573.862 Q120.112 578.445 120.112 587.195 Q120.112 595.922 117.033 600.528 Q113.978 605.111 108.168 605.111 Q102.358 605.111 99.2789 600.528 Q96.2234 595.922 96.2234 587.195 Q96.2234 578.445 99.2789 573.862 Q102.358 569.255 108.168 569.255 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M54.7421 171.017 L62.381 171.017 L62.381 144.652 L54.0708 146.318 L54.0708 142.059 L62.3347 140.393 L67.0106 140.393 L67.0106 171.017 L74.6494 171.017 L74.6494 174.953 L54.7421 174.953 L54.7421 171.017 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M84.0938 169.073 L88.978 169.073 L88.978 174.953 L84.0938 174.953 L84.0938 169.073 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M99.2095 140.393 L117.566 140.393 L117.566 144.328 L103.492 144.328 L103.492 152.8 Q104.51 152.453 105.529 152.291 Q106.547 152.105 107.566 152.105 Q113.353 152.105 116.733 155.277 Q120.112 158.448 120.112 163.865 Q120.112 169.443 116.64 172.545 Q113.168 175.624 106.848 175.624 Q104.672 175.624 102.404 175.253 Q100.159 174.883 97.7511 174.142 L97.7511 169.443 Q99.8345 170.578 102.057 171.133 Q104.279 171.689 106.756 171.689 Q110.76 171.689 113.098 169.582 Q115.436 167.476 115.436 163.865 Q115.436 160.254 113.098 158.147 Q110.76 156.041 106.756 156.041 Q104.881 156.041 103.006 156.457 Q101.154 156.874 99.2095 157.754 L99.2095 140.393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip982)" style="stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none" points="
  218.281,1445.72 228.643,1360.24 239.004,1324.66 249.366,1297.36 259.727,1274.34 270.089,1254.06 280.45,1235.73 290.812,1218.87 301.174,1203.18 311.535,1188.44 
  321.897,1174.5 332.258,1161.25 342.62,1148.58 352.981,1136.43 363.343,1124.74 373.704,1113.46 384.066,1102.55 394.427,1091.97 404.789,1081.7 415.15,1071.72 
  425.512,1061.99 435.873,1052.5 446.235,1043.24 456.596,1034.19 466.958,1025.32 477.319,1016.65 487.681,1008.14 498.043,999.799 508.404,991.608 518.766,983.563 
  529.127,975.655 539.489,967.878 549.85,960.225 560.212,952.692 570.573,945.271 580.935,937.959 591.296,930.75 601.658,923.641 612.019,916.627 622.381,909.705 
  632.742,902.872 643.104,896.123 653.465,889.456 663.827,882.867 674.188,876.356 684.55,869.917 694.912,863.55 705.273,857.251 715.635,851.02 725.996,844.853 
  736.358,838.748 746.719,832.704 757.081,826.719 767.442,820.792 777.804,814.92 788.165,809.102 798.527,803.337 808.888,797.623 819.25,791.959 829.611,786.344 
  839.973,780.776 850.334,775.254 860.696,769.777 871.058,764.345 881.419,758.955 891.781,753.607 902.142,748.3 912.504,743.034 922.865,737.806 933.227,732.617 
  943.588,727.465 953.95,722.35 964.311,717.271 974.673,712.226 985.034,707.217 995.396,702.241 1005.76,697.298 1016.12,692.388 1026.48,687.509 1036.84,682.661 
  1047.2,677.844 1057.56,673.058 1067.93,668.3 1078.29,663.572 1088.65,658.871 1099.01,654.199 1109.37,649.554 1119.73,644.937 1130.1,640.345 1140.46,635.78 
  1150.82,631.24 1161.18,626.725 1171.54,622.235 1181.9,617.77 1192.26,613.328 1202.63,608.91 1212.99,604.515 1223.35,600.143 1233.71,595.793 1244.07,591.466 
  1254.43,587.16 1264.8,582.876 1275.16,578.613 1285.52,574.371 1295.88,570.149 1306.24,565.948 1316.6,561.766 1326.96,557.604 1337.33,553.462 1347.69,549.339 
  1358.05,545.234 1368.41,541.149 1378.77,537.081 1389.13,533.032 1399.5,529.001 1409.86,524.987 1420.22,520.991 1430.58,517.012 1440.94,513.049 1451.3,509.104 
  1461.66,505.175 1472.03,501.263 1482.39,497.366 1492.75,493.486 1503.11,489.621 1513.47,485.772 1523.83,481.938 1534.2,478.119 1544.56,474.316 1554.92,470.527 
  1565.28,466.753 1575.64,462.993 1586,459.248 1596.36,455.517 1606.73,451.8 1617.09,448.096 1627.45,444.407 1637.81,440.731 1648.17,437.068 1658.53,433.418 
  1668.9,429.782 1679.26,426.159 1689.62,422.548 1699.98,418.95 1710.34,415.365 1720.7,411.792 1731.06,408.232 1741.43,404.683 1751.79,401.147 1762.15,397.622 
  1772.51,394.11 1782.87,390.609 1793.23,387.12 1803.59,383.642 1813.96,380.175 1824.32,376.72 1834.68,373.276 1845.04,369.842 1855.4,366.42 1865.76,363.009 
  1876.13,359.608 1886.49,356.218 1896.85,352.838 1907.21,349.469 1917.57,346.11 1927.93,342.762 1938.29,339.423 1948.66,336.095 1959.02,332.776 1969.38,329.468 
  1979.74,326.169 1990.1,322.879 2000.46,319.6 2010.83,316.33 2021.19,313.069 2031.55,309.818 2041.91,306.576 2052.27,303.343 2062.63,300.119 2072.99,296.905 
  2083.36,293.699 2093.72,290.502 2104.08,287.314 2114.44,284.135 2124.8,280.965 2135.16,277.803 2145.53,274.649 2155.89,271.504 2166.25,268.368 2176.61,265.24 
  2186.97,262.12 2197.33,259.008 2207.69,255.904 2218.06,252.809 2228.42,249.721 2238.78,246.642 2249.14,243.57 2259.5,240.506 2269.86,237.45 2280.23,234.402 
  2290.59,231.361 
  "/>
<polyline clip-path="url(#clip982)" style="stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none" stroke-dasharray="6, 12" points="
  321.897,87.9763 332.258,151.181 342.62,206.311 352.981,254.951 363.343,298.281 373.704,337.203 384.066,372.416 394.427,404.475 404.789,433.823 415.15,460.823 
  425.512,485.772 435.873,508.916 446.235,530.465 456.596,550.592 466.958,569.447 477.319,587.16 487.681,603.841 498.043,619.586 508.404,634.48 518.766,648.597 
  529.127,662.002 539.489,674.753 549.85,686.901 560.212,698.493 570.573,709.57 580.935,720.169 591.296,730.323 601.658,740.062 612.019,749.414 622.381,758.405 
  632.742,767.056 643.104,775.388 653.465,783.421 663.827,791.173 674.188,798.658 684.55,805.893 694.912,812.89 705.273,819.663 715.635,826.223 725.996,832.581 
  736.358,838.748 746.719,844.732 757.081,850.543 767.442,856.189 777.804,861.677 788.165,867.014 798.527,872.208 808.888,877.265 819.25,882.19 829.611,886.99 
  839.973,891.669 850.334,896.233 860.696,900.685 871.058,905.032 881.419,909.276 891.781,913.421 902.142,917.473 912.504,921.433 922.865,925.305 933.227,929.093 
  943.588,932.799 953.95,936.427 964.311,939.979 974.673,943.458 985.034,946.866 995.396,950.206 1005.76,953.479 1016.12,956.689 1026.48,959.836 1036.84,962.924 
  1047.2,965.953 1057.56,968.927 1067.93,971.845 1078.29,974.711 1088.65,977.526 1099.01,980.29 1109.37,983.007 1119.73,985.676 1130.1,988.3 1140.46,990.879 
  1150.82,993.416 1161.18,995.91 1171.54,998.363 1181.9,1000.78 1192.26,1003.15 1202.63,1005.49 1212.99,1007.79 1223.35,1010.06 1233.71,1012.29 1244.07,1014.48 
  1254.43,1016.65 1264.8,1018.78 1275.16,1020.88 1285.52,1022.95 1295.88,1024.99 1306.24,1027 1316.6,1028.98 1326.96,1030.93 1337.33,1032.86 1347.69,1034.76 
  1358.05,1036.63 1368.41,1038.48 1378.77,1040.31 1389.13,1042.11 1399.5,1043.88 1409.86,1045.64 1420.22,1047.37 1430.58,1049.07 1440.94,1050.76 1451.3,1052.42 
  1461.66,1054.07 1472.03,1055.69 1482.39,1057.3 1492.75,1058.88 1503.11,1060.44 1513.47,1061.99 1523.83,1063.52 1534.2,1065.03 1544.56,1066.52 1554.92,1067.99 
  1565.28,1069.45 1575.64,1070.89 1586,1072.31 1596.36,1073.72 1606.73,1075.11 1617.09,1076.49 1627.45,1077.85 1637.81,1079.2 1648.17,1080.53 1658.53,1081.85 
  1668.9,1083.15 1679.26,1084.44 1689.62,1085.72 1699.98,1086.98 1710.34,1088.23 1720.7,1089.47 1731.06,1090.69 1741.43,1091.9 1751.79,1093.1 1762.15,1094.29 
  1772.51,1095.46 1782.87,1096.62 1793.23,1097.77 1803.59,1098.92 1813.96,1100.04 1824.32,1101.16 1834.68,1102.27 1845.04,1103.37 1855.4,1104.45 1865.76,1105.53 
  1876.13,1106.6 1886.49,1107.65 1896.85,1108.7 1907.21,1109.73 1917.57,1110.76 1927.93,1111.78 1938.29,1112.79 1948.66,1113.79 1959.02,1114.78 1969.38,1115.76 
  1979.74,1116.73 1990.1,1117.7 2000.46,1118.65 2010.83,1119.6 2021.19,1120.54 2031.55,1121.47 2041.91,1122.4 2052.27,1123.31 2062.63,1124.22 2072.99,1125.12 
  2083.36,1126.01 2093.72,1126.9 2104.08,1127.78 2114.44,1128.65 2124.8,1129.51 2135.16,1130.37 2145.53,1131.22 2155.89,1132.06 2166.25,1132.9 2176.61,1133.73 
  2186.97,1134.55 2197.33,1135.37 2207.69,1136.18 2218.06,1136.98 2228.42,1137.78 2238.78,1138.57 2249.14,1139.36 2259.5,1140.14 2269.86,1140.91 2280.23,1141.68 
  2290.59,1142.44 
  "/>
<polyline clip-path="url(#clip982)" style="stroke:#3da44d; stroke-linecap:butt; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none" stroke-dasharray="48, 30" points="
  321.897,87.9763 332.258,151.181 342.62,206.311 352.981,254.951 363.343,298.281 373.704,337.203 384.066,372.416 394.427,404.475 404.789,433.823 415.15,460.823 
  425.512,485.772 435.873,508.916 446.235,530.465 456.596,550.592 466.958,569.447 477.319,587.16 487.681,603.841 498.043,619.586 508.404,634.48 518.766,648.597 
  529.127,662.002 539.489,674.753 549.85,686.901 560.212,698.493 570.573,709.57 580.935,720.169 591.296,730.323 601.658,740.062 612.019,749.414 622.381,758.405 
  632.742,767.056 643.104,775.388 653.465,783.421 663.827,791.173 674.188,798.658 684.55,805.893 694.912,812.89 705.273,819.663 715.635,826.223 725.996,832.581 
  736.358,838.748 746.719,844.732 757.081,850.543 767.442,856.189 777.804,861.677 788.165,867.014 798.527,872.208 808.888,877.265 819.25,882.19 829.611,886.99 
  839.973,891.669 850.334,896.233 860.696,900.685 871.058,905.032 881.419,909.276 891.781,913.421 902.142,917.473 912.504,921.433 922.865,925.305 933.227,929.093 
  943.588,932.799 953.95,936.427 964.311,939.979 974.673,943.458 985.034,946.866 995.396,950.206 1005.76,953.479 1016.12,956.689 1026.48,959.836 1036.84,962.924 
  1047.2,965.953 1057.56,968.927 1067.93,971.845 1078.29,974.711 1088.65,977.526 1099.01,980.29 1109.37,983.007 1119.73,985.676 1130.1,988.3 1140.46,990.879 
  1150.82,993.416 1161.18,995.91 1171.54,998.363 1181.9,1000.78 1192.26,1003.15 1202.63,1005.49 1212.99,1007.79 1223.35,1010.06 1233.71,1012.29 1244.07,1014.48 
  1254.43,1016.65 1264.8,1018.78 1275.16,1020.88 1285.52,1022.95 1295.88,1024.99 1306.24,1027 1316.6,1028.98 1326.96,1030.93 1337.33,1032.86 1347.69,1034.76 
  1358.05,1036.63 1368.41,1038.48 1378.77,1040.31 1389.13,1042.11 1399.5,1043.88 1409.86,1045.64 1420.22,1047.37 1430.58,1049.07 1440.94,1050.76 1451.3,1052.42 
  1461.66,1054.07 1472.03,1055.69 1482.39,1057.3 1492.75,1058.88 1503.11,1060.44 1513.47,1061.99 1523.83,1063.52 1534.2,1065.03 1544.56,1066.52 1554.92,1067.99 
  1565.28,1069.45 1575.64,1070.89 1586,1072.31 1596.36,1073.72 1606.73,1075.11 1617.09,1076.49 1627.45,1077.85 1637.81,1079.2 1648.17,1080.53 1658.53,1081.85 
  1668.9,1083.15 1679.26,1084.44 1689.62,1085.72 1699.98,1086.98 1710.34,1088.23 1720.7,1089.47 1731.06,1090.69 1741.43,1091.9 1751.79,1093.1 1762.15,1094.29 
  1772.51,1095.46 1782.87,1096.62 1793.23,1097.77 1803.59,1098.92 1813.96,1100.04 1824.32,1101.16 1834.68,1102.27 1845.04,1103.37 1855.4,1104.45 1865.76,1105.53 
  1876.13,1106.6 1886.49,1107.65 1896.85,1108.7 1907.21,1109.73 1917.57,1110.76 1927.93,1111.78 1938.29,1112.79 1948.66,1113.79 1959.02,1114.78 1969.38,1115.76 
  1979.74,1116.73 1990.1,1117.7 2000.46,1118.65 2010.83,1119.6 2021.19,1120.54 2031.55,1121.47 2041.91,1122.4 2052.27,1123.31 2062.63,1124.22 2072.99,1125.12 
  2083.36,1126.01 2093.72,1126.9 2104.08,1127.78 2114.44,1128.65 2124.8,1129.51 2135.16,1130.37 2145.53,1131.22 2155.89,1132.06 2166.25,1132.9 2176.61,1133.73 
  2186.97,1134.55 2197.33,1135.37 2207.69,1136.18 2218.06,1136.98 2228.42,1137.78 2238.78,1138.57 2249.14,1139.36 2259.5,1140.14 2269.86,1140.91 2280.23,1141.68 
  2290.59,1142.44 
  "/>
<path clip-path="url(#clip980)" d="
M1541.14 302.578 L2279.53 302.578 L2279.53 95.2176 L1541.14 95.2176  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip980)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1541.14,302.578 2279.53,302.578 2279.53,95.2176 1541.14,95.2176 1541.14,302.578 
  "/>
<polyline clip-path="url(#clip980)" style="stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1565.54,147.058 1711.99,147.058 
  "/>
<path clip-path="url(#clip980)" d="M1752.9 128.319 L1752.9 131.861 L1748.82 131.861 Q1746.53 131.861 1745.63 132.787 Q1744.75 133.713 1744.75 136.12 L1744.75 138.412 L1751.76 138.412 L1751.76 141.722 L1744.75 141.722 L1744.75 164.338 L1740.47 164.338 L1740.47 141.722 L1736.39 141.722 L1736.39 138.412 L1740.47 138.412 L1740.47 136.606 Q1740.47 132.278 1742.48 130.31 Q1744.5 128.319 1748.87 128.319 L1752.9 128.319 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1766.69 128.366 Q1763.59 133.69 1762.09 138.898 Q1760.58 144.106 1760.58 149.453 Q1760.58 154.801 1762.09 160.055 Q1763.62 165.287 1766.69 170.588 L1762.99 170.588 Q1759.52 165.148 1757.78 159.893 Q1756.07 154.639 1756.07 149.453 Q1756.07 144.291 1757.78 139.06 Q1759.5 133.828 1762.99 128.366 L1766.69 128.366 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1796.51 138.412 L1787.13 151.027 L1797 164.338 L1791.97 164.338 L1784.43 154.152 L1776.88 164.338 L1771.86 164.338 L1781.93 150.773 L1772.71 138.412 L1777.74 138.412 L1784.61 147.648 L1791.49 138.412 L1796.51 138.412 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1802.34 128.366 L1806.05 128.366 Q1809.52 133.828 1811.23 139.06 Q1812.97 144.291 1812.97 149.453 Q1812.97 154.639 1811.23 159.893 Q1809.52 165.148 1806.05 170.588 L1802.34 170.588 Q1805.42 165.287 1806.93 160.055 Q1808.45 154.801 1808.45 149.453 Q1808.45 144.106 1806.93 138.898 Q1805.42 133.69 1802.34 128.366 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1837.13 142.81 L1866.81 142.81 L1866.81 146.699 L1837.13 146.699 L1837.13 142.81 M1837.13 152.254 L1866.81 152.254 L1866.81 156.189 L1837.13 156.189 L1837.13 152.254 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1909.98 151.398 Q1909.98 146.699 1908.04 144.037 Q1906.12 141.352 1902.74 141.352 Q1899.36 141.352 1897.41 144.037 Q1895.49 146.699 1895.49 151.398 Q1895.49 156.097 1897.41 158.782 Q1899.36 161.444 1902.74 161.444 Q1906.12 161.444 1908.04 158.782 Q1909.98 156.097 1909.98 151.398 M1895.49 142.347 Q1896.83 140.032 1898.87 138.921 Q1900.93 137.787 1903.78 137.787 Q1908.5 137.787 1911.44 141.537 Q1914.4 145.287 1914.4 151.398 Q1914.4 157.509 1911.44 161.259 Q1908.5 165.009 1903.78 165.009 Q1900.93 165.009 1898.87 163.898 Q1896.83 162.763 1895.49 160.449 L1895.49 164.338 L1891.21 164.338 L1891.21 128.319 L1895.49 128.319 L1895.49 142.347 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1933.24 151.305 Q1928.08 151.305 1926.09 152.486 Q1924.1 153.666 1924.1 156.514 Q1924.1 158.782 1925.58 160.125 Q1927.09 161.444 1929.66 161.444 Q1933.2 161.444 1935.33 158.944 Q1937.48 156.421 1937.48 152.254 L1937.48 151.305 L1933.24 151.305 M1941.74 149.546 L1941.74 164.338 L1937.48 164.338 L1937.48 160.402 Q1936.02 162.763 1933.85 163.898 Q1931.67 165.009 1928.52 165.009 Q1924.54 165.009 1922.18 162.787 Q1919.84 160.541 1919.84 156.791 Q1919.84 152.416 1922.76 150.194 Q1925.7 147.972 1931.51 147.972 L1937.48 147.972 L1937.48 147.555 Q1937.48 144.615 1935.54 143.018 Q1933.62 141.398 1930.12 141.398 Q1927.9 141.398 1925.79 141.93 Q1923.68 142.463 1921.74 143.527 L1921.74 139.592 Q1924.08 138.69 1926.28 138.25 Q1928.48 137.787 1930.56 137.787 Q1936.18 137.787 1938.96 140.703 Q1941.74 143.62 1941.74 149.546 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1969.12 151.398 Q1969.12 146.699 1967.18 144.037 Q1965.26 141.352 1961.88 141.352 Q1958.5 141.352 1956.55 144.037 Q1954.63 146.699 1954.63 151.398 Q1954.63 156.097 1956.55 158.782 Q1958.5 161.444 1961.88 161.444 Q1965.26 161.444 1967.18 158.782 Q1969.12 156.097 1969.12 151.398 M1954.63 142.347 Q1955.98 140.032 1958.01 138.921 Q1960.07 137.787 1962.92 137.787 Q1967.64 137.787 1970.58 141.537 Q1973.55 145.287 1973.55 151.398 Q1973.55 157.509 1970.58 161.259 Q1967.64 165.009 1962.92 165.009 Q1960.07 165.009 1958.01 163.898 Q1955.98 162.763 1954.63 160.449 L1954.63 164.338 L1950.35 164.338 L1950.35 128.319 L1954.63 128.319 L1954.63 142.347 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1991.39 166.745 Q1989.59 171.375 1987.87 172.787 Q1986.16 174.199 1983.29 174.199 L1979.89 174.199 L1979.89 170.634 L1982.39 170.634 Q1984.15 170.634 1985.12 169.8 Q1986.09 168.967 1987.27 165.865 L1988.04 163.921 L1977.55 138.412 L1982.06 138.412 L1990.17 158.689 L1998.27 138.412 L2002.78 138.412 L1991.39 166.745 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2025.19 139.176 L2025.19 143.203 Q2023.38 142.277 2021.44 141.815 Q2019.49 141.352 2017.41 141.352 Q2014.24 141.352 2012.64 142.324 Q2011.07 143.296 2011.07 145.24 Q2011.07 146.722 2012.2 147.578 Q2013.34 148.412 2016.76 149.176 L2018.22 149.5 Q2022.76 150.472 2024.66 152.254 Q2026.58 154.014 2026.58 157.185 Q2026.58 160.796 2023.71 162.902 Q2020.86 165.009 2015.86 165.009 Q2013.78 165.009 2011.51 164.592 Q2009.26 164.199 2006.76 163.388 L2006.76 158.99 Q2009.12 160.217 2011.42 160.842 Q2013.71 161.444 2015.95 161.444 Q2018.96 161.444 2020.58 160.426 Q2022.2 159.384 2022.2 157.509 Q2022.2 155.773 2021.02 154.847 Q2019.86 153.921 2015.91 153.064 L2014.43 152.717 Q2010.47 151.884 2008.71 150.171 Q2006.95 148.435 2006.95 145.426 Q2006.95 141.768 2009.54 139.778 Q2012.13 137.787 2016.9 137.787 Q2019.26 137.787 2021.35 138.134 Q2023.43 138.481 2025.19 139.176 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2035.91 151.398 Q2035.91 156.097 2037.83 158.782 Q2039.77 161.444 2043.15 161.444 Q2046.53 161.444 2048.48 158.782 Q2050.42 156.097 2050.42 151.398 Q2050.42 146.699 2048.48 144.037 Q2046.53 141.352 2043.15 141.352 Q2039.77 141.352 2037.83 144.037 Q2035.91 146.699 2035.91 151.398 M2050.42 160.449 Q2049.08 162.763 2047.02 163.898 Q2044.98 165.009 2042.11 165.009 Q2037.41 165.009 2034.45 161.259 Q2031.51 157.509 2031.51 151.398 Q2031.51 145.287 2034.45 141.537 Q2037.41 137.787 2042.11 137.787 Q2044.98 137.787 2047.02 138.921 Q2049.08 140.032 2050.42 142.347 L2050.42 138.412 L2054.68 138.412 L2054.68 174.199 L2050.42 174.199 L2050.42 160.449 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2078.48 142.393 Q2077.76 141.977 2076.9 141.791 Q2076.07 141.583 2075.05 141.583 Q2071.44 141.583 2069.49 143.944 Q2067.57 146.282 2067.57 150.68 L2067.57 164.338 L2063.29 164.338 L2063.29 138.412 L2067.57 138.412 L2067.57 142.44 Q2068.92 140.078 2071.07 138.944 Q2073.22 137.787 2076.3 137.787 Q2076.74 137.787 2077.27 137.856 Q2077.8 137.903 2078.45 138.018 L2078.48 142.393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2087.16 131.051 L2087.16 138.412 L2095.93 138.412 L2095.93 141.722 L2087.16 141.722 L2087.16 155.796 Q2087.16 158.967 2088.01 159.87 Q2088.89 160.773 2091.55 160.773 L2095.93 160.773 L2095.93 164.338 L2091.55 164.338 Q2086.62 164.338 2084.75 162.509 Q2082.87 160.657 2082.87 155.796 L2082.87 141.722 L2079.75 141.722 L2079.75 138.412 L2082.87 138.412 L2082.87 131.051 L2087.16 131.051 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2111.76 128.366 Q2108.66 133.69 2107.16 138.898 Q2105.65 144.106 2105.65 149.453 Q2105.65 154.801 2107.16 160.055 Q2108.68 165.287 2111.76 170.588 L2108.06 170.588 Q2104.59 165.148 2102.85 159.893 Q2101.14 154.639 2101.14 149.453 Q2101.14 144.291 2102.85 139.06 Q2104.56 133.828 2108.06 128.366 L2111.76 128.366 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2141.58 138.412 L2132.2 151.027 L2142.06 164.338 L2137.04 164.338 L2129.49 154.152 L2121.95 164.338 L2116.92 164.338 L2126.99 150.773 L2117.78 138.412 L2122.8 138.412 L2129.68 147.648 L2136.55 138.412 L2141.58 138.412 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2147.41 128.366 L2151.11 128.366 Q2154.59 133.828 2156.3 139.06 Q2158.04 144.291 2158.04 149.453 Q2158.04 154.639 2156.3 159.893 Q2154.59 165.148 2151.11 170.588 L2147.41 170.588 Q2150.49 165.287 2151.99 160.055 Q2153.52 154.801 2153.52 149.453 Q2153.52 144.106 2151.99 138.898 Q2150.49 133.69 2147.41 128.366 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip980)" style="stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" stroke-dasharray="2, 4" points="
  1565.54,198.898 1711.99,198.898 
  "/>
<path clip-path="url(#clip980)" d="M1752.23 186.224 L1745.88 203.423 L1758.59 203.423 L1752.23 186.224 M1749.59 181.618 L1754.89 181.618 L1768.06 216.178 L1763.2 216.178 L1760.05 207.312 L1744.47 207.312 L1741.32 216.178 L1736.39 216.178 L1749.59 181.618 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1794.47 200.529 L1794.47 216.178 L1790.21 216.178 L1790.21 200.668 Q1790.21 196.988 1788.78 195.159 Q1787.34 193.33 1784.47 193.33 Q1781.02 193.33 1779.03 195.53 Q1777.04 197.729 1777.04 201.525 L1777.04 216.178 L1772.76 216.178 L1772.76 190.252 L1777.04 190.252 L1777.04 194.28 Q1778.57 191.942 1780.63 190.784 Q1782.71 189.627 1785.42 189.627 Q1789.89 189.627 1792.18 192.405 Q1794.47 195.159 1794.47 200.529 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1814.75 203.145 Q1809.59 203.145 1807.6 204.326 Q1805.61 205.506 1805.61 208.354 Q1805.61 210.622 1807.09 211.965 Q1808.59 213.284 1811.16 213.284 Q1814.7 213.284 1816.83 210.784 Q1818.99 208.261 1818.99 204.094 L1818.99 203.145 L1814.75 203.145 M1823.25 201.386 L1823.25 216.178 L1818.99 216.178 L1818.99 212.242 Q1817.53 214.603 1815.35 215.738 Q1813.18 216.849 1810.03 216.849 Q1806.05 216.849 1803.69 214.627 Q1801.35 212.381 1801.35 208.631 Q1801.35 204.256 1804.26 202.034 Q1807.2 199.812 1813.01 199.812 L1818.99 199.812 L1818.99 199.395 Q1818.99 196.455 1817.04 194.858 Q1815.12 193.238 1811.63 193.238 Q1809.4 193.238 1807.3 193.77 Q1805.19 194.303 1803.25 195.367 L1803.25 191.432 Q1805.58 190.53 1807.78 190.09 Q1809.98 189.627 1812.06 189.627 Q1817.69 189.627 1820.47 192.543 Q1823.25 195.46 1823.25 201.386 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1832.02 180.159 L1836.28 180.159 L1836.28 216.178 L1832.02 216.178 L1832.02 180.159 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1855.98 218.585 Q1854.17 223.215 1852.46 224.627 Q1850.75 226.039 1847.87 226.039 L1844.47 226.039 L1844.47 222.474 L1846.97 222.474 Q1848.73 222.474 1849.7 221.64 Q1850.68 220.807 1851.86 217.705 L1852.62 215.761 L1842.13 190.252 L1846.65 190.252 L1854.75 210.529 L1862.85 190.252 L1867.37 190.252 L1855.98 218.585 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1877.46 182.891 L1877.46 190.252 L1886.23 190.252 L1886.23 193.562 L1877.46 193.562 L1877.46 207.636 Q1877.46 210.807 1878.31 211.71 Q1879.19 212.613 1881.86 212.613 L1886.23 212.613 L1886.23 216.178 L1881.86 216.178 Q1876.93 216.178 1875.05 214.349 Q1873.18 212.497 1873.18 207.636 L1873.18 193.562 L1870.05 193.562 L1870.05 190.252 L1873.18 190.252 L1873.18 182.891 L1877.46 182.891 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1891.83 190.252 L1896.09 190.252 L1896.09 216.178 L1891.83 216.178 L1891.83 190.252 M1891.83 180.159 L1896.09 180.159 L1896.09 185.553 L1891.83 185.553 L1891.83 180.159 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1923.66 191.247 L1923.66 195.229 Q1921.86 194.233 1920.03 193.747 Q1918.22 193.238 1916.37 193.238 Q1912.23 193.238 1909.93 195.877 Q1907.64 198.492 1907.64 203.238 Q1907.64 207.983 1909.93 210.622 Q1912.23 213.238 1916.37 213.238 Q1918.22 213.238 1920.03 212.752 Q1921.86 212.242 1923.66 211.247 L1923.66 215.182 Q1921.88 216.016 1919.96 216.432 Q1918.06 216.849 1915.91 216.849 Q1910.05 216.849 1906.6 213.168 Q1903.15 209.488 1903.15 203.238 Q1903.15 196.895 1906.62 193.261 Q1910.12 189.627 1916.18 189.627 Q1918.15 189.627 1920.03 190.043 Q1921.9 190.437 1923.66 191.247 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1959.26 180.159 L1959.26 183.701 L1955.19 183.701 Q1952.9 183.701 1951.99 184.627 Q1951.12 185.553 1951.12 187.96 L1951.12 190.252 L1958.13 190.252 L1958.13 193.562 L1951.12 193.562 L1951.12 216.178 L1946.83 216.178 L1946.83 193.562 L1942.76 193.562 L1942.76 190.252 L1946.83 190.252 L1946.83 188.446 Q1946.83 184.118 1948.85 182.15 Q1950.86 180.159 1955.24 180.159 L1959.26 180.159 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1966.86 181.618 L1966.86 194.465 L1962.92 194.465 L1962.92 181.618 L1966.86 181.618 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip980)" style="stroke:#3da44d; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" stroke-dasharray="16, 10" points="
  1565.54,250.738 1711.99,250.738 
  "/>
<path clip-path="url(#clip980)" d="M1741.07 237.3 L1741.07 264.175 L1746.72 264.175 Q1753.87 264.175 1757.18 260.934 Q1760.51 257.694 1760.51 250.703 Q1760.51 243.758 1757.18 240.541 Q1753.87 237.3 1746.72 237.3 L1741.07 237.3 M1736.39 233.458 L1746 233.458 Q1756.05 233.458 1760.75 237.647 Q1765.44 241.814 1765.44 250.703 Q1765.44 259.638 1760.72 263.828 Q1756 268.018 1746 268.018 L1736.39 268.018 L1736.39 233.458 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1772.27 257.786 L1772.27 242.092 L1776.53 242.092 L1776.53 257.624 Q1776.53 261.305 1777.97 263.156 Q1779.4 264.985 1782.27 264.985 Q1785.72 264.985 1787.71 262.786 Q1789.73 260.587 1789.73 256.791 L1789.73 242.092 L1793.99 242.092 L1793.99 268.018 L1789.73 268.018 L1789.73 264.036 Q1788.18 266.397 1786.12 267.555 Q1784.08 268.689 1781.37 268.689 Q1776.9 268.689 1774.59 265.911 Q1772.27 263.133 1772.27 257.786 M1782.99 241.467 L1782.99 241.467 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1814.54 254.985 Q1809.38 254.985 1807.39 256.166 Q1805.4 257.346 1805.4 260.194 Q1805.4 262.462 1806.88 263.805 Q1808.38 265.124 1810.95 265.124 Q1814.5 265.124 1816.63 262.624 Q1818.78 260.101 1818.78 255.934 L1818.78 254.985 L1814.54 254.985 M1823.04 253.226 L1823.04 268.018 L1818.78 268.018 L1818.78 264.082 Q1817.32 266.443 1815.14 267.578 Q1812.97 268.689 1809.82 268.689 Q1805.84 268.689 1803.48 266.467 Q1801.14 264.221 1801.14 260.471 Q1801.14 256.096 1804.06 253.874 Q1807 251.652 1812.81 251.652 L1818.78 251.652 L1818.78 251.235 Q1818.78 248.295 1816.83 246.698 Q1814.91 245.078 1811.42 245.078 Q1809.19 245.078 1807.09 245.61 Q1804.98 246.143 1803.04 247.207 L1803.04 243.272 Q1805.38 242.37 1807.57 241.93 Q1809.77 241.467 1811.86 241.467 Q1817.48 241.467 1820.26 244.383 Q1823.04 247.3 1823.04 253.226 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1831.81 231.999 L1836.07 231.999 L1836.07 268.018 L1831.81 268.018 L1831.81 231.999 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1860.24 233.458 L1880.1 233.458 L1880.1 237.393 L1864.91 237.393 L1864.91 247.578 L1878.62 247.578 L1878.62 251.513 L1864.91 251.513 L1864.91 268.018 L1860.24 268.018 L1860.24 233.458 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1895.68 245.078 Q1892.25 245.078 1890.26 247.763 Q1888.27 250.425 1888.27 255.078 Q1888.27 259.731 1890.24 262.416 Q1892.23 265.078 1895.68 265.078 Q1899.08 265.078 1901.07 262.393 Q1903.06 259.707 1903.06 255.078 Q1903.06 250.471 1901.07 247.786 Q1899.08 245.078 1895.68 245.078 M1895.68 241.467 Q1901.23 241.467 1904.4 245.078 Q1907.57 248.689 1907.57 255.078 Q1907.57 261.444 1904.4 265.078 Q1901.23 268.689 1895.68 268.689 Q1890.1 268.689 1886.93 265.078 Q1883.78 261.444 1883.78 255.078 Q1883.78 248.689 1886.93 245.078 Q1890.1 241.467 1895.68 241.467 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1929.66 246.073 Q1928.94 245.657 1928.08 245.471 Q1927.25 245.263 1926.23 245.263 Q1922.62 245.263 1920.68 247.624 Q1918.75 249.962 1918.75 254.36 L1918.75 268.018 L1914.47 268.018 L1914.47 242.092 L1918.75 242.092 L1918.75 246.12 Q1920.1 243.758 1922.25 242.624 Q1924.4 241.467 1927.48 241.467 Q1927.92 241.467 1928.45 241.536 Q1928.99 241.583 1929.63 241.698 L1929.66 246.073 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1931.65 242.092 L1935.91 242.092 L1941.23 262.323 L1946.53 242.092 L1951.55 242.092 L1956.88 262.323 L1962.18 242.092 L1966.44 242.092 L1959.66 268.018 L1954.63 268.018 L1949.05 246.768 L1943.45 268.018 L1938.43 268.018 L1931.65 242.092 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M1984.68 254.985 Q1979.52 254.985 1977.53 256.166 Q1975.54 257.346 1975.54 260.194 Q1975.54 262.462 1977.02 263.805 Q1978.52 265.124 1981.09 265.124 Q1984.63 265.124 1986.76 262.624 Q1988.92 260.101 1988.92 255.934 L1988.92 254.985 L1984.68 254.985 M1993.18 253.226 L1993.18 268.018 L1988.92 268.018 L1988.92 264.082 Q1987.46 266.443 1985.28 267.578 Q1983.11 268.689 1979.96 268.689 Q1975.98 268.689 1973.62 266.467 Q1971.28 264.221 1971.28 260.471 Q1971.28 256.096 1974.19 253.874 Q1977.13 251.652 1982.94 251.652 L1988.92 251.652 L1988.92 251.235 Q1988.92 248.295 1986.97 246.698 Q1985.05 245.078 1981.55 245.078 Q1979.33 245.078 1977.23 245.61 Q1975.12 246.143 1973.18 247.207 L1973.18 243.272 Q1975.51 242.37 1977.71 241.93 Q1979.91 241.467 1981.99 241.467 Q1987.62 241.467 1990.4 244.383 Q1993.18 247.3 1993.18 253.226 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2016.97 246.073 Q2016.25 245.657 2015.4 245.471 Q2014.56 245.263 2013.55 245.263 Q2009.93 245.263 2007.99 247.624 Q2006.07 249.962 2006.07 254.36 L2006.07 268.018 L2001.79 268.018 L2001.79 242.092 L2006.07 242.092 L2006.07 246.12 Q2007.41 243.758 2009.56 242.624 Q2011.72 241.467 2014.8 241.467 Q2015.24 241.467 2015.77 241.536 Q2016.3 241.583 2016.95 241.698 L2016.97 246.073 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2037.67 246.027 L2037.67 231.999 L2041.92 231.999 L2041.92 268.018 L2037.67 268.018 L2037.67 264.129 Q2036.32 266.443 2034.26 267.578 Q2032.23 268.689 2029.36 268.689 Q2024.66 268.689 2021.69 264.939 Q2018.75 261.189 2018.75 255.078 Q2018.75 248.967 2021.69 245.217 Q2024.66 241.467 2029.36 241.467 Q2032.23 241.467 2034.26 242.601 Q2036.32 243.712 2037.67 246.027 M2023.15 255.078 Q2023.15 259.777 2025.07 262.462 Q2027.02 265.124 2030.4 265.124 Q2033.78 265.124 2035.72 262.462 Q2037.67 259.777 2037.67 255.078 Q2037.67 250.379 2035.72 247.717 Q2033.78 245.032 2030.4 245.032 Q2027.02 245.032 2025.07 247.717 Q2023.15 250.379 2023.15 255.078 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2065.95 233.458 L2072.92 233.458 L2081.74 256.976 L2090.61 233.458 L2097.57 233.458 L2097.57 268.018 L2093.01 268.018 L2093.01 237.67 L2084.1 261.374 L2079.4 261.374 L2070.49 237.67 L2070.49 268.018 L2065.95 268.018 L2065.95 233.458 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2116.72 245.078 Q2113.29 245.078 2111.3 247.763 Q2109.31 250.425 2109.31 255.078 Q2109.31 259.731 2111.28 262.416 Q2113.27 265.078 2116.72 265.078 Q2120.12 265.078 2122.11 262.393 Q2124.1 259.707 2124.1 255.078 Q2124.1 250.471 2122.11 247.786 Q2120.12 245.078 2116.72 245.078 M2116.72 241.467 Q2122.27 241.467 2125.44 245.078 Q2128.61 248.689 2128.61 255.078 Q2128.61 261.444 2125.44 265.078 Q2122.27 268.689 2116.72 268.689 Q2111.14 268.689 2107.97 265.078 Q2104.82 261.444 2104.82 255.078 Q2104.82 248.689 2107.97 245.078 Q2111.14 241.467 2116.72 241.467 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2152.73 246.027 L2152.73 231.999 L2156.99 231.999 L2156.99 268.018 L2152.73 268.018 L2152.73 264.129 Q2151.39 266.443 2149.33 267.578 Q2147.29 268.689 2144.42 268.689 Q2139.73 268.689 2136.76 264.939 Q2133.82 261.189 2133.82 255.078 Q2133.82 248.967 2136.76 245.217 Q2139.73 241.467 2144.42 241.467 Q2147.29 241.467 2149.33 242.601 Q2151.39 243.712 2152.73 246.027 M2138.22 255.078 Q2138.22 259.777 2140.14 262.462 Q2142.09 265.124 2145.47 265.124 Q2148.85 265.124 2150.79 262.462 Q2152.73 259.777 2152.73 255.078 Q2152.73 250.379 2150.79 247.717 Q2148.85 245.032 2145.47 245.032 Q2142.09 245.032 2140.14 247.717 Q2138.22 250.379 2138.22 255.078 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2187.94 253.99 L2187.94 256.073 L2168.36 256.073 Q2168.64 260.471 2171 262.786 Q2173.38 265.078 2177.62 265.078 Q2180.07 265.078 2182.36 264.476 Q2184.68 263.874 2186.95 262.67 L2186.95 266.698 Q2184.66 267.67 2182.25 268.18 Q2179.84 268.689 2177.36 268.689 Q2171.16 268.689 2167.53 265.078 Q2163.91 261.467 2163.91 255.309 Q2163.91 248.944 2167.34 245.217 Q2170.79 241.467 2176.62 241.467 Q2181.85 241.467 2184.89 244.846 Q2187.94 248.203 2187.94 253.99 M2183.68 252.74 Q2183.64 249.245 2181.72 247.161 Q2179.82 245.078 2176.67 245.078 Q2173.1 245.078 2170.95 247.092 Q2168.82 249.106 2168.5 252.763 L2183.68 252.74 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2223.13 231.999 L2223.13 235.541 L2219.05 235.541 Q2216.76 235.541 2215.86 236.467 Q2214.98 237.393 2214.98 239.8 L2214.98 242.092 L2221.99 242.092 L2221.99 245.402 L2214.98 245.402 L2214.98 268.018 L2210.7 268.018 L2210.7 245.402 L2206.62 245.402 L2206.62 242.092 L2210.7 242.092 L2210.7 240.286 Q2210.7 235.958 2212.71 233.99 Q2214.72 231.999 2219.1 231.999 L2223.13 231.999 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip980)" d="M2230.72 233.458 L2230.72 246.305 L2226.78 246.305 L2226.78 233.458 L2230.72 233.458 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /></svg>
<hr/><h3 id="Takeaways"><a class="docs-heading-anchor" href="#Takeaways">Takeaways</a><a id="Takeaways-1"></a><a class="docs-heading-anchor-permalink" href="#Takeaways" title="Permalink"></a></h3><ol><li>Forward mode <span>$f&#39;$</span> is obtained simply by pushing a <code>Dual</code> through <code>babysqrt</code></li><li>To make the forward diff work in Julia, we only need to <strong><em>overload</em></strong> a few <strong><em>operators</em></strong> for forward mode AD to work on <strong><em>any function</em></strong></li><li>For vector valued function we can use <a href="http://adl.stanford.edu/hyperdual/"><strong><em>Hyperduals</em></strong></a></li><li>Forward diff can differentiation through the <code>setindex!</code> (called each time an element is assigned to a place in array, e.g. <code>x = [1,2,3]; x[2] = 1</code>)</li><li>ForwardDiff is implemented in <a href="https://github.com/JuliaDiff/ForwardDiff.jl"><code>ForwardDiff.jl</code></a>, which might appear to be neglected, but the truth is that it is very stable and general implementation.</li><li>ForwardDiff does not have to be implemented through Dual numbers. It can be implemented similarly to ReverseDiff through multiplication of Jacobians, which is what is the community work on now (in <a href="https://github.com/JuliaDiff/Diffractor.jl"><code>Diffractor</code></a>, <a href="https://github.com/FluxML/Zygote.jl"><code>Zygote</code></a> with rules defined in <a href="https://github.com/JuliaDiff/ChainRules.jl"><code>ChainRules</code></a>).</li></ol><hr/><h2 id="Reverse-mode"><a class="docs-heading-anchor" href="#Reverse-mode">Reverse mode</a><a id="Reverse-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-mode" title="Permalink"></a></h2><p>In reverse mode, the computation of the gradient follow the opposite order.  We initialize the computation by setting <span>$\mathbf{J}_0 = \frac{\partial y}{\partial y_0},$</span> which is again an identity matrix. Then we compute Jacobians and multiplications in the opposite order. The problem is that to calculate <span>$J_i$</span> we need to know the value of <span>$y_i^0$</span>, which cannot be calculated in the reverse pass. The backward pass therefore needs to be preceded by the forward pass, where  <span>$\{y_i^0\}_{i=1}^n$</span> are calculated.</p><p>The complete reverse mode algorithm therefore proceeds as </p><ol><li>Forward pass: iterate <code>i</code> from <code>n</code> down to <code>1</code> as<ul><li>calculate the next intermediate output as <span>$y^0_{i-1} = f_i(y^0_i)$</span> </li></ul></li><li>Backward pass: iterate <code>i</code> from <code>1</code> down to <code>n</code> as<ul><li>calculate Jacobian <span>$J_i = \left.\frac{f_i}{\partial y_i}\right|_{y_i^0}$</span> at point <span>$y_i^0$</span></li><li><em>pull back</em> the gradient as <span>$\left.\frac{\partial f(x)}{\partial y_{i}}\right|_{y^0_i} = \left.\frac{\partial y_0}{\partial y_{i-1}}\right|_{y^0_{i-1}} \times J_i$</span></li></ul></li></ol><p>The need to store intermediate outs has a huge impact on memory requirements, which particularly on GPU is a big deal. Recall few lectures ago we have been discussing how excessive memory allocations can be damaging for performance, here we are given an algorithm where the excessive allocation is by design.</p><h3 id="Tricks-to-decrease-memory-consumptions"><a class="docs-heading-anchor" href="#Tricks-to-decrease-memory-consumptions">Tricks to decrease memory consumptions</a><a id="Tricks-to-decrease-memory-consumptions-1"></a><a class="docs-heading-anchor-permalink" href="#Tricks-to-decrease-memory-consumptions" title="Permalink"></a></h3><ul><li>Define <strong>custom rules</strong> over large functional blocks. For example while we can auto-grad (in theory) matrix product, it is much more efficient to define make a matrix multiplication as one large function, for which we define Jacobians (note that by doing so, we can dispatch on Blas). e.g</li></ul><p class="math-container">\[\begin{alignat*}{2}
  \mathbf{C} &amp;= \mathbf{A} * \mathbf{B} \\
  \frac{\partial{\mathbf{C}}}{\partial \mathbf{A}} &amp;= \mathbf{B} \\
  \frac{\partial{\mathbf{C}}}{\partial \mathbf{B}} &amp;= \mathbf{A}^{\mathrm{T}} \\
\end{alignat*}\]</p><ul><li>When differentiating <strong>Invertible functions</strong>, calculate intermediate outputs from the output. This can lead to huge performance gain, as all data needed for computations are in caches.  </li><li><strong>Checkpointing</strong> does not store intermediate ouputs after larger sequence of operations. When they are needed for forward pass, they are recalculated on demand.</li></ul><p>Most reverse mode AD engines does not support mutating values of arrays (<code>setindex!</code> in julia). This is related to the memory consumption, where after every <code>setindex!</code> you need in theory save the full matrix. <a href="https://github.com/wsmoses/Enzyme.jl"><code>Enzyme</code></a> differentiating directly LLVM code supports this, since in LLVM every variable is assigned just once. ForwardDiff methods does not suffer this problem, as the gradient is computed at the time of the values.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Reverse mode AD was first published in 1976 by Seppo Linnainmaa<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>, a finnish computer scientist. It was popularized in the end of 80s when applied to training multi-layer perceptrons, which gave rise to the famous <strong>backpropagation</strong> algorithm<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>, which is a special case of reverse mode AD.</p></div></div><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>The terminology in automatic differentiation is everything but fixed. The community around <code>ChainRules.jl</code> went a great length to use something reasonable. They use <strong>pullback</strong> for a function realizing vector-Jacobian product in the reverse-diff reminding that the gradient is pulled back to the origin of the computation. The use <strong>pushforward</strong> to denote the same operation in the ForwardDiff, as the gradient is push forward through the computation.</p></div></div><h2 id="Implementation-details-of-reverse-AD"><a class="docs-heading-anchor" href="#Implementation-details-of-reverse-AD">Implementation details of reverse AD</a><a id="Implementation-details-of-reverse-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-details-of-reverse-AD" title="Permalink"></a></h2><h3 id="Graph-based-AD"><a class="docs-heading-anchor" href="#Graph-based-AD">Graph-based AD</a><a id="Graph-based-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Graph-based-AD" title="Permalink"></a></h3><p>In Graph-based approach, we start with a complete knowledge of the computation graph (which is known in many cases like classical neural networks) and augment it with nodes representing the computation of the computation of the gradient (backward path). We need to be careful to add all edges representing the flow of information needed to calculate the gradient. Once the computation graph is augmented, we can find the subgraph needed to compute the desired node(s). </p><p>Recall the example from the beginning of the lecture <span>$f(x, y) = \sin(x) + xy$</span>, let&#39;s observe, how the extension of the computational graph will look like. The computation graph of function <span>$f$</span> looks like</p><p><img src="../graphdiff_6.svg" alt="diff graph"/></p><p>where arrows <span>$\rightarrow$</span> denote the flow of operations and we have denoted the output of function <span>$f$</span> as <span>$z$</span> and outputs of intermediate nodes as <span>$h_i$</span> standing for <em>hidden</em>.</p><p>We start from the top and add a node calculating <span>$\frac{\partial z}{\partial h_3}$</span> which is an identity, needed to jump-start the differentiation. </p><p><img src="../graphdiff_7.svg" alt="diff graph"/></p><p>We connect it with the output of <span>$h_3$</span>, even though technically in this case it is not needed, as the <span>$z = h_3$</span>. We then add a node calculating <span>$\frac{\partial h_3}{\partial h_2}$</span> for which we only need information about <span>$h_2$</span> and mark it in the graph (again, this edge can be theoretically dropped due to being equal to one regardless the inputs). Following the chain rule, we need to combine <span>$\frac{\partial h_3}{\partial h_2}$</span> with <span>$\frac{\partial z}{\partial h_3}$</span> to compute <span>$\frac{\partial z}{\partial h_2}$</span> which we note in the graph.</p><p><img src="../graphdiff_9.svg" alt="diff graph"/></p><p>We continue with the same process with <span>$\frac{\partial h_3}{\partial h_1}$</span>, which we again combine with <span>$\frac{\partial z}{\partial h_1}$</span> to obtain <span>$\frac{\partial z}{\partial h_1}$</span>. Continuing the reverse diff we obtain the final graph</p><p><img src="../graphdiff_14.svg" alt="diff graph"/> </p><p>containing the desired nodes <span>$\frac{\partial z}{\partial x}$</span> and <span>$\frac{\partial z}{\partial y}$</span>. This computational graph can be passed to the compiler to compute desired values.</p><p>This approach to AD has been taken for example by <a href="https://github.com/Theano/Theano">Theano</a> and by <a href="https://www.tensorflow.org/">TensorFlow</a>. In Tensorflow when you use functions like <code>tf.mul( a, b )</code> or <code>tf.add(a,b)</code>, you are not performing the computation in Python, but you are building the computational graph shown as above. You can then compute the values using <code>tf.run</code> with a desired inputs, but you are in fact computing the values in a different interpreter / compiler then in python.</p><p>Advantages:</p><ul><li>Knowing the computational graph in advance is great, as you can do expensive optimization steps to simplify the graph. </li><li>The computational graph have a simple semantics (limited support for loops, branches, no objects), and the compiler is therefore simpler than the compiler of full languages.</li><li>Since the computation of gradient augments the graph, you can run the process again to obtain higher order gradients. </li><li>TensorFlow allows you to specialize on sizes of Tensors, which means that it knows precisely how much memory you will need and where, which decreases the number of allocations. This is quite important in GPU.</li></ul><p>Disadvantages:</p><ul><li>You are restricted to fixed computation graph. It is generally difficult to implement <code>if</code> or <code>while</code>, and hence to change the computation according to values computed during the forward pass.</li><li>Development and debugging can be difficult, since you are not developing the computation graph in the host language.</li><li>Exploiting within computation graph parallelism might be difficult.</li></ul><h3 id="Tracking-based-AD"><a class="docs-heading-anchor" href="#Tracking-based-AD">Tracking-based AD</a><a id="Tracking-based-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Tracking-based-AD" title="Permalink"></a></h3><p>Alternative to static-graph based methods are methods, which builds the graph during invocation of functions and then use this dynamically built graph to know, how to compute the gradient. The dynamically built graph is frequently called <em>tape</em>. This approach is used by popular libraries like <a href="https://pytorch.org/"><strong><em>PyTorch</em></strong></a>, <a href="https://github.com/HIPS/autograd"><strong><em>AutoGrad</em></strong></a>, and <a href="https://chainer.org/"><strong><em>Chainer</em></strong></a> in Python ecosystem, or by <a href="https://github.com/FluxML/Tracker.jl"><strong><em>Tracker.jl</em></strong></a> (<code>Flux.jl</code>&#39;s former AD backend), <a href="https://github.com/JuliaDiff/ReverseDiff.jl"><strong><em>ReverseDiff.jl</em></strong></a>, and <a href="https://github.com/denizyuret/AutoGrad.jl"><strong><em>AutoGrad.jl</em></strong></a> (<code>Knet.jl</code>&#39;s AD backend) in Julia. This type of AD systems is also called <em>operator overloading</em>, since in order to record the operations performed on the arguments we need to replace/wrap the original implementation.</p><p>How do we build the tracing? Let&#39;s take a look what <code>ReverseDiff.jl</code> is doing. It defines <code>TrackedArray</code> (it also defines <code>TrackedReal</code>, but <code>TrackedArray</code> is more interesting) as</p><pre><code class="language-julia hljs">struct TrackedArray{T,N,V&lt;:AbstractArray{T,N}} &lt;: AbstractArray{T,N}
    value::V
    deriv::Union{Nothing,V}
    tape::Vector{Any}
    string_tape::String
end</code></pre><p>where in</p><ul><li><code>value</code> it stores the value of the array</li><li><code>deriv</code> will hold the gradient of the tracked array</li><li><code>tape</code> of will log operations performed with the tracked array, such that we can calculate the gradient as a sum of operations performed over the tape.</li></ul><p>What do we need to store on the tape? Let&#39;s denote as <span>$a$</span> the current <code>TrackedArray</code>. The gradient with respect to some output <span>$z$</span> is equal to <span>$\frac{\partial z}{\partial a} = \sum_{g_i} \frac{\partial z}{\partial g_i} \times \frac{\partial g_i}{\partial a}$</span> where  <span>$g_i$</span> is the output of any function (in the computational graph) where <span>$a$</span> was a direct input. The <code>InstructionTape</code> will therefore contain a reference to <span>$g_i$</span> (which has to be of <code>TrackedArray</code> and where we know <span>$\frac{\partial z}{\partial g_i}$</span> will be stored in <code>deriv</code> field) and we also need to a method calculating <span>$\frac{\partial g_i}{\partial a}$</span>, which can be stored as an anonymous function will accepting the grad as an argument.</p><pre><code class="language-julia hljs">TrackedArray(a::AbstractArray, string_tape::String = &quot;&quot;) = TrackedArray(a, similar(a) .= 0, [], string_tape)
TrackedMatrix{T,V} = TrackedArray{T,2,V} where {T,V&lt;:AbstractMatrix{T}}
TrackedVector{T,V} = TrackedArray{T,1,V} where {T,V&lt;:AbstractVector{T}}
Base.show(io::IO, ::MIME&quot;text/plain&quot;, a::TrackedArray) = show(io, a)
Base.show(io::IO, a::TrackedArray) = print(io, &quot;TrackedArray($(size(a.value)))&quot;)
value(A::TrackedArray) = A.value
value(A) = A
track(A, string_tape = &quot;&quot;) = TrackedArray(A, string_tape)
track(a::Number, string_tape) = TrackedArray(reshape([a], 1, 1), string_tape)

import Base: +, *
function *(A::TrackedMatrix, B::TrackedMatrix)
    a, b = value.((A, B))
    C = TrackedArray(a * b, &quot;($(A.string_tape) * $(B.string_tape))&quot;)
    push!(A.tape, (C, Δ -&gt; Δ * b&#39;))
    push!(B.tape, (C, Δ -&gt; a&#39; * Δ))
    C
end

function +(A::TrackedMatrix, B::TrackedMatrix)
    C = TrackedArray(value(A) + value(B), &quot;($(A.string_tape) + $(B.string_tape))&quot;)
    push!(A.tape, (C, Δ -&gt; Δ))
    push!(B.tape, (C, Δ -&gt; Δ))
    C
end

function msin(A::TrackedMatrix)
    a = value(A)
    C = TrackedArray(sin.(a), &quot;sin($(A.string_tape))&quot;)
    push!(A.tape, (C, Δ -&gt; cos.(a) .* Δ))
    C
end</code></pre><p>Let&#39;s observe that the operations are recorded on the tape as they should</p><pre><code class="language-julia hljs">a = rand()
b = rand()
A = track(a, &quot;A&quot;)
B = track(b, &quot;B&quot;)
# R = A * B + msin(A)
C = A * B 
A.tape
B.tape
C.string_tape
R = C + msin(A)
A.tape
B.tape
R.string_tape</code></pre><p>Let&#39;s now implement a function that will recursively calculate the gradient of a term of interest. It goes over its childs, if they not have calculated the gradients, calculate it, otherwise it adds it to its own after  if not, ask them to calculate the gradient and otherwise </p><pre><code class="language-julia hljs">function accum!(A::TrackedArray)
    isempty(A.tape) &amp;&amp; return(A.deriv)
    A.deriv .= sum(g(accum!(r)) for (r, g) in A.tape)
    empty!(A.tape)
    A.deriv
end</code></pre><p>We can calculate the gradient by initializing the gradient of the result to vector of ones simulating the <code>sum</code> function</p><pre><code class="language-julia hljs">using Zygote
R.deriv .= 1
accum!(A)[1]
gradient(a -&gt; a*b + sin(a), a)[1]
accum!(B)[1]
gradient(b -&gt; a*b + sin(a), b)[1]</code></pre><p>The api function for computing the grad might look like</p><pre><code class="language-julia hljs">function grad(f, args...)
    args = track.(args)
    o = f(args...)
    fill!(o.deriv, 1)
    map(accum!, args)
end</code></pre><p>where we should assert that the output dimension is 1. In our implementation we dirtily expect the output of f to be summed to a scalar.</p><p>Let&#39;s compare the results to those computed by Zygote</p><pre><code class="language-julia hljs">A = rand(4,4)
B = rand(4,4)
grad(A -&gt; A * B + msin(A), A)[1]
gradient(A -&gt; sum(A * B + sin.(A)), A)[1]
grad(A -&gt; A * B + msin(A), B)[1]
gradient(A -&gt; sum(A * B + sin.(A)), B)[1]</code></pre><p>To make the above AD system really useful, we would need to </p><ol><li>Add support for <code>TrackedReal</code>, which is straightforward (we might skip the anonymous function, as the derivative of a scalar function is always a number).</li><li>We would need to add a lot of rules, how to work with basic values. This is why the the approach is called <strong>operator overloading</strong> since you need to overload a lot of functions (or methods or operators).</li></ol><p>For example to add all combinations for <code>+</code> and <code>*</code>, we would need to add following rules.</p><pre><code class="language-julia hljs">function *(A::TrackedMatrix, B::AbstractMatrix)
   C = TrackedArray(value(A) * B, &quot;($(A.string_tape) * B)&quot;)
   push!(A.tape, (C, Δ -&gt; Δ * B&#39;))
   C
end

function *(A::AbstractMatrix, B::TrackedMatrix)
   C = TrackedArray(value(A) * value(B), &quot;($(A.string_tape) * $(B.string_tape))&quot;)
   push!(B.tape, (C, Δ -&gt; A&#39; * Δ))
   C
end

function +(A::TrackedMatrix, B::TrackedMatrix)
   C = TrackedArray(value(A) + value(B), &quot;($(A.string_tape) + $(B.string_tape))&quot;)
   push!(A.tape, (C, Δ -&gt; Δ ))
   push!(B.tape, (C, Δ -&gt; Δ))
   C
end

function +(A::AbstractMatrix, B::TrackedMatrix)
   C = TrackedArray(A * value(B), &quot;(A + $(B.string_tape))&quot;)
   push!(B.tape, (C, Δ -&gt; Δ))
   C
end</code></pre><p>Advantages:</p><ul><li>Debugging and development is nicer, as AD is implemented in the same language.</li><li>The computation graph, tape, is dynamic, which makes it simpler to take the gradient in the presence of <code>if</code> and <code>while</code>.</li></ul><p>Disadvantages:</p><ul><li>The computation graph is created and differentiated during every computation, which might be costly. In most deep learning applications, this overhead is negligible in comparison to time of needed to perform the operations itself (<code>ReverseDiff.jl</code> allows to compile the tape).</li><li>Since computation graph is dynamic, it cannot be optimized as the static graph, the same holds for the memory allocations. </li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>The difference between tracking and graph-based AD systems is conceptually similar to interpreted and compiled programming languages. Tracking AD systems interpret the time while computing the gradient, while graph-based AD systems compile the computation of the gradient.</p></div></div><h2 id="ChainRules"><a class="docs-heading-anchor" href="#ChainRules">ChainRules</a><a id="ChainRules-1"></a><a class="docs-heading-anchor-permalink" href="#ChainRules" title="Permalink"></a></h2><p>From our discussions about AD systems so far we see that while the basic, <em>engine</em>, part is relatively straightforward, it devil is in writing the rules prescribing the computation of gradients. These rules are needed for every system whether it is graph based, tracking, or Wengert list based. ForwardDiff also needs a rule system, but rules are a bit different (as they are pushing the gradient forward rather than pulling it back). It is obviously a waste of effort for each AD system to have its own set of rules. Therefore the community (initiated by Lyndon White backed by <a href="https://github.com/invenia">Invenia</a>) have started to work on a unified system to express differentiation rules, such that they can be shared between systems. So far, they are supported by <code>Zygote.jl</code>, <code>Nabla.jl</code>, <code>ReverseDiff.jl</code> and <code>Diffractor.jl</code>, suggesting that the unification approach is working.</p><p>The definition of reverse diff rules follows the idea we have nailed above (we refer readers interested in forward diff rules <a href="https://juliadiff.org/ChainRulesCore.jl">to official documentation</a>).</p><p><code>ChainRules</code> defines the reverse rules for function <code>foo</code> in a function <code>rrule</code> with the following signature</p><pre><code class="language-julia hljs">function rrule(::typeof(foo), args...; kwargs...)
    ...
    return y, pullback
end</code></pre><p>where</p><ul><li>the first argument <code>::typeof(foo)</code> allows to dispatch on the function for which the rules is written</li><li>the output of function <code>foo(args...)</code> is returned as the first argument</li><li><code>pullback(Δy)</code> takes the gradient of upstream functions with respect to the output of <code>foo(args)</code> and returns it multiplied by the jacobian of the output of <code>foo(args)</code> with respect to parameters of the function itself (recall the function can have paramters, as it can be a closure or a functor), and with respect to the arguments.</li></ul><pre><code class="language-julia hljs">function pullback(Δy)
    ...
    return ∂self, ∂args...
end</code></pre><p>Notice that key-word arguments are not differentiated. This is a design decision with the explanation that parametrize the function, but most of the time, they are not differentiable.</p><p><code>ChainRules.jl</code> provides support for lazy (delayed) computation using <code>Thunk</code>. Its argument is a function, which is not evaluated until <code>unthunk</code> is called. There is also a support to signal that gradient is zero using <code>ZeroTangent</code> (which can save valuable memory) or to signal that the gradient does not exist using <code>NoTangent</code>. </p><p>How can we use ChainRules to define rules for our AD system? Let&#39;s first observe the output</p><pre><code class="language-julia hljs">using ChainRulesCore, ChainRules
r, g = rrule(*, rand(2,2), rand(2,2))
g(r)</code></pre><p>With that, we can extend our AD system as follows</p><pre><code class="language-julia hljs">import Base: *, +, -
for f in [:*, :+, :-]
    @eval function $(f)(A::TrackedMatrix, B::AbstractMatrix)
       C, pullback = rrule($(f), value(A), B)
       C = track(C)
       push!(A.tape, (C, Δ -&gt; pullback(Δ)[2]))
       C
    end

    @eval function $(f)(A::AbstractMatrix, B::TrackedMatrix)
       C, pullback = rrule($(f), A, value(B))
       C = track(C)
       push!(B.tape, (C, Δ -&gt; pullback(Δ)[3]))
       C
    end

    @eval function $(f)(A::TrackedMatrix, B::TrackedMatrix)
       C, pullback = rrule($(f), value(A), value(B))
       C = track(C)
       push!(A.tape, (C, Δ -&gt; pullback(Δ)[2]))
       push!(B.tape, (C, Δ -&gt; pullback(Δ)[3]))
       C
    end
end</code></pre><p>and we need to modify our <code>accum!</code> code to <code>unthunk</code> if needed</p><pre><code class="language-julia hljs">function accum!(A::TrackedArray)
    isempty(A.tape) &amp;&amp; return(A.deriv)
    A.deriv .= sum(unthunk(g(accum!(r))) for (r, g) in A.tape)
end</code></pre><pre><code class="language-julia hljs">A = rand(4,4)
B = rand(4,4)
grad(A -&gt; (A * B + msin(A))*B, A)[1]
gradient(A -&gt; sum(A * B + sin.(A)), A)[1]
grad(A -&gt; A * B + msin(A), B)[1]
gradient(A -&gt; sum(A * B + sin.(A)), B)[1]</code></pre><h2 id="Source-to-source-AD-using-Wengert"><a class="docs-heading-anchor" href="#Source-to-source-AD-using-Wengert">Source-to-source AD using Wengert</a><a id="Source-to-source-AD-using-Wengert-1"></a><a class="docs-heading-anchor-permalink" href="#Source-to-source-AD-using-Wengert" title="Permalink"></a></h2><p>Recall the compile stages of julia and look, how the lowered code for</p><pre><code class="language-julia hljs">f(x,y) = x*y + sin(x)</code></pre><p>looks like</p><pre><code class="language-julia hljs">julia&gt; @code_lowered f(1.0, 1.0)
CodeInfo(
1 ─ %1 = x * y
│   %2 = Main.sin(x)
│   %3 = %1 + %2
└──      return %3
)</code></pre><p>This form is particularly nice for automatic differentiation, as we have on the left hand side always a single variable, which means the compiler has provided us with a form, on which we know, how to apply AD rules.</p><p>What if we somehow be able to talk to the compiler and get this form from him?</p><ul><li><a href="https://juliadiff.org/ChainRulesCore.jl/dev/autodiff/operator_overloading.html#ReverseDiffZero">simplest viable implementation</a></li></ul><h2 id="ChainRules-2"><a class="docs-heading-anchor" href="#ChainRules-2">ChainRules</a><a class="docs-heading-anchor-permalink" href="#ChainRules-2" title="Permalink"></a></h2><p>From our discussions about AD systems so far we see that while the basic, <em>engine</em>, part is relatively straightforward, it devil is in writing the rules prescribing the computation of gradients. These rules are needed for every system whether it is graph based, tracking, or wengert list based. ForwardDiff also needs a rule system, but rules are a bit different (as they are pushing the gradient forward rather than pulling it back). It is obviously a waste of effort for each AD system to have its own set of rules. Hence the community (initiated by Lyndon White and hence probably funded mainly by Invenia) have started to work on a unified system to express differentiation rules, such that they can be shared between systems. So far, they are supported by <code>Zygote.jl</code>, <code>Nabla.jl</code>, <code>ReverseDiff.jl</code> and <code>Diffractor.jl</code>, hence the approach seems to work.</p><p>The definition of reverse diff rules follows the idea we have nailed above (we refer readers interested in forward diff rules <a href="https://juliadiff.org/ChainRulesCore.jl">to official documentation</a>).</p><p><code>ChainRules</code> defines the reverse rules for function <code>foo</code> in a function <code>rrule</code> with the following signature</p><pre><code class="language-julia hljs">function rrule(::typeof(foo), args...; kwargs...)
    ...
    return y, pullback
end</code></pre><p>where</p><ul><li>the first argument <code>::typeof(foo)</code> allows to dispatch on the function for which the rules is written</li><li>the output of function <code>foo(args...)</code> is returned as the first argument</li><li><code>pullback(Δy)</code> takes the gradient of upstream functions with respect to the output of <code>foo(args)</code> and returns it multiplied by the jacobian of the output of <code>foo(args)</code> with respect to parameters of the function itself (recall the function can have paramters, as it can be a closure or a functor), and with respect to the arguments.</li></ul><pre><code class="language-julia hljs">function pullback(Δy)
    ...
    return ∂self, ∂args...
end</code></pre><p>Notice that key-word arguments are not differentiated. This is a design decision with the explanation that parametrize the function, but most of the time, they are not differentiable.</p><p><code>ChainRules.jl</code> provides support for lazy (delayed) computation using <code>Thunk</code>. Its argument is a function, which is not evaluated until <code>unthunk</code> is called. There is also a support to signal that gradient is zero using <code>ZeroTangent</code> (which can save valuable memory) or to signal that the gradient does not exist using <code>NoTangent</code>. </p><p>How can we use ChainRules to define rules for our AD system? Let&#39;s first observe the output</p><pre><code class="language-julia hljs">using ChainRulesCore, ChainRules
r, g = rrule(*, rand(2,2), rand(2,2))
g(r)</code></pre><p>With that, we can extend our AD system as follows</p><pre><code class="language-julia hljs">import Base: *, +, -
for f in [:*, :+, :-]
    @eval function $(f)(A::TrackedMatrix, B::AbstractMatrix)
       C, pullback = rrule($(f), value(A), B)
       C = track(C)
       push!(A.tape, (C, Δ -&gt; pullback(Δ)[2]))
       C
    end

    @eval function $(f)(A::AbstractMatrix, B::TrackedMatrix)
       C, pullback = rrule($(f), A, value(B))
       C = track(C)
       push!(B.tape, (C, Δ -&gt; pullback(Δ)[3]))
       C
    end

    @eval function $(f)(A::TrackedMatrix, B::TrackedMatrix)
       C, pullback = rrule($(f), value(A), value(B))
       C = track(C)
       push!(A.tape, (C, Δ -&gt; pullback(Δ)[2]))
       push!(B.tape, (C, Δ -&gt; pullback(Δ)[3]))
       C
    end
end</code></pre><p>and we need to modify our <code>accum!</code> code to <code>unthunk</code> if needed</p><pre><code class="language-julia hljs">function accum!(A::TrackedArray)
    isempty(A.tape) &amp;&amp; return(A.deriv)
    A.deriv .= sum(unthunk(g(accum!(r))) for (r, g) in A.tape)
end</code></pre><pre><code class="nohighlight hljs">A = track(rand(4,4))
B = rand(4,1)
C = rand(4,1)
R = A * B + C
R.deriv .= 1
accum!(A)</code></pre><h3 id="Sources-for-this-lecture"><a class="docs-heading-anchor" href="#Sources-for-this-lecture">Sources for this lecture</a><a id="Sources-for-this-lecture-1"></a><a class="docs-heading-anchor-permalink" href="#Sources-for-this-lecture" title="Permalink"></a></h3><ul><li>Mike Innes&#39; <a href="https://github.com/MikeInnes/diff-zoo">diff-zoo</a></li><li><a href="https://blog.rogerluo.me/2019/07/27/yassad/">Write Your Own StS in One Day</a></li><li><a href="https://arxiv.org/pdf/1810.07951.pdf">Zygote.jl Paper</a> and <a href="https://fluxml.ai/Zygote.jl/dev/internals/">Zygote.jl Internals</a></li><li>Keno&#39;s <a href="https://www.youtube.com/watch?v=mQnSRfseu0c&amp;feature=youtu.be">Talk</a></li><li>Chris&#39; <a href="https://mitmath.github.io/18337/lecture11/adjoints">Lecture</a></li><li><a href="https://liebing.org.cn/2019/07/22/Automatic-Differentiation-Based-on-Computation-Graph/">Automatic-Differentiation-Based-on-Computation-Graph</a></li></ul><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Linnainmaa, S. (1976). Taylor expansion of the accumulated rounding error. <em>BIT Numerical Mathematics</em>, 16(2), 146-160.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. <em>Nature</em>, 323, 533–536.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_07/hw/">« Homework</a><a class="docs-footer-nextpage" href="../lab/">Lab »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Monday 6 December 2021 11:33">Monday 6 December 2021</span>. Using Julia version 1.7.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
