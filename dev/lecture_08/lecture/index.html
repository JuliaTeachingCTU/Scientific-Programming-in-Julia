<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Lecture · Scientific Programming in Julia</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_08/lecture/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/onlinestats.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Scientific Programming in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Scientific Programming in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Programming in Julia</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation/">Installation</a></li><li><a class="tocitem" href="../../projects/">Projects</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Introduction</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/motivation/">Motivation</a></li><li><a class="tocitem" href="../../lecture_01/basics/">Basics</a></li><li><a class="tocitem" href="../../lecture_01/demo/">Examples</a></li><li><a class="tocitem" href="../../lecture_01/outline/">Outline</a></li><li><a class="tocitem" href="../../lecture_01/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_01/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: The power of Type System &amp; multiple dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_02/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_02/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Design Patterns</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_03/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_03/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Package development, Unit test &amp; CI</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_04/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_04/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Performance Benchmarking</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_05/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_05/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Language introspection</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_06/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_06/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Macros</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_07/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_07/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox" checked/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Automatic differentiation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Lecture</a><ul class="internal"><li><a class="tocitem" href="#Motivation"><span>Motivation</span></a></li><li><a class="tocitem" href="#Theory"><span>Theory</span></a></li><li><a class="tocitem" href="#Calculation-of-the-Forward-mode"><span>Calculation of the Forward mode</span></a></li><li><a class="tocitem" href="#Reverse-mode"><span>Reverse mode</span></a></li><li><a class="tocitem" href="#Implementation-details-of-reverse-AD"><span>Implementation details of reverse AD</span></a></li><li><a class="tocitem" href="#ChainRules"><span>ChainRules</span></a></li><li><a class="tocitem" href="#Source-to-source-AD-using-Wengert"><span>Source-to-source AD using Wengert</span></a></li><li><a class="tocitem" href="#ChainRules-2"><span>ChainRules</span></a></li></ul></li><li><a class="tocitem" href="../lab/">Lab</a></li><li><a class="tocitem" href="../hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Intermediate representation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_09/lab/">Lab</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Parallel programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_10/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_10/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">12: Uncertainty propagation in ODE</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_12/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_12/hw/">Homework</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">8: Automatic differentiation</a></li><li class="is-active"><a href>Lecture</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Lecture</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/2022W/docs/src/lecture_08/lecture.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Automatic-Differentiation"><a class="docs-heading-anchor" href="#Automatic-Differentiation">Automatic Differentiation</a><a id="Automatic-Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation" title="Permalink"></a></h1><h2 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h2><ul><li>It supports a lot of modern machine learning by allowing quick differentiation of complex mathematical functions. The 1st order optimization methods are ubiquitous in finding parameters of functions (not only in  deep learning).</li><li>AD is interesting to study from the implementation perspective. There are different takes on it with different trade-offs and Julia offers many implementations (some of them are not maintained anymore).</li><li>We (authors of this course) believe that it is good to understand (at least roughly), how the methods work in order to use them effectively in our work.</li><li>Julia is unique in the effort separating definitions of AD rules from AD engines that use those rules to perform the AD. This allows authors of generic libraries to add new rules that would be compatible with many frameworks. See <a href="https://juliadiff.org/">juliadiff.org</a> for a list.</li></ul><h2 id="Theory"><a class="docs-heading-anchor" href="#Theory">Theory</a><a id="Theory-1"></a><a class="docs-heading-anchor-permalink" href="#Theory" title="Permalink"></a></h2><p>The differentiation is routine process, as most of the time we break complicated functions down into small pieces that we know, how to differentiate and from that to assemble the gradient of the complex function back. Thus, the essential piece is the differentiation of the composed function <span>$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span></p><p class="math-container">\[f(x) = f_1(f_2(f_3(\ldots f_n(x)))) = (f_1 \circ f_2 \circ \ldots \circ f_n)(x)\]</p><p>which is computed by chainrule. Before we dive into the details, let&#39;s define the notation, which for the sake of clarity needs to be precise. The gradient of function <span>$f(x)$</span> with respect to <span>$x$</span> at point <span>$x_0$</span> will be denoted as  <span>$\left.\frac{\partial f}{\partial x}\right|_{x^0}$</span></p><p>For a composed function <span>$f(x)$</span> the gradient with respect to <span>$x$</span> at point <span>$x_0$</span> is equal to</p><p class="math-container">\[\left.\frac{\partial f}{\partial x}\right|_{x^0} = \left.\frac{f_1}{\partial y_1}\right|_{y_1^0} \times \left.\frac{f_2}{\partial y_2}\right|_{y_2^0} \times \ldots \times \left.\frac{f_n}{\partial y_n}\right|_{y_n^0},\]</p><p>where <span>$y_i$</span> denotes the input of function <span>$f_i$</span> and</p><p class="math-container">\[\begin{alignat*}{2}
y_i^0 = &amp;\ \left(f_{i+1} \circ \ldots \circ f_n\right) (x^0) \\
y_n^0 = &amp;\ x^0 \\
y_0^0 = &amp;\ f(x^0) \\
\end{alignat*}\]</p><p>How <span>$\left.\frac{f_i}{\partial y_i}\right|_{y_i^0}$</span> looks like? </p><ul><li>If <span>$f_i: \mathbb{R} \rightarrow \mathbb{R}$</span>, then <span>$\frac{f_i}{\partial y_i} \in \mathbb{R}$</span> is a real number <span>$\mathbb{R}$</span> and we live in a high-school world, where it was sufficient to multiply real numbers.</li><li>If <span>$f_i: \mathbb{R}^{m_i} \rightarrow \mathbb{R}^{n_i}$</span>, then <span>$\mathbf{J}_i = \left.\frac{f_i}{\partial y_i}\right|_{y_i^0} \in \mathbb{R}^{n_i,m_i}$</span> is a matrix with <span>$m_i$</span> rows and <span>$n_i$</span> columns. </li></ul><p>The computation of gradient <span>$\frac{\partial f}{\partial x}$</span> <em>theoretically</em> boils down to </p><ol><li>computing Jacobians <span>$\left\{\mathbf{J}_i\right\}_{i=1}^n$</span> </li><li>multiplication of Jacobians as it holds that <span>$\left.\frac{\partial f}{\partial x}\right|_{y_0} = J_1 \times J_2 \times \ldots \times J_n$</span>. </li></ol><p>The complexity of the computation (at least one part of it) is therefore therefore determined by the  Matrix multiplication, which is generally expensive, as theoretically it has complexity at least <span>$O(n^{2.3728596}),$</span> but in practice a little bit more as the lower bound hides the devil in the <span>$O$</span> notation. The order in which the Jacobians are multiplied has therefore a profound effect on the complexity of the AD engine. While determining the optimal order of multiplication of sequence of matrices is costly, in practice, we recognize two important cases.</p><ol><li>Jacobians are multiplied from right to left as  <span>$J_1 \times (J_2 \times ( \ldots \times (J_{n-1} \times J_n) \ldots))$</span> which has the advantage when the input dimension of <span>$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span> is smaller than the output dimension, <span>$n &lt; m$</span>. - referred to as the <strong>FORWARD MODE</strong></li><li>Jacobians are multiplied from left to right as <span>$( \ldots ((J_1 \times J_2) \times J_3) \times \ldots ) \times J_n$</span> which has the advantage when the input dimension of <span>$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span> is larger than the output dimension, <span>$n &gt; m$</span>. - referred to as the <strong>BACKWARD MODE</strong></li></ol><p>The ubiquitous in machine learning to minimization of a scalar (loss) function of a large number of parameters. Also notice that for <code>f</code> of certain structures, it pays-off to do a mixed-mode AD, where some parts are done using forward diff and some parts using reverse diff. </p><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><p>Let&#39;s workout an example</p><p class="math-container">\[z = xy + sin(x)\]</p><p>How it maps to the notation we have used above? Particularly, what are <span>$f_1, f_2, \ldots, f_n$</span> and the corresponding <span>$\{y_i\}_{i=1}^n$</span>, such that <span>$(f_1 \circ f_2 \circ \ldots \circ f_n)(x,y) = xy + sin(x)$</span> ?</p><p class="math-container">\[\begin{alignat*}{6}
f_1:&amp;\mathbb{R}^2 \rightarrow \mathbb{R} \quad&amp;f_1(y_1)&amp; = y_{1,1} + y_{1,2}            \quad &amp; y_0 = &amp; (xy + \sin(x))           \\
f_2:&amp;\mathbb{R}^3 \rightarrow \mathbb{R}^2 \quad&amp;f_2(y_2)&amp; = (y_{2,1}y_{2,2}, y_{2,3}) \quad &amp; y_1 = &amp;  (xy, \sin(x))&amp;\\
f_3:&amp; \mathbb{R}^2 \rightarrow \mathbb{R}^3 \quad&amp;f_3(y_3)&amp; = (y_{3,1}, y_{3,2}, \sin(y_{3,1}))  \quad &amp; y_2 =&amp; (x, y, \sin(x))\\
\end{alignat*}\]</p><p>The corresponding jacobians are </p><p class="math-container">\[\begin{alignat*}{4}
f_1(y_1) &amp; = y_{1,1} + y_{1,2}             \quad &amp; \mathbf{J}_1&amp; = \begin{bmatrix} 1 \\ 1 \end{bmatrix}  \\
f_2(y_2) &amp; = (y_{2,1}y_{2,2}, y_{2,3})     \quad &amp; \mathbf{J}_2&amp; = \begin{bmatrix} y_{2, 2} &amp; 0 \\ y_{2,1} &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\\
f_3(y_3) &amp; = (y_{3,1}, y_{3,2}, \sin(y_{3,1}))     \quad &amp; \mathbf{J}_3 &amp; = \begin{bmatrix} 1 &amp; 0 &amp; \cos(y_{3,1}) \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \\
\end{alignat*}\]</p><p>and for the gradient it holds that</p><p class="math-container">\[\begin{bmatrix} \frac{\partial f(x, y)}{\partial{x}} \\ \frac{\partial f(x,y)}{\partial{y}} \end{bmatrix} = \mathbf{J}_3 \times \mathbf{J}_2 \times \mathbf{J}_1 =  \begin{bmatrix} 1 &amp; 0 &amp; \cos(x) \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \\  \times \begin{bmatrix} y &amp; 0 \\ x &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \times \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} y &amp; \cos(x) \\ x &amp; 0 \end{bmatrix} \times \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} y + \cos(x) \\ x \end{bmatrix}\]</p><p>Note that from theoretical point of view this decomposition of a function is not unique, however as we will see later it usually given by the computational graph in a particular language/environment.</p><h2 id="Calculation-of-the-Forward-mode"><a class="docs-heading-anchor" href="#Calculation-of-the-Forward-mode">Calculation of the Forward mode</a><a id="Calculation-of-the-Forward-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Calculation-of-the-Forward-mode" title="Permalink"></a></h2><p>In theory, we can calculate the gradient using forward mode as follows Initialize the Jacobian of <span>$y_n$</span> with respect to <span>$x$</span> to an identity matrix, because as we have stated above <span>$y^0_n = x$</span>, i.e. <span>$\frac{\partial y_n}{\partial x} = \mathbb{I}$</span>. Iterate <code>i</code> from <code>n</code> down to <code>1</code> as</p><ul><li>calculate the next intermediate output as <span>$y^0_{i-1} = f_i({y^0_i})$</span> </li><li>calculate Jacobian <span>$J_i = \left.\frac{f_i}{\partial y_i}\right|_{y^0_i}$</span></li><li><em>push forward</em> the gradient as <span>$\left.\frac{\partial y_{i-1}}{\partial x}\right|_x = J_i \times \left.\frac{\partial y_n}{\partial x}\right|_x$</span></li></ul><p>Notice that </p><ul><li>on the very end, we are left with <span>$y = y^0_0$</span> and with <span>$\frac{\partial y_0}{\partial x}$</span>, which is the gradient we wanted to calculate;</li><li>if <code>y</code> is a scalar, then <span>$\frac{\partial y_0}{\partial x}$</span> is a matrix with single row</li><li>the Jacobian and the output of the function is calculated in one sweep.</li></ul><p>The above is an idealized computation. The real implementation is a bit different, as we will see later.</p><h3 id="Implementation-of-the-forward-mode-using-Dual-numbers"><a class="docs-heading-anchor" href="#Implementation-of-the-forward-mode-using-Dual-numbers">Implementation of the forward mode using Dual numbers</a><a id="Implementation-of-the-forward-mode-using-Dual-numbers-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-of-the-forward-mode-using-Dual-numbers" title="Permalink"></a></h3><p>Forward modes need to keep track of the output of the function and of the derivative at each computation step in the computation of the complicated function <span>$f$</span>. This can be elegantly realized with a <a href="https://en.wikipedia.org/wiki/Dual_number"><strong>dual number</strong></a>, which are conceptually similar to complex numbers, but instead of the imaginary number <span>$i$</span> dual numbers use <span>$\epsilon$</span> in its second component:</p><p class="math-container">\[x = v + \dot v \epsilon,\]</p><p>where <span>$(v,\dot v) \in \mathbb R$</span> and by definition <span>$\epsilon^2=0$</span> (instead of <span>$i^2=-1$</span> in complex numbers). What are the properties of these Dual numbers?</p><p class="math-container">\[\begin{align}
(v + \dot v \epsilon) + (u + \dot u \epsilon) &amp;= (v + u) + (\dot v + \dot u)\epsilon  \\
(v + \dot v \epsilon)(u + \dot u \epsilon) &amp;= vu + (u\dot v + \dot u v)\epsilon + \dot v \dot u \epsilon^2 = vu + (u\dot v + \dot u v)\epsilon \\
\frac{v + \dot v \epsilon}{u + \dot u \epsilon} &amp;= \frac{v + \dot v \epsilon}{u + \dot u \epsilon} \frac{u - \dot u \epsilon}{u - \dot u \epsilon} = \frac{v}{u} - \frac{(\dot u v - u \dot v)\epsilon}{u^2}
\end{align}\]</p><h4 id="How-are-dual-numbers-related-to-differentiation?"><a class="docs-heading-anchor" href="#How-are-dual-numbers-related-to-differentiation?">How are dual numbers related to differentiation?</a><a id="How-are-dual-numbers-related-to-differentiation?-1"></a><a class="docs-heading-anchor-permalink" href="#How-are-dual-numbers-related-to-differentiation?" title="Permalink"></a></h4><p>Let&#39;s evaluate the above equations at <span>$(v, \dot v) = (v, 1)$</span> and <span>$(u, \dot u) = (u, 0)$</span> we obtain </p><p class="math-container">\[\begin{align}
(v + \dot v \epsilon) + (u + \dot u \epsilon) &amp;= (v + u) + 1\epsilon  \\
(v + \dot v \epsilon)(u + \dot u \epsilon) &amp;= vu + u\epsilon\\
\frac{v + \dot v \epsilon}{u + \dot u \epsilon} &amp;= \frac{v}{u}  + \frac{1}{u} \epsilon
\end{align}\]</p><p>and notice that terms <span>$(1, u, \frac{1}{u})$</span> corresponds to gradient of functions <span>$(u+v, uv, \frac{v}{u})$</span> with respect to <span>$v$</span>. We can repeat it with changed values of <span>$\epsilon$</span> as <span>$(v, \dot v) = (v, 0)$</span> and <span>$(u, \dot u) = (u, 1)$</span> and we obtain</p><p class="math-container">\[\begin{align}
(v + \dot v \epsilon) + (u + \dot u \epsilon) &amp;= (v + u) + 1\epsilon  \\
(v + \dot v \epsilon)(u + \dot u \epsilon) &amp;= vu + v\epsilon\\
\frac{v + \dot v \epsilon}{u + \dot u \epsilon} &amp;= \frac{v}{u}  - \frac{v}{u^2} \epsilon
\end{align}\]</p><p>meaning that at this moment we have obtained gradients with respect to <span>$u$</span>.</p><p>All above functions <span>$(u+v, uv, \frac{u}{v})$</span> are of <span>$\mathbb{R}^2 \rightarrow \mathbb{R}$</span>, therefore we had to repeat the calculations twice to get gradients with respect to both inputs. This is inline with the above theory, where we have said that if input dimension is larger then output dimension, the backward mode is better. But consider a case, where we have a function </p><p class="math-container">\[f(v) = (v + 5, 5*v, 5 / v) \]</p><p>which is <span>$\mathbb{R} \rightarrow \mathbb{R}^3$</span>. In this case, we obtain the Jacobian <span>$[1, 5, -\frac{5}{v^2}]$</span> in a single forward pass (whereas the reverse would require three passes over the backward calculation, as will be seen later).</p><h4 id="Does-dual-numbers-work-universally?"><a class="docs-heading-anchor" href="#Does-dual-numbers-work-universally?">Does dual numbers work universally?</a><a id="Does-dual-numbers-work-universally?-1"></a><a class="docs-heading-anchor-permalink" href="#Does-dual-numbers-work-universally?" title="Permalink"></a></h4><p>Let&#39;s first work out polynomial. Let&#39;s assume the polynomial</p><p class="math-container">\[p(v) = \sum_{i=1}^n p_iv^i\]</p><p>and compute its value at <span>$v + \dot v \epsilon$</span> (note that we know how to do addition and multiplication)</p><p class="math-container">\[\begin{split}
p(v) &amp;= 
    \sum_{i=0}^n p_i(v + \dot{v} \epsilon )^i = 
    \sum_{i=0}^n \left[p_i \sum_{j=0}^{n}\binom{i}{j}v^{i-j}(\dot v \epsilon)^{i}\right] = 
    p_0 + \sum_{i=1}^n \left[p_i \sum_{j=0}^{1}\binom{i}{j}v^{i-j}(\dot v \epsilon)^{j}\right] = \\
    &amp;= p_0 + \sum_{i=1}^n p_i(v^i + i v^{i-1} \dot v \epsilon ) 
    = p(v) + \left(\sum_{i=1}^n ip_i v^{i-1}\right) \dot v \epsilon
\end{split}\]</p><p>where in the multiplier of <span>$\dot{v} \epsilon$</span>: <span>$\sum_{i=1}^n ip_i v^{i - 1}$</span>, we recognize the derivative of <span>$p(v)$</span> with respect to <span>$v$</span>. This proves that Dual numbers can be used to calculate the gradient of polynomials.</p><p>Let&#39;s now consider a general function <span>$f:\mathbb{R} \rightarrow \mathbb{R}$</span>. Its value at point <span>$v + \dot v \epsilon$</span> can be approximated using Taylor expansion at function at point <span>$v$</span> as</p><p class="math-container">\[f(v+\dot v \epsilon) = \sum_{i=0}^\infty \frac{f^i(v)\dot v^i\epsilon^n}{i!}
  = f(v) + f&#39;(v)\dot v\epsilon,\]</p><p>where all higher order terms can be dropped because <span>$\epsilon^i=0$</span> for <span>$i&gt;1$</span>. This shows that we can calculate the gradient of <span>$f$</span> at point <span>$v$</span> by calculating its value at <span>$f(v + \epsilon)$</span> and taking the multiplier of <span>$\epsilon$</span>.</p><h4 id="Implementing-Dual-number-with-Julia"><a class="docs-heading-anchor" href="#Implementing-Dual-number-with-Julia">Implementing Dual number with Julia</a><a id="Implementing-Dual-number-with-Julia-1"></a><a class="docs-heading-anchor-permalink" href="#Implementing-Dual-number-with-Julia" title="Permalink"></a></h4><p>To demonstrate the simplicity of Dual numbers, consider following definition of Dual numbers, where we define a new number type and overload functions <code>+</code>, <code>-</code>, <code>*</code>, and <code>/</code>.  In Julia, this reads:</p><pre><code class="language-julia hljs">struct Dual{T&lt;:Number} &lt;: Number
    x::T
    d::T
end

Base.:+(a::Dual, b::Dual)   = Dual(a.x+b.x, a.d+b.d)
Base.:-(a::Dual, b::Dual)   = Dual(a.x-b.x, a.d-b.d)
Base.:/(a::Dual, b::Dual)   = Dual(a.x/b.x, (a.d*b.x - a.x*b.d)/b.x^2) # recall  (a/b) =  a/b + (a&#39;b - ab&#39;)/b^2 ϵ
Base.:*(a::Dual, b::Dual)   = Dual(a.x*b.x, a.d*b.x + a.x*b.d)

# Let&#39;s define some promotion rules
Dual(x::S, d::T) where {S&lt;:Number, T&lt;:Number} = Dual{promote_type(S, T)}(x, d)
Dual(x::Number) = Dual(x, zero(typeof(x)))
Dual{T}(x::Number) where {T} = Dual(T(x), zero(T))
Base.promote_rule(::Type{Dual{T}}, ::Type{S}) where {T&lt;:Number,S&lt;:Number} = Dual{promote_type(T,S)}
Base.promote_rule(::Type{Dual{T}}, ::Type{Dual{S}}) where {T&lt;:Number,S&lt;:Number} = Dual{promote_type(T,S)}

# and define api for forward differentionation
forward_diff(f::Function, x::Real) = _dual(f(Dual(x,1.0)))
_dual(x::Dual) = x.d
_dual(x::Vector) = _dual.(x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">_dual (generic function with 2 methods)</code></pre><p>And let&#39;s test the <strong><em>Babylonian Square Root</em></strong> (an algorithm to compute <span>$\sqrt x$</span>):</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; babysqrt(x, t=(1+x)/2, n=10) = n==0 ? t : babysqrt(x, (t+x/t)/2, n-1)</code><code class="nohighlight hljs ansi" style="display:block;">babysqrt (generic function with 3 methods)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_diff(babysqrt, 2)</code><code class="nohighlight hljs ansi" style="display:block;">0.35355339059327373</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_diff(babysqrt, 2) ≈ 1/(2sqrt(2))</code><code class="nohighlight hljs ansi" style="display:block;">true</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_diff(x -&gt; [1 + x, 5x, 5/x], 2)</code><code class="nohighlight hljs ansi" style="display:block;">3-element Vector{Float64}:
  1.0
  5.0
 -1.25</code></pre><p>We now compare the analytic solution to values computed by the <code>forward_diff</code> and byt he finite differencing</p><p class="math-container">\[f(x) = \sqrt{x} \qquad f&#39;(x) = \frac{1}{2\sqrt{x}}\]</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using FiniteDifferences</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_dsqrt(x) = forward_diff(babysqrt,x)</code><code class="nohighlight hljs ansi" style="display:block;">forward_dsqrt (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; analytc_dsqrt(x) = 1/(2babysqrt(x))</code><code class="nohighlight hljs ansi" style="display:block;">analytc_dsqrt (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; forward_dsqrt(2.0)</code><code class="nohighlight hljs ansi" style="display:block;">0.35355339059327373</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; analytc_dsqrt(2.0)</code><code class="nohighlight hljs ansi" style="display:block;">0.3535533905932738</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; central_fdm(5, 1)(babysqrt, 2.0)</code><code class="nohighlight hljs ansi" style="display:block;">0.3535533905932651</code></pre><pre><code class="language-julia hljs">plot(0.0:0.01:2, babysqrt, label=&quot;f(x) = babysqrt(x)&quot;, lw=3)
plot!(0.1:0.01:2, analytc_dsqrt, label=&quot;Analytic f&#39;&quot;, ls=:dot, lw=3)
plot!(0.1:0.01:2, forward_dsqrt, label=&quot;Dual Forward Mode f&#39;&quot;, lw=3, ls=:dash)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip410">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip410)" d="
M0 1600 L2400 1600 L2400 0 L0 0  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip411">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip410)" d="
M156.112 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.112 47.2441  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip412">
    <rect x="156" y="47" width="2198" height="1440"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip412)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  218.281,1486.45 218.281,47.2441 
  "/>
<polyline clip-path="url(#clip412)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  736.358,1486.45 736.358,47.2441 
  "/>
<polyline clip-path="url(#clip412)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1254.43,1486.45 1254.43,47.2441 
  "/>
<polyline clip-path="url(#clip412)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1772.51,1486.45 1772.51,47.2441 
  "/>
<polyline clip-path="url(#clip412)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  2290.59,1486.45 2290.59,47.2441 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,1486.45 2352.76,1486.45 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  218.281,1486.45 218.281,1467.55 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  736.358,1486.45 736.358,1467.55 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1254.43,1486.45 1254.43,1467.55 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1772.51,1486.45 1772.51,1467.55 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  2290.59,1486.45 2290.59,1467.55 
  "/>
<path clip-path="url(#clip410)" d="M195.666 1517.37 Q192.055 1517.37 190.226 1520.93 Q188.42 1524.47 188.42 1531.6 Q188.42 1538.71 190.226 1542.27 Q192.055 1545.82 195.666 1545.82 Q199.3 1545.82 201.105 1542.27 Q202.934 1538.71 202.934 1531.6 Q202.934 1524.47 201.105 1520.93 Q199.3 1517.37 195.666 1517.37 M195.666 1513.66 Q201.476 1513.66 204.531 1518.27 Q207.61 1522.85 207.61 1531.6 Q207.61 1540.33 204.531 1544.94 Q201.476 1549.52 195.666 1549.52 Q189.856 1549.52 186.777 1544.94 Q183.721 1540.33 183.721 1531.6 Q183.721 1522.85 186.777 1518.27 Q189.856 1513.66 195.666 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M215.828 1542.97 L220.712 1542.97 L220.712 1548.85 L215.828 1548.85 L215.828 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M240.897 1517.37 Q237.286 1517.37 235.457 1520.93 Q233.652 1524.47 233.652 1531.6 Q233.652 1538.71 235.457 1542.27 Q237.286 1545.82 240.897 1545.82 Q244.531 1545.82 246.337 1542.27 Q248.165 1538.71 248.165 1531.6 Q248.165 1524.47 246.337 1520.93 Q244.531 1517.37 240.897 1517.37 M240.897 1513.66 Q246.707 1513.66 249.763 1518.27 Q252.841 1522.85 252.841 1531.6 Q252.841 1540.33 249.763 1544.94 Q246.707 1549.52 240.897 1549.52 Q235.087 1549.52 232.008 1544.94 Q228.953 1540.33 228.953 1531.6 Q228.953 1522.85 232.008 1518.27 Q235.087 1513.66 240.897 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M714.24 1517.37 Q710.629 1517.37 708.8 1520.93 Q706.994 1524.47 706.994 1531.6 Q706.994 1538.71 708.8 1542.27 Q710.629 1545.82 714.24 1545.82 Q717.874 1545.82 719.68 1542.27 Q721.508 1538.71 721.508 1531.6 Q721.508 1524.47 719.68 1520.93 Q717.874 1517.37 714.24 1517.37 M714.24 1513.66 Q720.05 1513.66 723.105 1518.27 Q726.184 1522.85 726.184 1531.6 Q726.184 1540.33 723.105 1544.94 Q720.05 1549.52 714.24 1549.52 Q708.43 1549.52 705.351 1544.94 Q702.295 1540.33 702.295 1531.6 Q702.295 1522.85 705.351 1518.27 Q708.43 1513.66 714.24 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M734.402 1542.97 L739.286 1542.97 L739.286 1548.85 L734.402 1548.85 L734.402 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M749.517 1514.29 L767.874 1514.29 L767.874 1518.22 L753.8 1518.22 L753.8 1526.7 Q754.818 1526.35 755.837 1526.19 Q756.855 1526 757.874 1526 Q763.661 1526 767.04 1529.17 Q770.42 1532.34 770.42 1537.76 Q770.42 1543.34 766.948 1546.44 Q763.476 1549.52 757.156 1549.52 Q754.98 1549.52 752.712 1549.15 Q750.466 1548.78 748.059 1548.04 L748.059 1543.34 Q750.142 1544.47 752.365 1545.03 Q754.587 1545.58 757.064 1545.58 Q761.068 1545.58 763.406 1543.48 Q765.744 1541.37 765.744 1537.76 Q765.744 1534.15 763.406 1532.04 Q761.068 1529.94 757.064 1529.94 Q755.189 1529.94 753.314 1530.35 Q751.462 1530.77 749.517 1531.65 L749.517 1514.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M1221.59 1544.91 L1229.23 1544.91 L1229.23 1518.55 L1220.92 1520.21 L1220.92 1515.95 L1229.18 1514.29 L1233.86 1514.29 L1233.86 1544.91 L1241.49 1544.91 L1241.49 1548.85 L1221.59 1548.85 L1221.59 1544.91 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M1250.94 1542.97 L1255.82 1542.97 L1255.82 1548.85 L1250.94 1548.85 L1250.94 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M1276.01 1517.37 Q1272.4 1517.37 1270.57 1520.93 Q1268.76 1524.47 1268.76 1531.6 Q1268.76 1538.71 1270.57 1542.27 Q1272.4 1545.82 1276.01 1545.82 Q1279.64 1545.82 1281.45 1542.27 Q1283.28 1538.71 1283.28 1531.6 Q1283.28 1524.47 1281.45 1520.93 Q1279.64 1517.37 1276.01 1517.37 M1276.01 1513.66 Q1281.82 1513.66 1284.87 1518.27 Q1287.95 1522.85 1287.95 1531.6 Q1287.95 1540.33 1284.87 1544.94 Q1281.82 1549.52 1276.01 1549.52 Q1270.2 1549.52 1267.12 1544.94 Q1264.06 1540.33 1264.06 1531.6 Q1264.06 1522.85 1267.12 1518.27 Q1270.2 1513.66 1276.01 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M1740.16 1544.91 L1747.8 1544.91 L1747.8 1518.55 L1739.49 1520.21 L1739.49 1515.95 L1747.75 1514.29 L1752.43 1514.29 L1752.43 1544.91 L1760.07 1544.91 L1760.07 1548.85 L1740.16 1548.85 L1740.16 1544.91 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M1769.51 1542.97 L1774.4 1542.97 L1774.4 1548.85 L1769.51 1548.85 L1769.51 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M1784.63 1514.29 L1802.98 1514.29 L1802.98 1518.22 L1788.91 1518.22 L1788.91 1526.7 Q1789.93 1526.35 1790.95 1526.19 Q1791.97 1526 1792.98 1526 Q1798.77 1526 1802.15 1529.17 Q1805.53 1532.34 1805.53 1537.76 Q1805.53 1543.34 1802.06 1546.44 Q1798.59 1549.52 1792.27 1549.52 Q1790.09 1549.52 1787.82 1549.15 Q1785.58 1548.78 1783.17 1548.04 L1783.17 1543.34 Q1785.25 1544.47 1787.48 1545.03 Q1789.7 1545.58 1792.17 1545.58 Q1796.18 1545.58 1798.52 1543.48 Q1800.86 1541.37 1800.86 1537.76 Q1800.86 1534.15 1798.52 1532.04 Q1796.18 1529.94 1792.17 1529.94 Q1790.3 1529.94 1788.42 1530.35 Q1786.57 1530.77 1784.63 1531.65 L1784.63 1514.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M2261.83 1544.91 L2278.14 1544.91 L2278.14 1548.85 L2256.2 1548.85 L2256.2 1544.91 Q2258.86 1542.16 2263.45 1537.53 Q2268.05 1532.88 2269.23 1531.53 Q2271.48 1529.01 2272.36 1527.27 Q2273.26 1525.51 2273.26 1523.82 Q2273.26 1521.07 2271.32 1519.33 Q2269.39 1517.6 2266.29 1517.6 Q2264.09 1517.6 2261.64 1518.36 Q2259.21 1519.13 2256.43 1520.68 L2256.43 1515.95 Q2259.26 1514.82 2261.71 1514.24 Q2264.16 1513.66 2266.2 1513.66 Q2271.57 1513.66 2274.77 1516.35 Q2277.96 1519.03 2277.96 1523.52 Q2277.96 1525.65 2277.15 1527.57 Q2276.36 1529.47 2274.26 1532.07 Q2273.68 1532.74 2270.58 1535.95 Q2267.47 1539.15 2261.83 1544.91 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M2287.96 1542.97 L2292.84 1542.97 L2292.84 1548.85 L2287.96 1548.85 L2287.96 1542.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M2313.03 1517.37 Q2309.42 1517.37 2307.59 1520.93 Q2305.78 1524.47 2305.78 1531.6 Q2305.78 1538.71 2307.59 1542.27 Q2309.42 1545.82 2313.03 1545.82 Q2316.66 1545.82 2318.47 1542.27 Q2320.3 1538.71 2320.3 1531.6 Q2320.3 1524.47 2318.47 1520.93 Q2316.66 1517.37 2313.03 1517.37 M2313.03 1513.66 Q2318.84 1513.66 2321.89 1518.27 Q2324.97 1522.85 2324.97 1531.6 Q2324.97 1540.33 2321.89 1544.94 Q2318.84 1549.52 2313.03 1549.52 Q2307.22 1549.52 2304.14 1544.94 Q2301.08 1540.33 2301.08 1531.6 Q2301.08 1522.85 2304.14 1518.27 Q2307.22 1513.66 2313.03 1513.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip412)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  156.112,1446.14 2352.76,1446.14 
  "/>
<polyline clip-path="url(#clip412)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  156.112,1016.65 2352.76,1016.65 
  "/>
<polyline clip-path="url(#clip412)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  156.112,587.16 2352.76,587.16 
  "/>
<polyline clip-path="url(#clip412)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  156.112,157.673 2352.76,157.673 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,1486.45 156.112,47.2441 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,1446.14 175.01,1446.14 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,1016.65 175.01,1016.65 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,587.16 175.01,587.16 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  156.112,157.673 175.01,157.673 
  "/>
<path clip-path="url(#clip410)" d="M62.9365 1431.93 Q59.3254 1431.93 57.4967 1435.5 Q55.6912 1439.04 55.6912 1446.17 Q55.6912 1453.28 57.4967 1456.84 Q59.3254 1460.38 62.9365 1460.38 Q66.5707 1460.38 68.3763 1456.84 Q70.205 1453.28 70.205 1446.17 Q70.205 1439.04 68.3763 1435.5 Q66.5707 1431.93 62.9365 1431.93 M62.9365 1428.23 Q68.7467 1428.23 71.8022 1432.84 Q74.8809 1437.42 74.8809 1446.17 Q74.8809 1454.9 71.8022 1459.5 Q68.7467 1464.09 62.9365 1464.09 Q57.1264 1464.09 54.0477 1459.5 Q50.9921 1454.9 50.9921 1446.17 Q50.9921 1437.42 54.0477 1432.84 Q57.1264 1428.23 62.9365 1428.23 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M83.0984 1457.54 L87.9827 1457.54 L87.9827 1463.42 L83.0984 1463.42 L83.0984 1457.54 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M108.168 1431.93 Q104.557 1431.93 102.728 1435.5 Q100.922 1439.04 100.922 1446.17 Q100.922 1453.28 102.728 1456.84 Q104.557 1460.38 108.168 1460.38 Q111.802 1460.38 113.608 1456.84 Q115.436 1453.28 115.436 1446.17 Q115.436 1439.04 113.608 1435.5 Q111.802 1431.93 108.168 1431.93 M108.168 1428.23 Q113.978 1428.23 117.033 1432.84 Q120.112 1437.42 120.112 1446.17 Q120.112 1454.9 117.033 1459.5 Q113.978 1464.09 108.168 1464.09 Q102.358 1464.09 99.2789 1459.5 Q96.2234 1454.9 96.2234 1446.17 Q96.2234 1437.42 99.2789 1432.84 Q102.358 1428.23 108.168 1428.23 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M63.9319 1002.45 Q60.3208 1002.45 58.4921 1006.01 Q56.6865 1009.55 56.6865 1016.68 Q56.6865 1023.79 58.4921 1027.35 Q60.3208 1030.9 63.9319 1030.9 Q67.5661 1030.9 69.3717 1027.35 Q71.2004 1023.79 71.2004 1016.68 Q71.2004 1009.55 69.3717 1006.01 Q67.5661 1002.45 63.9319 1002.45 M63.9319 998.743 Q69.742 998.743 72.7976 1003.35 Q75.8763 1007.93 75.8763 1016.68 Q75.8763 1025.41 72.7976 1030.02 Q69.742 1034.6 63.9319 1034.6 Q58.1217 1034.6 55.043 1030.02 Q51.9875 1025.41 51.9875 1016.68 Q51.9875 1007.93 55.043 1003.35 Q58.1217 998.743 63.9319 998.743 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M84.0938 1028.05 L88.978 1028.05 L88.978 1033.93 L84.0938 1033.93 L84.0938 1028.05 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M99.2095 999.368 L117.566 999.368 L117.566 1003.3 L103.492 1003.3 L103.492 1011.77 Q104.51 1011.43 105.529 1011.27 Q106.547 1011.08 107.566 1011.08 Q113.353 1011.08 116.733 1014.25 Q120.112 1017.42 120.112 1022.84 Q120.112 1028.42 116.64 1031.52 Q113.168 1034.6 106.848 1034.6 Q104.672 1034.6 102.404 1034.23 Q100.159 1033.86 97.7511 1033.12 L97.7511 1028.42 Q99.8345 1029.55 102.057 1030.11 Q104.279 1030.66 106.756 1030.66 Q110.76 1030.66 113.098 1028.56 Q115.436 1026.45 115.436 1022.84 Q115.436 1019.23 113.098 1017.12 Q110.76 1015.02 106.756 1015.02 Q104.881 1015.02 103.006 1015.43 Q101.154 1015.85 99.2095 1016.73 L99.2095 999.368 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M53.7467 600.505 L61.3856 600.505 L61.3856 574.139 L53.0754 575.806 L53.0754 571.547 L61.3393 569.88 L66.0152 569.88 L66.0152 600.505 L73.654 600.505 L73.654 604.44 L53.7467 604.44 L53.7467 600.505 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M83.0984 598.56 L87.9827 598.56 L87.9827 604.44 L83.0984 604.44 L83.0984 598.56 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M108.168 572.959 Q104.557 572.959 102.728 576.524 Q100.922 580.065 100.922 587.195 Q100.922 594.301 102.728 597.866 Q104.557 601.408 108.168 601.408 Q111.802 601.408 113.608 597.866 Q115.436 594.301 115.436 587.195 Q115.436 580.065 113.608 576.524 Q111.802 572.959 108.168 572.959 M108.168 569.255 Q113.978 569.255 117.033 573.862 Q120.112 578.445 120.112 587.195 Q120.112 595.922 117.033 600.528 Q113.978 605.111 108.168 605.111 Q102.358 605.111 99.2789 600.528 Q96.2234 595.922 96.2234 587.195 Q96.2234 578.445 99.2789 573.862 Q102.358 569.255 108.168 569.255 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M54.7421 171.017 L62.381 171.017 L62.381 144.652 L54.0708 146.318 L54.0708 142.059 L62.3347 140.393 L67.0106 140.393 L67.0106 171.017 L74.6494 171.017 L74.6494 174.953 L54.7421 174.953 L54.7421 171.017 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M84.0938 169.073 L88.978 169.073 L88.978 174.953 L84.0938 174.953 L84.0938 169.073 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M99.2095 140.393 L117.566 140.393 L117.566 144.328 L103.492 144.328 L103.492 152.8 Q104.51 152.453 105.529 152.291 Q106.547 152.105 107.566 152.105 Q113.353 152.105 116.733 155.277 Q120.112 158.448 120.112 163.865 Q120.112 169.443 116.64 172.545 Q113.168 175.624 106.848 175.624 Q104.672 175.624 102.404 175.253 Q100.159 174.883 97.7511 174.142 L97.7511 169.443 Q99.8345 170.578 102.057 171.133 Q104.279 171.689 106.756 171.689 Q110.76 171.689 113.098 169.582 Q115.436 167.476 115.436 163.865 Q115.436 160.254 113.098 158.147 Q110.76 156.041 106.756 156.041 Q104.881 156.041 103.006 156.457 Q101.154 156.874 99.2095 157.754 L99.2095 140.393 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip412)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none" points="
  218.281,1445.72 228.643,1360.24 239.004,1324.66 249.366,1297.36 259.727,1274.34 270.089,1254.06 280.45,1235.73 290.812,1218.87 301.174,1203.18 311.535,1188.44 
  321.897,1174.5 332.258,1161.25 342.62,1148.58 352.981,1136.43 363.343,1124.74 373.704,1113.46 384.066,1102.55 394.427,1091.97 404.789,1081.7 415.15,1071.72 
  425.512,1061.99 435.873,1052.5 446.235,1043.24 456.596,1034.19 466.958,1025.32 477.319,1016.65 487.681,1008.14 498.043,999.799 508.404,991.608 518.766,983.563 
  529.127,975.655 539.489,967.878 549.85,960.225 560.212,952.692 570.573,945.271 580.935,937.959 591.296,930.75 601.658,923.641 612.019,916.627 622.381,909.705 
  632.742,902.872 643.104,896.123 653.465,889.456 663.827,882.867 674.188,876.356 684.55,869.917 694.912,863.55 705.273,857.251 715.635,851.02 725.996,844.853 
  736.358,838.748 746.719,832.704 757.081,826.719 767.442,820.792 777.804,814.92 788.165,809.102 798.527,803.337 808.888,797.623 819.25,791.959 829.611,786.344 
  839.973,780.776 850.334,775.254 860.696,769.777 871.058,764.345 881.419,758.955 891.781,753.607 902.142,748.3 912.504,743.034 922.865,737.806 933.227,732.617 
  943.588,727.465 953.95,722.35 964.311,717.271 974.673,712.226 985.034,707.217 995.396,702.241 1005.76,697.298 1016.12,692.388 1026.48,687.509 1036.84,682.661 
  1047.2,677.844 1057.56,673.058 1067.93,668.3 1078.29,663.572 1088.65,658.871 1099.01,654.199 1109.37,649.554 1119.73,644.937 1130.1,640.345 1140.46,635.78 
  1150.82,631.24 1161.18,626.725 1171.54,622.235 1181.9,617.77 1192.26,613.328 1202.63,608.91 1212.99,604.515 1223.35,600.143 1233.71,595.793 1244.07,591.466 
  1254.43,587.16 1264.8,582.876 1275.16,578.613 1285.52,574.371 1295.88,570.149 1306.24,565.948 1316.6,561.766 1326.96,557.604 1337.33,553.462 1347.69,549.339 
  1358.05,545.234 1368.41,541.149 1378.77,537.081 1389.13,533.032 1399.5,529.001 1409.86,524.987 1420.22,520.991 1430.58,517.012 1440.94,513.049 1451.3,509.104 
  1461.66,505.175 1472.03,501.263 1482.39,497.366 1492.75,493.486 1503.11,489.621 1513.47,485.772 1523.83,481.938 1534.2,478.119 1544.56,474.316 1554.92,470.527 
  1565.28,466.753 1575.64,462.993 1586,459.248 1596.36,455.517 1606.73,451.8 1617.09,448.096 1627.45,444.407 1637.81,440.731 1648.17,437.068 1658.53,433.418 
  1668.9,429.782 1679.26,426.159 1689.62,422.548 1699.98,418.95 1710.34,415.365 1720.7,411.792 1731.06,408.232 1741.43,404.683 1751.79,401.147 1762.15,397.622 
  1772.51,394.11 1782.87,390.609 1793.23,387.12 1803.59,383.642 1813.96,380.175 1824.32,376.72 1834.68,373.276 1845.04,369.842 1855.4,366.42 1865.76,363.009 
  1876.13,359.608 1886.49,356.218 1896.85,352.838 1907.21,349.469 1917.57,346.11 1927.93,342.762 1938.29,339.423 1948.66,336.095 1959.02,332.776 1969.38,329.468 
  1979.74,326.169 1990.1,322.879 2000.46,319.6 2010.83,316.33 2021.19,313.069 2031.55,309.818 2041.91,306.576 2052.27,303.343 2062.63,300.119 2072.99,296.905 
  2083.36,293.699 2093.72,290.502 2104.08,287.314 2114.44,284.135 2124.8,280.965 2135.16,277.803 2145.53,274.649 2155.89,271.504 2166.25,268.368 2176.61,265.24 
  2186.97,262.12 2197.33,259.008 2207.69,255.904 2218.06,252.809 2228.42,249.721 2238.78,246.642 2249.14,243.57 2259.5,240.506 2269.86,237.45 2280.23,234.402 
  2290.59,231.361 
  "/>
<polyline clip-path="url(#clip412)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none" stroke-dasharray="6, 12" points="
  321.897,87.9763 332.258,151.181 342.62,206.311 352.981,254.951 363.343,298.281 373.704,337.203 384.066,372.416 394.427,404.475 404.789,433.823 415.15,460.823 
  425.512,485.772 435.873,508.916 446.235,530.465 456.596,550.592 466.958,569.447 477.319,587.16 487.681,603.841 498.043,619.586 508.404,634.48 518.766,648.597 
  529.127,662.002 539.489,674.753 549.85,686.901 560.212,698.493 570.573,709.57 580.935,720.169 591.296,730.323 601.658,740.062 612.019,749.414 622.381,758.405 
  632.742,767.056 643.104,775.388 653.465,783.421 663.827,791.173 674.188,798.658 684.55,805.893 694.912,812.89 705.273,819.663 715.635,826.223 725.996,832.581 
  736.358,838.748 746.719,844.732 757.081,850.543 767.442,856.189 777.804,861.677 788.165,867.014 798.527,872.208 808.888,877.265 819.25,882.19 829.611,886.99 
  839.973,891.669 850.334,896.233 860.696,900.685 871.058,905.032 881.419,909.276 891.781,913.421 902.142,917.473 912.504,921.433 922.865,925.305 933.227,929.093 
  943.588,932.799 953.95,936.427 964.311,939.979 974.673,943.458 985.034,946.866 995.396,950.206 1005.76,953.479 1016.12,956.689 1026.48,959.836 1036.84,962.924 
  1047.2,965.953 1057.56,968.927 1067.93,971.845 1078.29,974.711 1088.65,977.526 1099.01,980.29 1109.37,983.007 1119.73,985.676 1130.1,988.3 1140.46,990.879 
  1150.82,993.416 1161.18,995.91 1171.54,998.363 1181.9,1000.78 1192.26,1003.15 1202.63,1005.49 1212.99,1007.79 1223.35,1010.06 1233.71,1012.29 1244.07,1014.48 
  1254.43,1016.65 1264.8,1018.78 1275.16,1020.88 1285.52,1022.95 1295.88,1024.99 1306.24,1027 1316.6,1028.98 1326.96,1030.93 1337.33,1032.86 1347.69,1034.76 
  1358.05,1036.63 1368.41,1038.48 1378.77,1040.31 1389.13,1042.11 1399.5,1043.88 1409.86,1045.64 1420.22,1047.37 1430.58,1049.07 1440.94,1050.76 1451.3,1052.42 
  1461.66,1054.07 1472.03,1055.69 1482.39,1057.3 1492.75,1058.88 1503.11,1060.44 1513.47,1061.99 1523.83,1063.52 1534.2,1065.03 1544.56,1066.52 1554.92,1067.99 
  1565.28,1069.45 1575.64,1070.89 1586,1072.31 1596.36,1073.72 1606.73,1075.11 1617.09,1076.49 1627.45,1077.85 1637.81,1079.2 1648.17,1080.53 1658.53,1081.85 
  1668.9,1083.15 1679.26,1084.44 1689.62,1085.72 1699.98,1086.98 1710.34,1088.23 1720.7,1089.47 1731.06,1090.69 1741.43,1091.9 1751.79,1093.1 1762.15,1094.29 
  1772.51,1095.46 1782.87,1096.62 1793.23,1097.77 1803.59,1098.92 1813.96,1100.04 1824.32,1101.16 1834.68,1102.27 1845.04,1103.37 1855.4,1104.45 1865.76,1105.53 
  1876.13,1106.6 1886.49,1107.65 1896.85,1108.7 1907.21,1109.73 1917.57,1110.76 1927.93,1111.78 1938.29,1112.79 1948.66,1113.79 1959.02,1114.78 1969.38,1115.76 
  1979.74,1116.73 1990.1,1117.7 2000.46,1118.65 2010.83,1119.6 2021.19,1120.54 2031.55,1121.47 2041.91,1122.4 2052.27,1123.31 2062.63,1124.22 2072.99,1125.12 
  2083.36,1126.01 2093.72,1126.9 2104.08,1127.78 2114.44,1128.65 2124.8,1129.51 2135.16,1130.37 2145.53,1131.22 2155.89,1132.06 2166.25,1132.9 2176.61,1133.73 
  2186.97,1134.55 2197.33,1135.37 2207.69,1136.18 2218.06,1136.98 2228.42,1137.78 2238.78,1138.57 2249.14,1139.36 2259.5,1140.14 2269.86,1140.91 2280.23,1141.68 
  2290.59,1142.44 
  "/>
<polyline clip-path="url(#clip412)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none" stroke-dasharray="48, 30" points="
  321.897,87.9763 332.258,151.181 342.62,206.311 352.981,254.951 363.343,298.281 373.704,337.203 384.066,372.416 394.427,404.475 404.789,433.823 415.15,460.823 
  425.512,485.772 435.873,508.916 446.235,530.465 456.596,550.592 466.958,569.447 477.319,587.16 487.681,603.841 498.043,619.586 508.404,634.48 518.766,648.597 
  529.127,662.002 539.489,674.753 549.85,686.901 560.212,698.493 570.573,709.57 580.935,720.169 591.296,730.323 601.658,740.062 612.019,749.414 622.381,758.405 
  632.742,767.056 643.104,775.388 653.465,783.421 663.827,791.173 674.188,798.658 684.55,805.893 694.912,812.89 705.273,819.663 715.635,826.223 725.996,832.581 
  736.358,838.748 746.719,844.732 757.081,850.543 767.442,856.189 777.804,861.677 788.165,867.014 798.527,872.208 808.888,877.265 819.25,882.19 829.611,886.99 
  839.973,891.669 850.334,896.233 860.696,900.685 871.058,905.032 881.419,909.276 891.781,913.421 902.142,917.473 912.504,921.433 922.865,925.305 933.227,929.093 
  943.588,932.799 953.95,936.427 964.311,939.979 974.673,943.458 985.034,946.866 995.396,950.206 1005.76,953.479 1016.12,956.689 1026.48,959.836 1036.84,962.924 
  1047.2,965.953 1057.56,968.927 1067.93,971.845 1078.29,974.711 1088.65,977.526 1099.01,980.29 1109.37,983.007 1119.73,985.676 1130.1,988.3 1140.46,990.879 
  1150.82,993.416 1161.18,995.91 1171.54,998.363 1181.9,1000.78 1192.26,1003.15 1202.63,1005.49 1212.99,1007.79 1223.35,1010.06 1233.71,1012.29 1244.07,1014.48 
  1254.43,1016.65 1264.8,1018.78 1275.16,1020.88 1285.52,1022.95 1295.88,1024.99 1306.24,1027 1316.6,1028.98 1326.96,1030.93 1337.33,1032.86 1347.69,1034.76 
  1358.05,1036.63 1368.41,1038.48 1378.77,1040.31 1389.13,1042.11 1399.5,1043.88 1409.86,1045.64 1420.22,1047.37 1430.58,1049.07 1440.94,1050.76 1451.3,1052.42 
  1461.66,1054.07 1472.03,1055.69 1482.39,1057.3 1492.75,1058.88 1503.11,1060.44 1513.47,1061.99 1523.83,1063.52 1534.2,1065.03 1544.56,1066.52 1554.92,1067.99 
  1565.28,1069.45 1575.64,1070.89 1586,1072.31 1596.36,1073.72 1606.73,1075.11 1617.09,1076.49 1627.45,1077.85 1637.81,1079.2 1648.17,1080.53 1658.53,1081.85 
  1668.9,1083.15 1679.26,1084.44 1689.62,1085.72 1699.98,1086.98 1710.34,1088.23 1720.7,1089.47 1731.06,1090.69 1741.43,1091.9 1751.79,1093.1 1762.15,1094.29 
  1772.51,1095.46 1782.87,1096.62 1793.23,1097.77 1803.59,1098.92 1813.96,1100.04 1824.32,1101.16 1834.68,1102.27 1845.04,1103.37 1855.4,1104.45 1865.76,1105.53 
  1876.13,1106.6 1886.49,1107.65 1896.85,1108.7 1907.21,1109.73 1917.57,1110.76 1927.93,1111.78 1938.29,1112.79 1948.66,1113.79 1959.02,1114.78 1969.38,1115.76 
  1979.74,1116.73 1990.1,1117.7 2000.46,1118.65 2010.83,1119.6 2021.19,1120.54 2031.55,1121.47 2041.91,1122.4 2052.27,1123.31 2062.63,1124.22 2072.99,1125.12 
  2083.36,1126.01 2093.72,1126.9 2104.08,1127.78 2114.44,1128.65 2124.8,1129.51 2135.16,1130.37 2145.53,1131.22 2155.89,1132.06 2166.25,1132.9 2176.61,1133.73 
  2186.97,1134.55 2197.33,1135.37 2207.69,1136.18 2218.06,1136.98 2228.42,1137.78 2238.78,1138.57 2249.14,1139.36 2259.5,1140.14 2269.86,1140.91 2280.23,1141.68 
  2290.59,1142.44 
  "/>
<path clip-path="url(#clip410)" d="
M229.334 1438.47 L943.324 1438.47 L943.324 1231.11 L229.334 1231.11  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip410)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  229.334,1438.47 943.324,1438.47 943.324,1231.11 229.334,1231.11 229.334,1438.47 
  "/>
<polyline clip-path="url(#clip410)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none" points="
  253.741,1282.95 400.184,1282.95 
  "/>
<path clip-path="url(#clip410)" d="M441.095 1264.22 L441.095 1267.76 L437.021 1267.76 Q434.73 1267.76 433.827 1268.68 Q432.947 1269.61 432.947 1272.02 L432.947 1274.31 L439.961 1274.31 L439.961 1277.62 L432.947 1277.62 L432.947 1300.23 L428.665 1300.23 L428.665 1277.62 L424.591 1277.62 L424.591 1274.31 L428.665 1274.31 L428.665 1272.5 Q428.665 1268.17 430.679 1266.21 Q432.693 1264.22 437.068 1264.22 L441.095 1264.22 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M454.892 1264.26 Q451.79 1269.59 450.285 1274.79 Q448.78 1280 448.78 1285.35 Q448.78 1290.7 450.285 1295.95 Q451.813 1301.18 454.892 1306.48 L451.188 1306.48 Q447.716 1301.04 445.98 1295.79 Q444.267 1290.54 444.267 1285.35 Q444.267 1280.19 445.98 1274.96 Q447.693 1269.73 451.188 1264.26 L454.892 1264.26 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M484.706 1274.31 L475.331 1286.92 L485.192 1300.23 L480.169 1300.23 L472.623 1290.05 L465.077 1300.23 L460.054 1300.23 L470.123 1286.67 L460.91 1274.31 L465.933 1274.31 L472.808 1283.54 L479.683 1274.31 L484.706 1274.31 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M490.54 1264.26 L494.243 1264.26 Q497.715 1269.73 499.428 1274.96 Q501.164 1280.19 501.164 1285.35 Q501.164 1290.54 499.428 1295.79 Q497.715 1301.04 494.243 1306.48 L490.54 1306.48 Q493.618 1301.18 495.123 1295.95 Q496.651 1290.7 496.651 1285.35 Q496.651 1280 495.123 1274.79 Q493.618 1269.59 490.54 1264.26 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M525.331 1278.71 L555.007 1278.71 L555.007 1282.6 L525.331 1282.6 L525.331 1278.71 M525.331 1288.15 L555.007 1288.15 L555.007 1292.09 L525.331 1292.09 L525.331 1288.15 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M598.178 1287.29 Q598.178 1282.6 596.233 1279.93 Q594.312 1277.25 590.933 1277.25 Q587.553 1277.25 585.608 1279.93 Q583.687 1282.6 583.687 1287.29 Q583.687 1291.99 585.608 1294.68 Q587.553 1297.34 590.933 1297.34 Q594.312 1297.34 596.233 1294.68 Q598.178 1291.99 598.178 1287.29 M583.687 1278.24 Q585.03 1275.93 587.067 1274.82 Q589.127 1273.68 591.974 1273.68 Q596.696 1273.68 599.636 1277.43 Q602.599 1281.18 602.599 1287.29 Q602.599 1293.41 599.636 1297.16 Q596.696 1300.91 591.974 1300.91 Q589.127 1300.91 587.067 1299.79 Q585.03 1298.66 583.687 1296.35 L583.687 1300.23 L579.405 1300.23 L579.405 1264.22 L583.687 1264.22 L583.687 1278.24 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M621.442 1287.2 Q616.28 1287.2 614.289 1288.38 Q612.298 1289.56 612.298 1292.41 Q612.298 1294.68 613.78 1296.02 Q615.284 1297.34 617.854 1297.34 Q621.395 1297.34 623.525 1294.84 Q625.678 1292.32 625.678 1288.15 L625.678 1287.2 L621.442 1287.2 M629.937 1285.44 L629.937 1300.23 L625.678 1300.23 L625.678 1296.3 Q624.219 1298.66 622.043 1299.79 Q619.868 1300.91 616.719 1300.91 Q612.738 1300.91 610.377 1298.68 Q608.039 1296.44 608.039 1292.69 Q608.039 1288.31 610.956 1286.09 Q613.895 1283.87 619.706 1283.87 L625.678 1283.87 L625.678 1283.45 Q625.678 1280.51 623.733 1278.92 Q621.812 1277.29 618.317 1277.29 Q616.094 1277.29 613.988 1277.83 Q611.881 1278.36 609.937 1279.42 L609.937 1275.49 Q612.275 1274.59 614.474 1274.15 Q616.673 1273.68 618.756 1273.68 Q624.381 1273.68 627.159 1276.6 Q629.937 1279.52 629.937 1285.44 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M657.321 1287.29 Q657.321 1282.6 655.377 1279.93 Q653.455 1277.25 650.076 1277.25 Q646.696 1277.25 644.752 1279.93 Q642.83 1282.6 642.83 1287.29 Q642.83 1291.99 644.752 1294.68 Q646.696 1297.34 650.076 1297.34 Q653.455 1297.34 655.377 1294.68 Q657.321 1291.99 657.321 1287.29 M642.83 1278.24 Q644.173 1275.93 646.21 1274.82 Q648.27 1273.68 651.117 1273.68 Q655.84 1273.68 658.779 1277.43 Q661.742 1281.18 661.742 1287.29 Q661.742 1293.41 658.779 1297.16 Q655.84 1300.91 651.117 1300.91 Q648.27 1300.91 646.21 1299.79 Q644.173 1298.66 642.83 1296.35 L642.83 1300.23 L638.548 1300.23 L638.548 1264.22 L642.83 1264.22 L642.83 1278.24 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M679.589 1302.64 Q677.784 1307.27 676.071 1308.68 Q674.358 1310.1 671.488 1310.1 L668.085 1310.1 L668.085 1306.53 L670.585 1306.53 Q672.344 1306.53 673.316 1305.7 Q674.289 1304.86 675.469 1301.76 L676.233 1299.82 L665.747 1274.31 L670.261 1274.31 L678.363 1294.59 L686.464 1274.31 L690.978 1274.31 L679.589 1302.64 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M713.386 1275.07 L713.386 1279.1 Q711.58 1278.17 709.636 1277.71 Q707.691 1277.25 705.608 1277.25 Q702.437 1277.25 700.839 1278.22 Q699.265 1279.19 699.265 1281.14 Q699.265 1282.62 700.4 1283.48 Q701.534 1284.31 704.96 1285.07 L706.418 1285.4 Q710.955 1286.37 712.853 1288.15 Q714.774 1289.91 714.774 1293.08 Q714.774 1296.69 711.904 1298.8 Q709.057 1300.91 704.057 1300.91 Q701.974 1300.91 699.705 1300.49 Q697.46 1300.1 694.96 1299.29 L694.96 1294.89 Q697.321 1296.11 699.612 1296.74 Q701.904 1297.34 704.149 1297.34 Q707.159 1297.34 708.779 1296.32 Q710.399 1295.28 710.399 1293.41 Q710.399 1291.67 709.219 1290.74 Q708.062 1289.82 704.103 1288.96 L702.622 1288.61 Q698.663 1287.78 696.904 1286.07 Q695.145 1284.33 695.145 1281.32 Q695.145 1277.67 697.737 1275.67 Q700.33 1273.68 705.099 1273.68 Q707.46 1273.68 709.543 1274.03 Q711.626 1274.38 713.386 1275.07 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M724.103 1287.29 Q724.103 1291.99 726.024 1294.68 Q727.969 1297.34 731.348 1297.34 Q734.728 1297.34 736.672 1294.68 Q738.617 1291.99 738.617 1287.29 Q738.617 1282.6 736.672 1279.93 Q734.728 1277.25 731.348 1277.25 Q727.969 1277.25 726.024 1279.93 Q724.103 1282.6 724.103 1287.29 M738.617 1296.35 Q737.274 1298.66 735.214 1299.79 Q733.177 1300.91 730.307 1300.91 Q725.608 1300.91 722.645 1297.16 Q719.705 1293.41 719.705 1287.29 Q719.705 1281.18 722.645 1277.43 Q725.608 1273.68 730.307 1273.68 Q733.177 1273.68 735.214 1274.82 Q737.274 1275.93 738.617 1278.24 L738.617 1274.31 L742.876 1274.31 L742.876 1310.1 L738.617 1310.1 L738.617 1296.35 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M766.672 1278.29 Q765.955 1277.87 765.098 1277.69 Q764.265 1277.48 763.246 1277.48 Q759.635 1277.48 757.691 1279.84 Q755.77 1282.18 755.77 1286.58 L755.77 1300.23 L751.487 1300.23 L751.487 1274.31 L755.77 1274.31 L755.77 1278.34 Q757.112 1275.98 759.265 1274.84 Q761.418 1273.68 764.496 1273.68 Q764.936 1273.68 765.469 1273.75 Q766.001 1273.8 766.649 1273.92 L766.672 1278.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M775.353 1266.95 L775.353 1274.31 L784.126 1274.31 L784.126 1277.62 L775.353 1277.62 L775.353 1291.69 Q775.353 1294.86 776.209 1295.77 Q777.089 1296.67 779.751 1296.67 L784.126 1296.67 L784.126 1300.23 L779.751 1300.23 Q774.82 1300.23 772.945 1298.41 Q771.07 1296.55 771.07 1291.69 L771.07 1277.62 L767.945 1277.62 L767.945 1274.31 L771.07 1274.31 L771.07 1266.95 L775.353 1266.95 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M799.959 1264.26 Q796.857 1269.59 795.353 1274.79 Q793.848 1280 793.848 1285.35 Q793.848 1290.7 795.353 1295.95 Q796.88 1301.18 799.959 1306.48 L796.255 1306.48 Q792.783 1301.04 791.047 1295.79 Q789.334 1290.54 789.334 1285.35 Q789.334 1280.19 791.047 1274.96 Q792.76 1269.73 796.255 1264.26 L799.959 1264.26 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M829.774 1274.31 L820.399 1286.92 L830.26 1300.23 L825.237 1300.23 L817.691 1290.05 L810.144 1300.23 L805.121 1300.23 L815.191 1286.67 L805.978 1274.31 L811.001 1274.31 L817.876 1283.54 L824.751 1274.31 L829.774 1274.31 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M835.607 1264.26 L839.311 1264.26 Q842.783 1269.73 844.496 1274.96 Q846.232 1280.19 846.232 1285.35 Q846.232 1290.54 844.496 1295.79 Q842.783 1301.04 839.311 1306.48 L835.607 1306.48 Q838.686 1301.18 840.19 1295.95 Q841.718 1290.7 841.718 1285.35 Q841.718 1280 840.19 1274.79 Q838.686 1269.59 835.607 1264.26 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip410)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none" stroke-dasharray="6, 12" points="
  253.741,1334.79 400.184,1334.79 
  "/>
<path clip-path="url(#clip410)" d="M440.424 1322.12 L434.082 1339.32 L446.79 1339.32 L440.424 1322.12 M437.785 1317.51 L443.086 1317.51 L456.257 1352.07 L451.396 1352.07 L448.248 1343.21 L432.669 1343.21 L429.521 1352.07 L424.591 1352.07 L437.785 1317.51 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M482.669 1336.43 L482.669 1352.07 L478.41 1352.07 L478.41 1336.57 Q478.41 1332.88 476.975 1331.06 Q475.54 1329.23 472.669 1329.23 Q469.22 1329.23 467.229 1331.43 Q465.239 1333.63 465.239 1337.42 L465.239 1352.07 L460.956 1352.07 L460.956 1326.15 L465.239 1326.15 L465.239 1330.18 Q466.767 1327.84 468.827 1326.68 Q470.91 1325.52 473.618 1325.52 Q478.086 1325.52 480.378 1328.3 Q482.669 1331.06 482.669 1336.43 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M502.947 1339.04 Q497.785 1339.04 495.794 1340.22 Q493.803 1341.4 493.803 1344.25 Q493.803 1346.52 495.285 1347.86 Q496.789 1349.18 499.359 1349.18 Q502.901 1349.18 505.03 1346.68 Q507.183 1344.16 507.183 1339.99 L507.183 1339.04 L502.947 1339.04 M511.442 1337.28 L511.442 1352.07 L507.183 1352.07 L507.183 1348.14 Q505.725 1350.5 503.549 1351.63 Q501.373 1352.75 498.225 1352.75 Q494.243 1352.75 491.882 1350.52 Q489.544 1348.28 489.544 1344.53 Q489.544 1340.15 492.461 1337.93 Q495.401 1335.71 501.211 1335.71 L507.183 1335.71 L507.183 1335.29 Q507.183 1332.35 505.239 1330.76 Q503.317 1329.13 499.822 1329.13 Q497.6 1329.13 495.493 1329.67 Q493.387 1330.2 491.442 1331.26 L491.442 1327.33 Q493.78 1326.43 495.979 1325.99 Q498.178 1325.52 500.262 1325.52 Q505.887 1325.52 508.664 1328.44 Q511.442 1331.36 511.442 1337.28 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M520.215 1316.06 L524.475 1316.06 L524.475 1352.07 L520.215 1352.07 L520.215 1316.06 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M544.174 1354.48 Q542.368 1359.11 540.655 1360.52 Q538.942 1361.94 536.072 1361.94 L532.669 1361.94 L532.669 1358.37 L535.169 1358.37 Q536.928 1358.37 537.9 1357.54 Q538.873 1356.7 540.053 1353.6 L540.817 1351.66 L530.331 1326.15 L534.845 1326.15 L542.947 1346.43 L551.048 1326.15 L555.562 1326.15 L544.174 1354.48 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M565.655 1318.79 L565.655 1326.15 L574.428 1326.15 L574.428 1329.46 L565.655 1329.46 L565.655 1343.53 Q565.655 1346.7 566.511 1347.61 Q567.391 1348.51 570.053 1348.51 L574.428 1348.51 L574.428 1352.07 L570.053 1352.07 Q565.122 1352.07 563.247 1350.25 Q561.372 1348.39 561.372 1343.53 L561.372 1329.46 L558.248 1329.46 L558.248 1326.15 L561.372 1326.15 L561.372 1318.79 L565.655 1318.79 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M580.03 1326.15 L584.289 1326.15 L584.289 1352.07 L580.03 1352.07 L580.03 1326.15 M580.03 1316.06 L584.289 1316.06 L584.289 1321.45 L580.03 1321.45 L580.03 1316.06 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M611.858 1327.14 L611.858 1331.13 Q610.053 1330.13 608.224 1329.64 Q606.419 1329.13 604.567 1329.13 Q600.423 1329.13 598.132 1331.77 Q595.84 1334.39 595.84 1339.13 Q595.84 1343.88 598.132 1346.52 Q600.423 1349.13 604.567 1349.13 Q606.419 1349.13 608.224 1348.65 Q610.053 1348.14 611.858 1347.14 L611.858 1351.08 Q610.076 1351.91 608.155 1352.33 Q606.257 1352.75 604.104 1352.75 Q598.247 1352.75 594.798 1349.07 Q591.349 1345.38 591.349 1339.13 Q591.349 1332.79 594.821 1329.16 Q598.317 1325.52 604.382 1325.52 Q606.349 1325.52 608.224 1325.94 Q610.099 1326.33 611.858 1327.14 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M647.46 1316.06 L647.46 1319.6 L643.386 1319.6 Q641.094 1319.6 640.192 1320.52 Q639.312 1321.45 639.312 1323.86 L639.312 1326.15 L646.326 1326.15 L646.326 1329.46 L639.312 1329.46 L639.312 1352.07 L635.029 1352.07 L635.029 1329.46 L630.955 1329.46 L630.955 1326.15 L635.029 1326.15 L635.029 1324.34 Q635.029 1320.01 637.043 1318.05 Q639.057 1316.06 643.432 1316.06 L647.46 1316.06 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M655.053 1317.51 L655.053 1330.36 L651.117 1330.36 L651.117 1317.51 L655.053 1317.51 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip410)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none" stroke-dasharray="48, 30" points="
  253.741,1386.63 400.184,1386.63 
  "/>
<path clip-path="url(#clip410)" d="M429.267 1373.2 L429.267 1400.07 L434.915 1400.07 Q442.068 1400.07 445.378 1396.83 Q448.711 1393.59 448.711 1386.6 Q448.711 1379.66 445.378 1376.44 Q442.068 1373.2 434.915 1373.2 L429.267 1373.2 M424.591 1369.35 L434.197 1369.35 Q444.243 1369.35 448.943 1373.54 Q453.642 1377.71 453.642 1386.6 Q453.642 1395.53 448.919 1399.72 Q444.197 1403.91 434.197 1403.91 L424.591 1403.91 L424.591 1369.35 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M460.47 1393.68 L460.47 1377.99 L464.729 1377.99 L464.729 1393.52 Q464.729 1397.2 466.165 1399.05 Q467.6 1400.88 470.47 1400.88 Q473.919 1400.88 475.91 1398.68 Q477.924 1396.48 477.924 1392.69 L477.924 1377.99 L482.183 1377.99 L482.183 1403.91 L477.924 1403.91 L477.924 1399.93 Q476.373 1402.29 474.313 1403.45 Q472.276 1404.59 469.567 1404.59 Q465.1 1404.59 462.785 1401.81 Q460.47 1399.03 460.47 1393.68 M471.188 1377.36 L471.188 1377.36 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M502.739 1390.88 Q497.577 1390.88 495.586 1392.06 Q493.595 1393.24 493.595 1396.09 Q493.595 1398.36 495.077 1399.7 Q496.581 1401.02 499.151 1401.02 Q502.692 1401.02 504.822 1398.52 Q506.975 1396 506.975 1391.83 L506.975 1390.88 L502.739 1390.88 M511.234 1389.12 L511.234 1403.91 L506.975 1403.91 L506.975 1399.98 Q505.516 1402.34 503.34 1403.47 Q501.164 1404.59 498.016 1404.59 Q494.035 1404.59 491.674 1402.36 Q489.336 1400.12 489.336 1396.37 Q489.336 1391.99 492.252 1389.77 Q495.192 1387.55 501.002 1387.55 L506.975 1387.55 L506.975 1387.13 Q506.975 1384.19 505.03 1382.6 Q503.109 1380.97 499.614 1380.97 Q497.391 1380.97 495.285 1381.51 Q493.178 1382.04 491.234 1383.1 L491.234 1379.17 Q493.572 1378.27 495.771 1377.83 Q497.97 1377.36 500.053 1377.36 Q505.678 1377.36 508.456 1380.28 Q511.234 1383.2 511.234 1389.12 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M520.007 1367.9 L524.266 1367.9 L524.266 1403.91 L520.007 1403.91 L520.007 1367.9 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M548.433 1369.35 L568.294 1369.35 L568.294 1373.29 L553.109 1373.29 L553.109 1383.47 L566.812 1383.47 L566.812 1387.41 L553.109 1387.41 L553.109 1403.91 L548.433 1403.91 L548.433 1369.35 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M583.872 1380.97 Q580.446 1380.97 578.456 1383.66 Q576.465 1386.32 576.465 1390.97 Q576.465 1395.63 578.433 1398.31 Q580.423 1400.97 583.872 1400.97 Q587.275 1400.97 589.266 1398.29 Q591.257 1395.6 591.257 1390.97 Q591.257 1386.37 589.266 1383.68 Q587.275 1380.97 583.872 1380.97 M583.872 1377.36 Q589.428 1377.36 592.599 1380.97 Q595.77 1384.59 595.77 1390.97 Q595.77 1397.34 592.599 1400.97 Q589.428 1404.59 583.872 1404.59 Q578.294 1404.59 575.122 1400.97 Q571.974 1397.34 571.974 1390.97 Q571.974 1384.59 575.122 1380.97 Q578.294 1377.36 583.872 1377.36 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M617.854 1381.97 Q617.136 1381.55 616.28 1381.37 Q615.446 1381.16 614.428 1381.16 Q610.817 1381.16 608.872 1383.52 Q606.951 1385.86 606.951 1390.26 L606.951 1403.91 L602.669 1403.91 L602.669 1377.99 L606.951 1377.99 L606.951 1382.02 Q608.294 1379.66 610.446 1378.52 Q612.599 1377.36 615.678 1377.36 Q616.118 1377.36 616.65 1377.43 Q617.182 1377.48 617.831 1377.6 L617.854 1381.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M619.844 1377.99 L624.104 1377.99 L629.428 1398.22 L634.729 1377.99 L639.752 1377.99 L645.076 1398.22 L650.377 1377.99 L654.636 1377.99 L647.854 1403.91 L642.83 1403.91 L637.252 1382.66 L631.65 1403.91 L626.627 1403.91 L619.844 1377.99 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M672.877 1390.88 Q667.715 1390.88 665.724 1392.06 Q663.733 1393.24 663.733 1396.09 Q663.733 1398.36 665.215 1399.7 Q666.719 1401.02 669.289 1401.02 Q672.83 1401.02 674.96 1398.52 Q677.113 1396 677.113 1391.83 L677.113 1390.88 L672.877 1390.88 M681.372 1389.12 L681.372 1403.91 L677.113 1403.91 L677.113 1399.98 Q675.654 1402.34 673.478 1403.47 Q671.302 1404.59 668.154 1404.59 Q664.173 1404.59 661.812 1402.36 Q659.474 1400.12 659.474 1396.37 Q659.474 1391.99 662.39 1389.77 Q665.33 1387.55 671.14 1387.55 L677.113 1387.55 L677.113 1387.13 Q677.113 1384.19 675.168 1382.6 Q673.247 1380.97 669.752 1380.97 Q667.529 1380.97 665.423 1381.51 Q663.316 1382.04 661.372 1383.1 L661.372 1379.17 Q663.71 1378.27 665.909 1377.83 Q668.108 1377.36 670.191 1377.36 Q675.816 1377.36 678.594 1380.28 Q681.372 1383.2 681.372 1389.12 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M705.168 1381.97 Q704.45 1381.55 703.594 1381.37 Q702.761 1381.16 701.742 1381.16 Q698.131 1381.16 696.187 1383.52 Q694.265 1385.86 694.265 1390.26 L694.265 1403.91 L689.983 1403.91 L689.983 1377.99 L694.265 1377.99 L694.265 1382.02 Q695.608 1379.66 697.761 1378.52 Q699.913 1377.36 702.992 1377.36 Q703.432 1377.36 703.964 1377.43 Q704.497 1377.48 705.145 1377.6 L705.168 1381.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M725.862 1381.92 L725.862 1367.9 L730.122 1367.9 L730.122 1403.91 L725.862 1403.91 L725.862 1400.03 Q724.52 1402.34 722.46 1403.47 Q720.423 1404.59 717.552 1404.59 Q712.853 1404.59 709.89 1400.84 Q706.95 1397.09 706.95 1390.97 Q706.95 1384.86 709.89 1381.11 Q712.853 1377.36 717.552 1377.36 Q720.423 1377.36 722.46 1378.5 Q724.52 1379.61 725.862 1381.92 M711.349 1390.97 Q711.349 1395.67 713.27 1398.36 Q715.214 1401.02 718.594 1401.02 Q721.973 1401.02 723.918 1398.36 Q725.862 1395.67 725.862 1390.97 Q725.862 1386.28 723.918 1383.61 Q721.973 1380.93 718.594 1380.93 Q715.214 1380.93 713.27 1383.61 Q711.349 1386.28 711.349 1390.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M754.149 1369.35 L761.117 1369.35 L769.936 1392.87 L778.802 1369.35 L785.769 1369.35 L785.769 1403.91 L781.209 1403.91 L781.209 1373.57 L772.297 1397.27 L767.598 1397.27 L758.686 1373.57 L758.686 1403.91 L754.149 1403.91 L754.149 1369.35 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M804.913 1380.97 Q801.487 1380.97 799.496 1383.66 Q797.505 1386.32 797.505 1390.97 Q797.505 1395.63 799.473 1398.31 Q801.464 1400.97 804.913 1400.97 Q808.316 1400.97 810.306 1398.29 Q812.297 1395.6 812.297 1390.97 Q812.297 1386.37 810.306 1383.68 Q808.316 1380.97 804.913 1380.97 M804.913 1377.36 Q810.468 1377.36 813.64 1380.97 Q816.811 1384.59 816.811 1390.97 Q816.811 1397.34 813.64 1400.97 Q810.468 1404.59 804.913 1404.59 Q799.334 1404.59 796.163 1400.97 Q793.015 1397.34 793.015 1390.97 Q793.015 1384.59 796.163 1380.97 Q799.334 1377.36 804.913 1377.36 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M840.931 1381.92 L840.931 1367.9 L845.19 1367.9 L845.19 1403.91 L840.931 1403.91 L840.931 1400.03 Q839.589 1402.34 837.528 1403.47 Q835.491 1404.59 832.621 1404.59 Q827.922 1404.59 824.959 1400.84 Q822.019 1397.09 822.019 1390.97 Q822.019 1384.86 824.959 1381.11 Q827.922 1377.36 832.621 1377.36 Q835.491 1377.36 837.528 1378.5 Q839.589 1379.61 840.931 1381.92 M826.417 1390.97 Q826.417 1395.67 828.339 1398.36 Q830.283 1401.02 833.663 1401.02 Q837.042 1401.02 838.987 1398.36 Q840.931 1395.67 840.931 1390.97 Q840.931 1386.28 838.987 1383.61 Q837.042 1380.93 833.663 1380.93 Q830.283 1380.93 828.339 1383.61 Q826.417 1386.28 826.417 1390.97 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M876.139 1389.89 L876.139 1391.97 L856.556 1391.97 Q856.834 1396.37 859.195 1398.68 Q861.579 1400.97 865.815 1400.97 Q868.269 1400.97 870.561 1400.37 Q872.875 1399.77 875.144 1398.57 L875.144 1402.59 Q872.852 1403.57 870.445 1404.08 Q868.038 1404.59 865.561 1404.59 Q859.357 1404.59 855.723 1400.97 Q852.112 1397.36 852.112 1391.21 Q852.112 1384.84 855.538 1381.11 Q858.987 1377.36 864.82 1377.36 Q870.051 1377.36 873.084 1380.74 Q876.139 1384.1 876.139 1389.89 M871.88 1388.64 Q871.834 1385.14 869.912 1383.06 Q868.014 1380.97 864.866 1380.97 Q861.301 1380.97 859.149 1382.99 Q857.019 1385 856.695 1388.66 L871.88 1388.64 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M911.324 1367.9 L911.324 1371.44 L907.25 1371.44 Q904.959 1371.44 904.056 1372.36 Q903.176 1373.29 903.176 1375.7 L903.176 1377.99 L910.19 1377.99 L910.19 1381.3 L903.176 1381.3 L903.176 1403.91 L898.894 1403.91 L898.894 1381.3 L894.82 1381.3 L894.82 1377.99 L898.894 1377.99 L898.894 1376.18 Q898.894 1371.85 900.908 1369.89 Q902.922 1367.9 907.297 1367.9 L911.324 1367.9 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip410)" d="M918.917 1369.35 L918.917 1382.2 L914.982 1382.2 L914.982 1369.35 L918.917 1369.35 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /></svg>
<hr/><h3 id="Takeaways"><a class="docs-heading-anchor" href="#Takeaways">Takeaways</a><a id="Takeaways-1"></a><a class="docs-heading-anchor-permalink" href="#Takeaways" title="Permalink"></a></h3><ol><li>Forward mode <span>$f&#39;$</span> is obtained simply by pushing a <code>Dual</code> through <code>babysqrt</code></li><li>To make the forward diff work in Julia, we only need to <strong><em>overload</em></strong> a few <strong><em>operators</em></strong> for forward mode AD to work on <strong><em>any function.</em></strong> Therefore the name of the approach is called operator overloading.</li><li>For vector valued function we can use <a href="http://adl.stanford.edu/hyperdual/"><strong><em>Hyperduals</em></strong></a></li><li>Forward diff can differentiation through the <code>setindex!</code> (called each time an element is assigned to a place in array, e.g. <code>x = [1,2,3]; x[2] = 1</code>)</li><li>ForwardDiff is implemented in <a href="https://github.com/JuliaDiff/ForwardDiff.jl"><code>ForwardDiff.jl</code></a>, which might appear to be neglected, but the truth is that it is very stable and general implementation.</li><li>ForwardDiff does not have to be implemented through Dual numbers. It can be implemented similarly to ReverseDiff through multiplication of Jacobians, which is what is the community work on now (in <a href="https://github.com/JuliaDiff/Diffractor.jl"><code>Diffractor</code></a>, <a href="https://github.com/FluxML/Zygote.jl"><code>Zygote</code></a> with rules defined in <a href="https://github.com/JuliaDiff/ChainRules.jl"><code>ChainRules</code></a>).</li></ol><hr/><h2 id="Reverse-mode"><a class="docs-heading-anchor" href="#Reverse-mode">Reverse mode</a><a id="Reverse-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-mode" title="Permalink"></a></h2><p>In reverse mode, the computation of the gradient follow the opposite order.  We initialize the computation by setting <span>$\mathbf{J}_0 = \frac{\partial y}{\partial y_0},$</span> which is again an identity matrix. Then we compute Jacobians and multiplications in the opposite order. The problem is that to calculate <span>$J_i$</span> we need to know the value of <span>$y_i^0$</span>, which cannot be calculated in the reverse pass. The backward pass therefore needs to be preceded by the forward pass, where  <span>$\{y_i^0\}_{i=1}^n$</span> are calculated.</p><p>The complete reverse mode algorithm therefore proceeds as </p><ol><li>Forward pass: iterate <code>i</code> from <code>n</code> down to <code>1</code> as<ul><li>calculate the next intermediate output as <span>$y^0_{i-1} = f_i(y^0_i)$</span> </li></ul></li><li>Backward pass: iterate <code>i</code> from <code>1</code> down to <code>n</code> as<ul><li>calculate Jacobian <span>$J_i = \left.\frac{f_i}{\partial y_i}\right|_{y_i^0}$</span> at point <span>$y_i^0$</span></li><li><em>pull back</em> the gradient as <span>$\left.\frac{\partial f(x)}{\partial y_{i}}\right|_{y^0_i} = \left.\frac{\partial y_0}{\partial y_{i-1}}\right|_{y^0_{i-1}} \times J_i$</span></li></ul></li></ol><p>The need to store intermediate outs has a huge impact on memory requirements, which particularly on GPU is a big deal. Recall few lectures ago we have been discussing how excessive memory allocations can be damaging for performance, here we are given an algorithm where the excessive allocation is by design.</p><h3 id="Tricks-to-decrease-memory-consumptions"><a class="docs-heading-anchor" href="#Tricks-to-decrease-memory-consumptions">Tricks to decrease memory consumptions</a><a id="Tricks-to-decrease-memory-consumptions-1"></a><a class="docs-heading-anchor-permalink" href="#Tricks-to-decrease-memory-consumptions" title="Permalink"></a></h3><ul><li>Define <strong>custom rules</strong> over large functional blocks. For example while we can auto-grad (in theory) matrix product, it is much more efficient to define make a matrix multiplication as one large function, for which we define Jacobians (note that by doing so, we can dispatch on Blas). e.g</li></ul><p class="math-container">\[\begin{alignat*}{2}
  \mathbf{C} &amp;= \mathbf{A} * \mathbf{B} \\
  \frac{\partial{\mathbf{C}}}{\partial \mathbf{A}} &amp;= \mathbf{B} \\
  \frac{\partial{\mathbf{C}}}{\partial \mathbf{B}} &amp;= \mathbf{A}^{\mathrm{T}} \\
\end{alignat*}\]</p><ul><li>When differentiating <strong>Invertible functions</strong>, calculate intermediate outputs from the output. This can lead to huge performance gain, as all data needed for computations are in caches.  </li><li><strong>Checkpointing</strong> does not store intermediate ouputs after larger sequence of operations. When they are needed for forward pass, they are recalculated on demand.</li></ul><p>Most reverse mode AD engines does not support mutating values of arrays (<code>setindex!</code> in julia). This is related to the memory consumption, where after every <code>setindex!</code> you need in theory save the full matrix. <a href="https://github.com/wsmoses/Enzyme.jl"><code>Enzyme</code></a> differentiating directly LLVM code supports this, since in LLVM every variable is assigned just once. ForwardDiff methods does not suffer this problem, as the gradient is computed at the time of the values.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Reverse mode AD was first published in 1976 by Seppo Linnainmaa<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>, a finnish computer scientist. It was popularized in the end of 80s when applied to training multi-layer perceptrons, which gave rise to the famous <strong>backpropagation</strong> algorithm<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>, which is a special case of reverse mode AD.</p></div></div><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>The terminology in automatic differentiation is everything but fixed. The community around <code>ChainRules.jl</code> went a great length to use something reasonable. They use <strong>pullback</strong> for a function realizing vector-Jacobian product in the reverse-diff reminding that the gradient is pulled back to the origin of the computation. The use <strong>pushforward</strong> to denote the same operation in the ForwardDiff, as the gradient is push forward through the computation.</p></div></div><h2 id="Implementation-details-of-reverse-AD"><a class="docs-heading-anchor" href="#Implementation-details-of-reverse-AD">Implementation details of reverse AD</a><a id="Implementation-details-of-reverse-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-details-of-reverse-AD" title="Permalink"></a></h2><p>Reverse-mode AD needs to record operations over variables when computing the value of a differentiated function, such that it can walk back when computing the gradient. This <em>record</em> is called <strong><em>tape</em></strong>, but it is effectively a directed acyclic graph. The construction of the tape can be either explicit or implicit. The code computing the gradient can be produced by operator-overloading or code-rewriting techniques. This give rise of four different takes on AD, and Julia has libraries for alll four.</p><ul><li><a href="https://github.com/dfdx/Yota.jl"><code>Yota.jl</code></a>: explict tape, code-rewriting</li><li><a href="https://github.com/FluxML/Tracker.jl"><code>Tracker.jl</code></a>, <a href="https://github.com/denizyuret/AutoGrad.jl"><code>AutoGrad.jl</code></a>: implict tape, operator overloading</li><li><a href="https://github.com/JuliaDiff/ReverseDiff.jl"><code>ReverseDiff.jl</code></a>: explict tape, operator overloading</li><li><a href="https://github.com/FluxML/Zygote.jl"><code>Zygote.jl</code></a>: implict tape, code-rewriting</li></ul><h3 id="Graph-based-AD"><a class="docs-heading-anchor" href="#Graph-based-AD">Graph-based AD</a><a id="Graph-based-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Graph-based-AD" title="Permalink"></a></h3><p>In Graph-based approach, we start with a complete knowledge of the computation graph (which is known in many cases like classical neural networks) and augment it with nodes representing the computation of the computation of the gradient (backward path). We need to be careful to add all edges representing the flow of information needed to calculate the gradient. Once the computation graph is augmented, we can find the subgraph needed to compute the desired node(s). </p><p>Recall the example from the beginning of the lecture <span>$f(x, y) = \sin(x) + xy$</span>, let&#39;s observe, how the extension of the computational graph will look like. The computation graph of function <span>$f$</span> looks like</p><p><img src="../graphdiff_6.svg" alt="diff graph"/></p><p>where arrows <span>$\rightarrow$</span> denote the flow of operations and we have denoted the output of function <span>$f$</span> as <span>$z$</span> and outputs of intermediate nodes as <span>$h_i$</span> standing for <em>hidden</em>.</p><p>We start from the top and add a node calculating <span>$\frac{\partial z}{\partial h_3}$</span> which is an identity, needed to jump-start the differentiation. </p><p><img src="../graphdiff_7.svg" alt="diff graph"/></p><p>We connect it with the output of <span>$h_3$</span>, even though technically in this case it is not needed, as the <span>$z = h_3$</span>. We then add a node calculating <span>$\frac{\partial h_3}{\partial h_2}$</span> for which we only need information about <span>$h_2$</span> and mark it in the graph (again, this edge can be theoretically dropped due to being equal to one regardless the inputs). Following the chain rule, we need to combine <span>$\frac{\partial h_3}{\partial h_2}$</span> with <span>$\frac{\partial z}{\partial h_3}$</span> to compute <span>$\frac{\partial z}{\partial h_2}$</span> which we note in the graph.</p><p><img src="../graphdiff_9.svg" alt="diff graph"/></p><p>We continue with the same process with <span>$\frac{\partial h_3}{\partial h_1}$</span>, which we again combine with <span>$\frac{\partial z}{\partial h_1}$</span> to obtain <span>$\frac{\partial z}{\partial h_1}$</span>. Continuing the reverse diff we obtain the final graph</p><p><img src="../graphdiff_14.svg" alt="diff graph"/> </p><p>containing the desired nodes <span>$\frac{\partial z}{\partial x}$</span> and <span>$\frac{\partial z}{\partial y}$</span>. This computational graph can be passed to the compiler to compute desired values.</p><p>This approach to AD has been taken for example by <a href="https://github.com/Theano/Theano">Theano</a> and by <a href="https://www.tensorflow.org/">TensorFlow</a>. In Tensorflow when you use functions like <code>tf.mul( a, b )</code> or <code>tf.add(a,b)</code>, you are not performing the computation in Python, but you are building the computational graph shown as above. You can then compute the values using <code>tf.run</code> with a desired inputs, but you are in fact computing the values in a different interpreter / compiler then in python.</p><p>Advantages:</p><ul><li>Knowing the computational graph in advance is great, as you can do expensive optimization steps to simplify the graph. </li><li>The computational graph have a simple semantics (limited support for loops, branches, no objects), and the compiler is therefore simpler than the compiler of full languages.</li><li>Since the computation of gradient augments the graph, you can run the process again to obtain higher order gradients. </li><li>TensorFlow allows you to specialize on sizes of Tensors, which means that it knows precisely how much memory you will need and where, which decreases the number of allocations. This is quite important in GPU.</li></ul><p>Disadvantages:</p><ul><li>You are restricted to fixed computation graph. It is generally difficult to implement <code>if</code> or <code>while</code>, and hence to change the computation according to values computed during the forward pass.</li><li>Development and debugging can be difficult, since you are not developing the computation graph in the host language.</li><li>Exploiting within computation graph parallelism might be difficult.</li></ul><p>Comments:</p><ul><li><a href="https://github.com/FluxML/DaggerFlux.jl">DaggerFLux.jl</a> use this approach to perform model-based paralelism, where parts of the computation graph (and especially parameters) can reside on different machines.</li><li><a href="https://github.com/dfdx/Umlaut.jl">Umlaut.jl</a> allows to easily obtain the tape through <em>tracing</em> of the execution of a function, which can be then used to implement the AD as described above (see <a href="https://dfdx.github.io/Yota.jl/dev/design/">Yota&#39;s documentation</a> for complete example).</li></ul><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Umlaut</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; g(x, y) = x * y</code><code class="nohighlight hljs ansi" style="display:block;">g (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; f(x, y) = g(x, y)+sin(x)</code><code class="nohighlight hljs ansi" style="display:block;">f (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; tape = trace(f, 1.0, 2.0)[2]</code><code class="nohighlight hljs ansi" style="display:block;">Tape{Umlaut.BaseCtx}
  inp %1::typeof(Main.f)
  inp %2::Float64
  inp %3::Float64
  %4 = *(%2, %3)::Float64
  %5 = sin(%2)::Float64
  %6 = +(%4, %5)::Float64</code></pre><p><code>Yota.jl</code> use the tape to generate the gradient as </p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; tape = Yota.gradtape(f, 1.0, 2.0; seed=1.0)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: Yota not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Umlaut.to_expr(tape)</code><code class="nohighlight hljs ansi" style="display:block;">:(function var&quot;##tape_f#340&quot;(x1::typeof(Main.f), x2::Float64, x3::Float64)
      x4 = (*)(x2, x3)
      x5 = (sin)(x2)
      x6 = (+)(x4, x5)
      return x6
  end)</code></pre><h3 id="Tracking-based-AD"><a class="docs-heading-anchor" href="#Tracking-based-AD">Tracking-based AD</a><a id="Tracking-based-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Tracking-based-AD" title="Permalink"></a></h3><p>Alternative to static-graph based methods are methods, which builds the graph during invocation of functions and then use this dynamically built graph to know, how to compute the gradient. The dynamically built graph is frequently called <em>tape</em>. This approach is used by popular libraries like <a href="https://pytorch.org/"><strong><em>PyTorch</em></strong></a>, <a href="https://github.com/HIPS/autograd"><strong><em>AutoGrad</em></strong></a>, and <a href="https://chainer.org/"><strong><em>Chainer</em></strong></a> in Python ecosystem, or by <a href="https://github.com/FluxML/Tracker.jl"><strong><em>Tracker.jl</em></strong></a> (<code>Flux.jl</code>&#39;s former AD backend), <a href="https://github.com/JuliaDiff/ReverseDiff.jl"><strong><em>ReverseDiff.jl</em></strong></a>, and <a href="https://github.com/denizyuret/AutoGrad.jl"><strong><em>AutoGrad.jl</em></strong></a> (<code>Knet.jl</code>&#39;s AD backend) in Julia. This type of AD systems is also called <em>operator overloading</em>, since in order to record the operations performed on the arguments we need to replace/wrap the original implementation.</p><p>How do we build the tracing? Let&#39;s take a look what <code>ReverseDiff.jl</code> is doing. It defines <code>TrackedArray</code> (it also defines <code>TrackedReal</code>, but <code>TrackedArray</code> is more interesting) as</p><pre><code class="language-julia hljs">struct TrackedArray{T,N,V&lt;:AbstractArray{T,N}} &lt;: AbstractArray{T,N}
    value::V
    deriv::Union{Nothing,V}
    tape::Vector{Any}
    string_tape::String
end</code></pre><p>where in</p><ul><li><code>value</code> it stores the value of the array</li><li><code>deriv</code> will hold the gradient of the tracked array</li><li><code>tape</code> of will log operations performed with the tracked array, such that we can calculate the gradient as a sum of operations performed over the tape.</li></ul><p>What do we need to store on the tape? Let&#39;s denote as <span>$a$</span> the current <code>TrackedArray</code>. The gradient with respect to some output <span>$z$</span> is equal to <span>$\frac{\partial z}{\partial a} = \sum_{g_i} \frac{\partial z}{\partial g_i} \times \frac{\partial g_i}{\partial a}$</span> where  <span>$g_i$</span> is the output of any function (in the computational graph) where <span>$a$</span> was a direct input. The <code>InstructionTape</code> will therefore contain a reference to <span>$g_i$</span> (which has to be of <code>TrackedArray</code> and where we know <span>$\frac{\partial z}{\partial g_i}$</span> will be stored in <code>deriv</code> field) and we also need to a method calculating <span>$\frac{\partial g_i}{\partial a}$</span>, which can be stored as an anonymous function will accepting the grad as an argument.</p><pre><code class="language-julia hljs">TrackedArray(a::AbstractArray, string_tape::String = &quot;&quot;) = TrackedArray(a, similar(a) .= 0, [], string_tape)
TrackedMatrix{T,V} = TrackedArray{T,2,V} where {T,V&lt;:AbstractMatrix{T}}
TrackedVector{T,V} = TrackedArray{T,1,V} where {T,V&lt;:AbstractVector{T}}
Base.show(io::IO, ::MIME&quot;text/plain&quot;, a::TrackedArray) = show(io, a)
Base.show(io::IO, a::TrackedArray) = print(io, &quot;TrackedArray($(size(a.value)))&quot;)
value(A::TrackedArray) = A.value
value(A) = A
track(A, string_tape = &quot;&quot;) = TrackedArray(A, string_tape)
track(a::Number, string_tape) = TrackedArray(reshape([a], 1, 1), string_tape)

import Base: +, *
function *(A::TrackedMatrix, B::TrackedMatrix)
    a, b = value.((A, B))
    C = TrackedArray(a * b, &quot;($(A.string_tape) * $(B.string_tape))&quot;)
    push!(A.tape, (C, Δ -&gt; Δ * b&#39;))
    push!(B.tape, (C, Δ -&gt; a&#39; * Δ))
    C
end

function +(A::TrackedMatrix, B::TrackedMatrix)
    C = TrackedArray(value(A) + value(B), &quot;($(A.string_tape) + $(B.string_tape))&quot;)
    push!(A.tape, (C, Δ -&gt; Δ))
    push!(B.tape, (C, Δ -&gt; Δ))
    C
end

function msin(A::TrackedMatrix)
    a = value(A)
    C = TrackedArray(sin.(a), &quot;sin($(A.string_tape))&quot;)
    push!(A.tape, (C, Δ -&gt; cos.(a) .* Δ))
    C
end</code></pre><p>Let&#39;s observe that the operations are recorded on the tape as they should</p><pre><code class="language-julia hljs">a = rand()
b = rand()
A = track(a, &quot;A&quot;)
B = track(b, &quot;B&quot;)
# R = A * B + msin(A)
C = A * B 
A.tape
B.tape
C.string_tape
R = C + msin(A)
A.tape
B.tape
R.string_tape</code></pre><p>Let&#39;s now implement a function that will recursively calculate the gradient of a term of interest. It goes over its childs, if they not have calculated the gradients, calculate it, otherwise it adds it to its own after  if not, ask them to calculate the gradient and otherwise </p><pre><code class="language-julia hljs">function accum!(A::TrackedArray)
    isempty(A.tape) &amp;&amp; return(A.deriv)
    A.deriv .= sum(g(accum!(r)) for (r, g) in A.tape)
    empty!(A.tape)
    A.deriv
end</code></pre><p>We can calculate the gradient by initializing the gradient of the result to vector of ones simulating the <code>sum</code> function</p><pre><code class="language-julia hljs">using FiniteDifferences
R.deriv .= 1
accum!(A)[1]
∇a = grad(central_fdm(5,1), a -&gt; a*b + sin(a), a)[1]
A.deriv[1] ≈ ∇a
accum!(B)[1]
∇b = grad(central_fdm(5,1), b -&gt; a*b + sin(a), b)[1]
B.deriv[1] ≈ ∇b</code></pre><p>The api function for computing the grad might look like</p><pre><code class="language-julia hljs">function trackedgrad(f, args...)
    args = track.(args)
    o = f(args...)
    fill!(o.deriv, 1)
    map(accum!, args)
end</code></pre><p>where we should assert that the output dimension is 1. In our implementation we dirtily expect the output of f to be summed to a scalar.</p><p>Let&#39;s compare the results to those computed by FiniteDifferences</p><pre><code class="language-julia hljs">A = rand(4,4)
B = rand(4,4)
trackedgrad(A -&gt; A * B + msin(A), A)[1]
grad(central_fdm(5,1), A -&gt; sum(A * B + sin.(A)), A)[1]
trackedgrad(A -&gt; A * B + msin(A), B)[1]
grad(central_fdm(5,1), A -&gt; sum(A * B + sin.(A)), B)[1]</code></pre><p>To make the above AD system really useful, we would need to </p><ol><li>Add support for <code>TrackedReal</code>, which is straightforward (we might skip the anonymous function, as the derivative of a scalar function is always a number).</li><li>We would need to add a lot of rules, how to work with basic values. This is why the the approach is called <strong>operator overloading</strong> since you need to overload a lot of functions (or methods or operators).</li></ol><p>For example to add all combinations for <code>+</code> and <code>*</code>, we would need to add following rules.</p><pre><code class="language-julia hljs">function *(A::TrackedMatrix, B::AbstractMatrix)
   C = TrackedArray(value(A) * B, &quot;($(A.string_tape) * B)&quot;)
   push!(A.tape, (C, Δ -&gt; Δ * B&#39;))
   C
end

function *(A::AbstractMatrix, B::TrackedMatrix)
   C = TrackedArray(value(A) * value(B), &quot;($(A.string_tape) * $(B.string_tape))&quot;)
   push!(B.tape, (C, Δ -&gt; A&#39; * Δ))
   C
end

function +(A::TrackedMatrix, B::TrackedMatrix)
   C = TrackedArray(value(A) + value(B), &quot;($(A.string_tape) + $(B.string_tape))&quot;)
   push!(A.tape, (C, Δ -&gt; Δ ))
   push!(B.tape, (C, Δ -&gt; Δ))
   C
end

function +(A::AbstractMatrix, B::TrackedMatrix)
   C = TrackedArray(A * value(B), &quot;(A + $(B.string_tape))&quot;)
   push!(B.tape, (C, Δ -&gt; Δ))
   C
end</code></pre><p>Advantages:</p><ul><li>Debugging and development is nicer, as AD is implemented in the same language.</li><li>The computation graph, tape, is dynamic, which makes it simpler to take the gradient in the presence of <code>if</code> and <code>while</code>.</li></ul><p>Disadvantages:</p><ul><li>The computation graph is created and differentiated during every computation, which might be costly. In most deep learning applications, this overhead is negligible in comparison to time of needed to perform the operations itself (<code>ReverseDiff.jl</code> allows to compile the tape).</li><li>Since computation graph is dynamic, it cannot be optimized as the static graph, the same holds for the memory allocations. </li></ul><p>A more complete example which allow to train feed-forward neural network on GPU can be found <a href="../ffnn.jl">here</a>.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>The difference between tracking and graph-based AD systems is conceptually similar to interpreted and compiled programming languages. Tracking AD systems interpret the time while computing the gradient, while graph-based AD systems compile the computation of the gradient.</p></div></div><h2 id="ChainRules"><a class="docs-heading-anchor" href="#ChainRules">ChainRules</a><a id="ChainRules-1"></a><a class="docs-heading-anchor-permalink" href="#ChainRules" title="Permalink"></a></h2><p>From our discussions about AD systems so far we see that while the basic, <em>engine</em>, part is relatively straightforward, it devil is in writing the rules prescribing the computation of gradients. These rules are needed for every system whether it is graph based, tracking, or Wengert list based. ForwardDiff also needs a rule system, but rules are a bit different (as they are pushing the gradient forward rather than pulling it back). It is obviously a waste of effort for each AD system to have its own set of rules. Therefore the community (initiated by Catherine Frames White backed by <a href="https://github.com/invenia">Invenia</a>) have started to work on a unified system to express differentiation rules, such that they can be shared between systems. So far, they are supported by <code>Zygote.jl</code>, <code>Nabla.jl</code>, <code>ReverseDiff.jl</code> and <code>Diffractor.jl</code>, suggesting that the unification approach is working.</p><p>The definition of reverse diff rules follows the idea we have nailed above (we refer readers interested in forward diff rules <a href="https://juliadiff.org/ChainRulesCore.jl">to official documentation</a>).</p><p><code>ChainRules</code> defines the reverse rules for function <code>foo</code> in a function <code>rrule</code> with the following signature</p><pre><code class="language-julia hljs">function rrule(::typeof(foo), args...; kwargs...)
    ...
    return y, pullback
end</code></pre><p>where</p><ul><li>the first argument <code>::typeof(foo)</code> allows to dispatch on the function for which the rules is written</li><li>the output of function <code>foo(args...)</code> is returned as the first argument</li><li><code>pullback(Δy)</code> takes the gradient of upstream functions with respect to the output of <code>foo(args)</code> and returns it multiplied by the jacobian of the output of <code>foo(args)</code> with respect to parameters of the function itself (recall the function can have paramters, as it can be a closure or a functor), and with respect to the arguments.</li></ul><pre><code class="language-julia hljs">function pullback(Δy)
    ...
    return ∂self, ∂args...
end</code></pre><p>Notice that key-word arguments are not differentiated. This is a design decision with the explanation that parametrize the function, but most of the time, they are not differentiable.</p><p><code>ChainRules.jl</code> provides support for lazy (delayed) computation using <code>Thunk</code>. Its argument is a function, which is not evaluated until <code>unthunk</code> is called. There is also a support to signal that gradient is zero using <code>ZeroTangent</code> (which can save valuable memory) or to signal that the gradient does not exist using <code>NoTangent</code>. </p><p>How can we use ChainRules to define rules for our AD system? Let&#39;s first observe the output</p><pre><code class="language-julia hljs">using ChainRulesCore, ChainRules
r, g = rrule(*, rand(2,2), rand(2,2))
g(r)</code></pre><p>With that, we can extend our AD system as follows</p><pre><code class="language-julia hljs">import Base: *, +, -
for f in [:*, :+, :-]
    @eval function $(f)(A::TrackedMatrix, B::AbstractMatrix)
       C, pullback = rrule($(f), value(A), B)
       C = track(C)
       push!(A.tape, (C, Δ -&gt; pullback(Δ)[2]))
       C
    end

    @eval function $(f)(A::AbstractMatrix, B::TrackedMatrix)
       C, pullback = rrule($(f), A, value(B))
       C = track(C)
       push!(B.tape, (C, Δ -&gt; pullback(Δ)[3]))
       C
    end

    @eval function $(f)(A::TrackedMatrix, B::TrackedMatrix)
       C, pullback = rrule($(f), value(A), value(B))
       C = track(C)
       push!(A.tape, (C, Δ -&gt; pullback(Δ)[2]))
       push!(B.tape, (C, Δ -&gt; pullback(Δ)[3]))
       C
    end
end</code></pre><p>and we need to modify our <code>accum!</code> code to <code>unthunk</code> if needed</p><pre><code class="language-julia hljs">function accum!(A::TrackedArray)
    isempty(A.tape) &amp;&amp; return(A.deriv)
    A.deriv .= sum(unthunk(g(accum!(r))) for (r, g) in A.tape)
end</code></pre><pre><code class="language-julia hljs">A = rand(4,4)
B = rand(4,4)
grad(A -&gt; (A * B + msin(A))*B, A)[1]
gradient(A -&gt; sum(A * B + sin.(A)), A)[1]
grad(A -&gt; A * B + msin(A), B)[1]
gradient(A -&gt; sum(A * B + sin.(A)), B)[1]</code></pre><h2 id="Source-to-source-AD-using-Wengert"><a class="docs-heading-anchor" href="#Source-to-source-AD-using-Wengert">Source-to-source AD using Wengert</a><a id="Source-to-source-AD-using-Wengert-1"></a><a class="docs-heading-anchor-permalink" href="#Source-to-source-AD-using-Wengert" title="Permalink"></a></h2><p>Recall the compile stages of julia and look, how the lowered code for</p><pre><code class="language-julia hljs">f(x,y) = x*y + sin(x)</code></pre><p>looks like</p><pre><code class="language-julia hljs">julia&gt; @code_lowered f(1.0, 1.0)
CodeInfo(
1 ─ %1 = x * y
│   %2 = Main.sin(x)
│   %3 = %1 + %2
└──      return %3
)</code></pre><p>This form is particularly nice for automatic differentiation, as we have on the left hand side always a single variable, which means the compiler has provided us with a form, on which we know, how to apply AD rules.</p><p>What if we somehow be able to talk to the compiler and get this form from him?</p><ul><li><a href="https://juliadiff.org/ChainRulesCore.jl/dev/autodiff/operator_overloading.html#ReverseDiffZero">simplest viable implementation</a></li></ul><h2 id="ChainRules-2"><a class="docs-heading-anchor" href="#ChainRules-2">ChainRules</a><a class="docs-heading-anchor-permalink" href="#ChainRules-2" title="Permalink"></a></h2><p>From our discussions about AD systems so far we see that while the basic, <em>engine</em>, part is relatively straightforward, it devil is in writing the rules prescribing the computation of gradients. These rules are needed for every system whether it is graph based, tracking, or wengert list based. ForwardDiff also needs a rule system, but rules are a bit different (as they are pushing the gradient forward rather than pulling it back). It is obviously a waste of effort for each AD system to have its own set of rules. Hence the community (initiated by Lyndon White and hence probably funded mainly by Invenia) have started to work on a unified system to express differentiation rules, such that they can be shared between systems. So far, they are supported by <code>Zygote.jl</code>, <code>Nabla.jl</code>, <code>ReverseDiff.jl</code> and <code>Diffractor.jl</code>, hence the approach seems to work.</p><p>The definition of reverse diff rules follows the idea we have nailed above (we refer readers interested in forward diff rules <a href="https://juliadiff.org/ChainRulesCore.jl">to official documentation</a>).</p><p><code>ChainRules</code> defines the reverse rules for function <code>foo</code> in a function <code>rrule</code> with the following signature</p><pre><code class="language-julia hljs">function rrule(::typeof(foo), args...; kwargs...)
    ...
    return y, pullback
end</code></pre><p>where</p><ul><li>the first argument <code>::typeof(foo)</code> allows to dispatch on the function for which the rules is written</li><li>the output of function <code>foo(args...)</code> is returned as the first argument</li><li><code>pullback(Δy)</code> takes the gradient of upstream functions with respect to the output of <code>foo(args)</code> and returns it multiplied by the jacobian of the output of <code>foo(args)</code> with respect to parameters of the function itself (recall the function can have paramters, as it can be a closure or a functor), and with respect to the arguments.</li></ul><pre><code class="language-julia hljs">function pullback(Δy)
    ...
    return ∂self, ∂args...
end</code></pre><p>Notice that key-word arguments are not differentiated. This is a design decision with the explanation that parametrize the function, but most of the time, they are not differentiable.</p><p><code>ChainRules.jl</code> provides support for lazy (delayed) computation using <code>Thunk</code>. Its argument is a function, which is not evaluated until <code>unthunk</code> is called. There is also a support to signal that gradient is zero using <code>ZeroTangent</code> (which can save valuable memory) or to signal that the gradient does not exist using <code>NoTangent</code>. </p><p>How can we use ChainRules to define rules for our AD system? Let&#39;s first observe the output</p><pre><code class="language-julia hljs">using ChainRulesCore, ChainRules
r, g = rrule(*, rand(2,2), rand(2,2))
g(r)</code></pre><p>With that, we can extend our AD system as follows</p><pre><code class="language-julia hljs">import Base: *, +, -
for f in [:*, :+, :-]
    @eval function $(f)(A::TrackedMatrix, B::AbstractMatrix)
       C, pullback = rrule($(f), value(A), B)
       C = track(C)
       push!(A.tape, (C, Δ -&gt; pullback(Δ)[2]))
       C
    end

    @eval function $(f)(A::AbstractMatrix, B::TrackedMatrix)
       C, pullback = rrule($(f), A, value(B))
       C = track(C)
       push!(B.tape, (C, Δ -&gt; pullback(Δ)[3]))
       C
    end

    @eval function $(f)(A::TrackedMatrix, B::TrackedMatrix)
       C, pullback = rrule($(f), value(A), value(B))
       C = track(C)
       push!(A.tape, (C, Δ -&gt; pullback(Δ)[2]))
       push!(B.tape, (C, Δ -&gt; pullback(Δ)[3]))
       C
    end
end</code></pre><p>and we need to modify our <code>accum!</code> code to <code>unthunk</code> if needed</p><pre><code class="language-julia hljs">function accum!(A::TrackedArray)
    isempty(A.tape) &amp;&amp; return(A.deriv)
    A.deriv .= sum(unthunk(g(accum!(r))) for (r, g) in A.tape)
end</code></pre><pre><code class="nohighlight hljs">A = track(rand(4,4))
B = rand(4,1)
C = rand(4,1)
R = A * B + C
R.deriv .= 1
accum!(A)</code></pre><h3 id="Sources-for-this-lecture"><a class="docs-heading-anchor" href="#Sources-for-this-lecture">Sources for this lecture</a><a id="Sources-for-this-lecture-1"></a><a class="docs-heading-anchor-permalink" href="#Sources-for-this-lecture" title="Permalink"></a></h3><ul><li>Mike Innes&#39; <a href="https://github.com/MikeInnes/diff-zoo">diff-zoo</a></li><li><a href="https://blog.rogerluo.me/2019/07/27/yassad/">Write Your Own StS in One Day</a></li><li><a href="https://dfdx.github.io/Yota.jl/dev/design/">Build your own AD with Umlaut</a></li><li><a href="https://arxiv.org/pdf/1810.07951.pdf">Zygote.jl Paper</a> and <a href="https://fluxml.ai/Zygote.jl/dev/internals/">Zygote.jl Internals</a></li><li>Keno&#39;s <a href="https://www.youtube.com/watch?v=mQnSRfseu0c&amp;feature=youtu.be">Talk</a></li><li>Chris&#39; <a href="https://mitmath.github.io/18337/lecture11/adjoints">Lecture</a></li><li><a href="https://liebing.org.cn/2019/07/22/Automatic-Differentiation-Based-on-Computation-Graph/">Automatic-Differentiation-Based-on-Computation-Graph</a></li></ul><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Linnainmaa, S. (1976). Taylor expansion of the accumulated rounding error. <em>BIT Numerical Mathematics</em>, 16(2), 146-160.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. <em>Nature</em>, 323, 533–536.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_07/hw/">« Homework</a><a class="docs-footer-nextpage" href="../lab/">Lab »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Friday 23 December 2022 21:09">Friday 23 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
